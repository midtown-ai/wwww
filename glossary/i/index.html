
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Let's explore this transforming technology. Let's shape the future of AI together.">
      
      
        <meta name="author" content="info@midtown.ai (Emmanuel M.)">
      
      
        <link rel="canonical" href="https://midtown-ai.github.io/wwww/glossary/i/">
      
      
        <link rel="prev" href="../h/">
      
      
        <link rel="next" href="../j/">
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.3">
    
    
      
        <title>I - Midtown AI</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.d7758b05.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M96 0C43 0 0 43 0 96v320c0 53 43 96 96 96h320c17.7 0 32-14.3 32-32s-14.3-32-32-32v-64c17.7 0 32-14.3 32-32V32c0-17.7-14.3-32-32-32H96m0 384h256v64H96c-17.7 0-32-14.3-32-32s14.3-32 32-32m32-240c0-8.8 7.2-16 16-16h192c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16m16 48h192c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16s7.2-16 16-16"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M320 0c17.7 0 32 14.3 32 32v64h120c39.8 0 72 32.2 72 72v272c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72h120V32c0-17.7 14.3-32 32-32M208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zM264 256a40 40 0 1 0-80 0 40 40 0 1 0 80 0m152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80M48 224h16v192H48c-26.5 0-48-21.5-48-48v-96c0-26.5 21.5-48 48-48m544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48h-16V224z"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M288 0H128c-17.7 0-32 14.3-32 32s14.3 32 32 32v132.8c0 11.8-3.3 23.5-9.5 33.5L10.3 406.2C3.6 417.2 0 429.7 0 442.6 0 480.9 31.1 512 69.4 512h309.2c38.3 0 69.4-31.1 69.4-69.4 0-12.8-3.6-25.4-10.3-36.4L329.5 230.4c-6.2-10.1-9.5-21.7-9.5-33.5V64c17.7 0 32-14.3 32-32S337.7 0 320 0zm-96 196.8V64h64v132.8c0 23.7 6.6 46.9 19 67.1l34.5 56.1h-171l34.5-56.1c12.4-20.2 19-43.4 19-67.1"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.1 52.4 442.6 6.5c-1.9-3.9-6.1-6.5-10.5-6.5s-8.5 2.6-10.4 6.5l-16.5 45.9-46 16.8c-4.3 1.6-7.3 5.9-7.2 10.4 0 4.5 3 8.7 7.2 10.2l45.7 16.8 16.8 45.8c1.5 4.4 5.8 7.5 10.4 7.5s8.9-3.1 10.4-7.5l16.5-45.8 45.7-16.8c4.2-1.5 7.2-5.7 7.2-10.2 0-4.6-3-8.9-7.2-10.4zm-132.4 53c-12.5-12.5-32.8-12.5-45.3 0l-2.9 2.9c-22-8-45.8-12.3-70.5-12.3C93.1 96 0 189.1 0 304s93.1 208 208 208 208-93.1 208-208c0-24.7-4.3-48.5-12.2-70.5l2.9-2.9c12.5-12.5 12.5-32.8 0-45.3l-80-80zM200 192c-57.4 0-104 46.6-104 104v8c0 8.8-7.2 16-16 16s-16-7.2-16-16v-8c0-75.1 60.9-136 136-136h8c8.8 0 16 7.2 16 16s-7.2 16-16 16z"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-40-176h24v-64h-24c-13.3 0-24-10.7-24-24s10.7-24 24-24h48c13.3 0 24 10.7 24 24v88h8c13.3 0 24 10.7 24 24s-10.7 24-24 24h-80c-13.3 0-24-10.7-24-24s10.7-24 24-24m40-208a32 32 0 1 1 0 64 32 32 0 1 1 0-64"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 216C0 149.7 53.7 96 120 96h8c17.7 0 32 14.3 32 32s-14.3 32-32 32h-8c-30.9 0-56 25.1-56 56v8h64c35.3 0 64 28.7 64 64v64c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64V216m256 0c0-66.3 53.7-120 120-120h8c17.7 0 32 14.3 32 32s-14.3 32-32 32h-8c-30.9 0-56 25.1-56 56v8h64c35.3 0 64 28.7 64 64v64c0 35.3-28.7 64-64 64h-64c-35.3 0-64-28.7-64-64V216"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M438.6 105.4c12.5 12.5 12.5 32.8 0 45.3l-256 256c-12.5 12.5-32.8 12.5-45.3 0l-128-128c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L160 338.7l233.4-233.3c12.5-12.5 32.8-12.5 45.3 0z"/></svg>');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/custom_admonitions.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_effects.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_tables.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_text.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_theme.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="I - Midtown AI" >
      
        <meta  property="og:description"  content="Let's explore this transforming technology. Let's shape the future of AI together." >
      
        <meta  property="og:image"  content="https://midtown-ai.github.io/wwww/assets/images/social/glossary/i.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://midtown-ai.github.io/wwww/glossary/i/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="I - Midtown AI" >
      
        <meta  name="twitter:description"  content="Let's explore this transforming technology. Let's shape the future of AI together." >
      
        <meta  name="twitter:image"  content="https://midtown-ai.github.io/wwww/assets/images/social/glossary/i.png" >
      
    
    
  <link rel="stylesheet" href="../../stylesheets/custom.7c86dd97.min.css">

  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#i" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Midtown AI" class="md-header__button md-logo" aria-label="Midtown AI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M22.7 33.4c13.5-4.1 28.1 1.1 35.9 12.9l165.4 248 165.4-248c7.8-11.7 22.4-17 35.9-12.9S448 49.9 448 64v384c0 17.7-14.3 32-32 32s-32-14.3-32-32V169.7L250.6 369.8c-5.9 8.9-15.9 14.2-26.6 14.2s-20.7-5.3-26.6-14.2L64 169.7V448c0 17.7-14.3 32-32 32S0 465.7 0 448V64c0-14.1 9.2-26.5 22.7-30.6"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Midtown AI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              I
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../blog/" class="md-tabs__link">
          
  
    
  
  Blog

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
    
  
  Glossary

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../about/" class="md-tabs__link">
        
  
    
  
  About

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Midtown AI" class="md-nav__button md-logo" aria-label="Midtown AI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M22.7 33.4c13.5-4.1 28.1 1.1 35.9 12.9l165.4 248 165.4-248c7.8-11.7 22.4-17 35.9-12.9S448 49.9 448 64v384c0 17.7-14.3 32-32 32s-32-14.3-32-32V169.7L250.6 369.8c-5.9 8.9-15.9 14.2-26.6 14.2s-20.7-5.3-26.6-14.2L64 169.7V448c0 17.7-14.3 32-32 32S0 465.7 0 448V64c0-14.1 9.2-26.5 22.7-30.6"/></svg>

    </a>
    Midtown AI
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../blog/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Blog
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Blog
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Archive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/archive/2025/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2025
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Categories
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Categories
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/category/entertainment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Entertainment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/category/no-code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    No Code
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Glossary
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Glossary
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0-9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    0-9
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../a/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../b/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../c/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    D
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../e/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../f/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    F
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../g/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    G
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../h/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    H
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    I
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    I
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ibm-company" class="md-nav__link">
    <span class="md-ellipsis">
      IBM Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ibm-watson" class="md-nav__link">
    <span class="md-ellipsis">
      IBM Watson
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ilya-sutskever-person" class="md-nav__link">
    <span class="md-ellipsis">
      Ilya Sutskever Person
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Image Analysis
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-classifier" class="md-nav__link">
    <span class="md-ellipsis">
      Image Classifier
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Image Classification
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-compression" class="md-nav__link">
    <span class="md-ellipsis">
      Image Compression
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      Image Decoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      Image Encoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-enhancement" class="md-nav__link">
    <span class="md-ellipsis">
      Image Enhancement
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-filter" class="md-nav__link">
    <span class="md-ellipsis">
      Image Filter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-generator" class="md-nav__link">
    <span class="md-ellipsis">
      Image Generator
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-joint-embedding-predictive-architecture-i-jepa" class="md-nav__link">
    <span class="md-ellipsis">
      Image Joint-Embedding Predictive Architecture (I-JEPA)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-inpainting" class="md-nav__link">
    <span class="md-ellipsis">
      Image Inpainting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-processing" class="md-nav__link">
    <span class="md-ellipsis">
      Image Processing
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-reconstruction" class="md-nav__link">
    <span class="md-ellipsis">
      Image Reconstruction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-restoration" class="md-nav__link">
    <span class="md-ellipsis">
      Image Restoration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Image Segmentation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-aware-decoder-enhanced-a-la-flamingo-with-interleaved-cross-attentions-idefics-model" class="md-nav__link">
    <span class="md-ellipsis">
      Image-aware Decoder Enhanced a la Flamingo with Interleaved Cross-attentionS (IDEFICS) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagen-model-family" class="md-nav__link">
    <span class="md-ellipsis">
      Imagen Model Family
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagen-video-model" class="md-nav__link">
    <span class="md-ellipsis">
      Imagen Video Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagenet-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet Dataset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagenet-large-scale-visual-recognition-ilsvrc-challenge" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet Large Scale Visual Recognition (ILSVRC) Challenge
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imitation-learning-il" class="md-nav__link">
    <span class="md-ellipsis">
      Imitation Learning (IL)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imbalanced-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Imbalanced Dataset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact-factor" class="md-nav__link">
    <span class="md-ellipsis">
      Impact Factor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imputation" class="md-nav__link">
    <span class="md-ellipsis">
      Imputation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#incentive" class="md-nav__link">
    <span class="md-ellipsis">
      Incentive
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#indexgpt-model" class="md-nav__link">
    <span class="md-ellipsis">
      IndexGPT Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inductive-bias" class="md-nav__link">
    <span class="md-ellipsis">
      Inductive Bias
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inductive-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Inductive Reasoning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#industrial-robot" class="md-nav__link">
    <span class="md-ellipsis">
      Industrial Robot
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inertial-measurement-unit-imu" class="md-nav__link">
    <span class="md-ellipsis">
      Inertial Measurement Unit (IMU)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    <span class="md-ellipsis">
      Inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference-configuration-parameter" class="md-nav__link">
    <span class="md-ellipsis">
      Inference Configuration Parameter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference-point" class="md-nav__link">
    <span class="md-ellipsis">
      Inference Point
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inflection-ai-company" class="md-nav__link">
    <span class="md-ellipsis">
      Inflection AI Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#informal-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Informal Reasoning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#information" class="md-nav__link">
    <span class="md-ellipsis">
      Information
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#information-retrieval-ir" class="md-nav__link">
    <span class="md-ellipsis">
      Information Retrieval (IR)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inlier" class="md-nav__link">
    <span class="md-ellipsis">
      Inlier
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#input-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Input Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#input-space" class="md-nav__link">
    <span class="md-ellipsis">
      Input Space
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#input-weight" class="md-nav__link">
    <span class="md-ellipsis">
      Input Weight
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instance-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Instance Segmentation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instructgpt-model" class="md-nav__link">
    <span class="md-ellipsis">
      InstructGPT Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instruction-tuned-it-model" class="md-nav__link">
    <span class="md-ellipsis">
      Instruction Tuned (IT) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#insufficient-data-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Insufficient Data Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intel-company" class="md-nav__link">
    <span class="md-ellipsis">
      Intel Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligence" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligence
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligence-augmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligence Augmentation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligence-explosion" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligence Explosion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligent-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligent Agent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligent-digital-assistant-ida" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligent Digital Assistant (IDA)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intent-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Intent Analysis
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-planning" class="md-nav__link">
    <span class="md-ellipsis">
      Interactive Planning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#international-conference-on-learning-representations-iclr-conference" class="md-nav__link">
    <span class="md-ellipsis">
      International Conference on Learning Representations (ICLR) Conference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#international-conference-on-machine-learning-icml-conference" class="md-nav__link">
    <span class="md-ellipsis">
      International Conference on Machine Learning (ICML) Conference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interpretml" class="md-nav__link">
    <span class="md-ellipsis">
      InterpretML
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverse-document-frequency-idf" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse Document Frequency (IDF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverse-dynamics-model-idm" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse Dynamics Model (IDM)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverse-q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse Q-Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverse-rl-irl" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse RL (IRL)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverted-file-index-ivd" class="md-nav__link">
    <span class="md-ellipsis">
      Inverted File Index (IVD)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iq-learn-model" class="md-nav__link">
    <span class="md-ellipsis">
      IQ-Learn Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#isaac-gym-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Isaac Gym Environment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iso-42001-standard" class="md-nav__link">
    <span class="md-ellipsis">
      ISO 42001 Standard
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#isolation-forest-if" class="md-nav__link">
    <span class="md-ellipsis">
      Isolation Forest (IF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#isomorphic-labs-company" class="md-nav__link">
    <span class="md-ellipsis">
      Isomorphic Labs Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#istio" class="md-nav__link">
    <span class="md-ellipsis">
      Istio
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iteration" class="md-nav__link">
    <span class="md-ellipsis">
      Iteration
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../j/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    J
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../k/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    K
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../l/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    L
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    M
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../n/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    N
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../o/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    O
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../p/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    P
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../q/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Q
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../r/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    R
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../s/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    T
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../u/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    U
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../v/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../w/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    W
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../x/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    X
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../y/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Y
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../z/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Z
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ibm-company" class="md-nav__link">
    <span class="md-ellipsis">
      IBM Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ibm-watson" class="md-nav__link">
    <span class="md-ellipsis">
      IBM Watson
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ilya-sutskever-person" class="md-nav__link">
    <span class="md-ellipsis">
      Ilya Sutskever Person
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Image Analysis
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-classifier" class="md-nav__link">
    <span class="md-ellipsis">
      Image Classifier
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Image Classification
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-compression" class="md-nav__link">
    <span class="md-ellipsis">
      Image Compression
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      Image Decoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      Image Encoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-enhancement" class="md-nav__link">
    <span class="md-ellipsis">
      Image Enhancement
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-filter" class="md-nav__link">
    <span class="md-ellipsis">
      Image Filter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-generator" class="md-nav__link">
    <span class="md-ellipsis">
      Image Generator
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-joint-embedding-predictive-architecture-i-jepa" class="md-nav__link">
    <span class="md-ellipsis">
      Image Joint-Embedding Predictive Architecture (I-JEPA)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-inpainting" class="md-nav__link">
    <span class="md-ellipsis">
      Image Inpainting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-processing" class="md-nav__link">
    <span class="md-ellipsis">
      Image Processing
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-reconstruction" class="md-nav__link">
    <span class="md-ellipsis">
      Image Reconstruction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-restoration" class="md-nav__link">
    <span class="md-ellipsis">
      Image Restoration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Image Segmentation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-aware-decoder-enhanced-a-la-flamingo-with-interleaved-cross-attentions-idefics-model" class="md-nav__link">
    <span class="md-ellipsis">
      Image-aware Decoder Enhanced a la Flamingo with Interleaved Cross-attentionS (IDEFICS) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagen-model-family" class="md-nav__link">
    <span class="md-ellipsis">
      Imagen Model Family
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagen-video-model" class="md-nav__link">
    <span class="md-ellipsis">
      Imagen Video Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagenet-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet Dataset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imagenet-large-scale-visual-recognition-ilsvrc-challenge" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet Large Scale Visual Recognition (ILSVRC) Challenge
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imitation-learning-il" class="md-nav__link">
    <span class="md-ellipsis">
      Imitation Learning (IL)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imbalanced-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Imbalanced Dataset
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact-factor" class="md-nav__link">
    <span class="md-ellipsis">
      Impact Factor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imputation" class="md-nav__link">
    <span class="md-ellipsis">
      Imputation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#incentive" class="md-nav__link">
    <span class="md-ellipsis">
      Incentive
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#indexgpt-model" class="md-nav__link">
    <span class="md-ellipsis">
      IndexGPT Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inductive-bias" class="md-nav__link">
    <span class="md-ellipsis">
      Inductive Bias
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inductive-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Inductive Reasoning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#industrial-robot" class="md-nav__link">
    <span class="md-ellipsis">
      Industrial Robot
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inertial-measurement-unit-imu" class="md-nav__link">
    <span class="md-ellipsis">
      Inertial Measurement Unit (IMU)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    <span class="md-ellipsis">
      Inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference-configuration-parameter" class="md-nav__link">
    <span class="md-ellipsis">
      Inference Configuration Parameter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inference-point" class="md-nav__link">
    <span class="md-ellipsis">
      Inference Point
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inflection-ai-company" class="md-nav__link">
    <span class="md-ellipsis">
      Inflection AI Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#informal-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Informal Reasoning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#information" class="md-nav__link">
    <span class="md-ellipsis">
      Information
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#information-retrieval-ir" class="md-nav__link">
    <span class="md-ellipsis">
      Information Retrieval (IR)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Initialization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inlier" class="md-nav__link">
    <span class="md-ellipsis">
      Inlier
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#input-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Input Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#input-space" class="md-nav__link">
    <span class="md-ellipsis">
      Input Space
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#input-weight" class="md-nav__link">
    <span class="md-ellipsis">
      Input Weight
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instance-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Instance Segmentation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instructgpt-model" class="md-nav__link">
    <span class="md-ellipsis">
      InstructGPT Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#instruction-tuned-it-model" class="md-nav__link">
    <span class="md-ellipsis">
      Instruction Tuned (IT) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#insufficient-data-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Insufficient Data Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intel-company" class="md-nav__link">
    <span class="md-ellipsis">
      Intel Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligence" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligence
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligence-augmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligence Augmentation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligence-explosion" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligence Explosion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligent-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligent Agent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intelligent-digital-assistant-ida" class="md-nav__link">
    <span class="md-ellipsis">
      Intelligent Digital Assistant (IDA)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#intent-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Intent Analysis
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-planning" class="md-nav__link">
    <span class="md-ellipsis">
      Interactive Planning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#international-conference-on-learning-representations-iclr-conference" class="md-nav__link">
    <span class="md-ellipsis">
      International Conference on Learning Representations (ICLR) Conference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#international-conference-on-machine-learning-icml-conference" class="md-nav__link">
    <span class="md-ellipsis">
      International Conference on Machine Learning (ICML) Conference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interpretml" class="md-nav__link">
    <span class="md-ellipsis">
      InterpretML
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverse-document-frequency-idf" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse Document Frequency (IDF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverse-dynamics-model-idm" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse Dynamics Model (IDM)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverse-q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse Q-Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverse-rl-irl" class="md-nav__link">
    <span class="md-ellipsis">
      Inverse RL (IRL)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inverted-file-index-ivd" class="md-nav__link">
    <span class="md-ellipsis">
      Inverted File Index (IVD)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iq-learn-model" class="md-nav__link">
    <span class="md-ellipsis">
      IQ-Learn Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#isaac-gym-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Isaac Gym Environment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iso-42001-standard" class="md-nav__link">
    <span class="md-ellipsis">
      ISO 42001 Standard
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#isolation-forest-if" class="md-nav__link">
    <span class="md-ellipsis">
      Isolation Forest (IF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#isomorphic-labs-company" class="md-nav__link">
    <span class="md-ellipsis">
      Isomorphic Labs Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#istio" class="md-nav__link">
    <span class="md-ellipsis">
      Istio
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iteration" class="md-nav__link">
    <span class="md-ellipsis">
      Iteration
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="i">I<a class="headerlink" href="#i" title="Permanent link">#</a></h1>
<h2 id="ibm-company">IBM Company<a class="headerlink" href="#ibm-company" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, <a href="../c/#company">Company</a></p>
<h2 id="ibm-watson">IBM Watson<a class="headerlink" href="#ibm-watson" title="Permanent link">#</a></h2>
<p>IBM Watson is a question-answering computer system capable of answering questions posed in natural language, developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci. Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.</p>
<p>The computer system was initially developed to answer questions on the quiz show Jeopardy! and, in 2011, the Watson computer system competed on Jeopardy! against champions Brad Rutter and Ken Jennings, winning the first place prize of $1 million.</p>
<iframe src="https://www.youtube.com/embed/P18EdAKuC1U" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/b2M-SeKey4o" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>site - <a href="https://www.ibm.com/watson">https://www.ibm.com/watson</a></li>
<li>wikipedia - <a href="https://en.wikipedia.org/wiki/IBM_Watson">https://en.wikipedia.org/wiki/IBM_Watson</a></li>
</ul>
<p>See also <a href="./">I</a>, <a href="./#ibm-company">IBM</a></p>
<h2 id="ilya-sutskever-person">Ilya Sutskever Person<a class="headerlink" href="#ilya-sutskever-person" title="Permanent link">#</a></h2>
<p><a href="../o/#openai-company">OpenAI</a> Co-founder.</p>
<iframe src="https://www.youtube.com/embed/9wG56MdgmlE" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/UHSkjro-VbE" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">I</a>, <a href="../p/#people">People</a></p>
<h2 id="image-analysis">Image Analysis<a class="headerlink" href="#image-analysis" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, [Amazon Recognition]</p>
<h2 id="image-classifier">Image Classifier<a class="headerlink" href="#image-classifier" title="Permanent link">#</a></h2>
<p>A component that does image classification.</p>
<p>See also <a href="./">I</a>, <a href="./#image-classification">Image Classification</a></p>
<h2 id="image-classification">Image Classification<a class="headerlink" href="#image-classification" title="Permanent link">#</a></h2>
<p><a href="../c/#convolutional-neural-network-cnn">Convolutional Neural Network</a> such as [ResNet]. Supervised algorithm.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a> <span class="c1"># Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s1">&#39;yolov8n-cls.pt&#39;</span><span class="p">)</span>  <span class="c1"># load a pretrained YOLOv8n classification model</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">&#39;mnist160&#39;</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># train the model</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">model</span><span class="p">(</span><span class="s1">&#39;https://ultralytics.com/images/bus.jpg&#39;</span><span class="p">)</span>  <span class="c1"># predict on an image</span>
</span></code></pre></div>
<iframe src="https://www.youtube.com/embed/NAs-cfq9BDw" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b">https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b</a></li>
<li>colab - <a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb">https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="image-compression">Image Compression<a class="headerlink" href="#image-compression" title="Permanent link">#</a></h2>
<p>A type of <a href="./#image-processing">image processing</a> used to improve the appearance of an image by adjusting brightness, contrast, or reducing noise.</p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="image-decoder">Image Decoder<a class="headerlink" href="#image-decoder" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, <a href="../d/#decoder">Decoder</a>, <a href="./#image-encoder">Image Encoder</a></p>
<h2 id="image-encoder">Image Encoder<a class="headerlink" href="#image-encoder" title="Permanent link">#</a></h2>
<p>~ data compression where the encoder compress the data (not always) Ex: 2 images of lions. Instead of a comparing pixel to pixel, first encode the image to extract similarities and then compare the similarities. The translation from pixels to similarities is done by an encoder. First, let’s call encoder the process that produce the “new features” representation from the “old features” representation (by selection or by extraction) and decoder the reverse process. Dimensionality reduction can then be interpreted as data compression where the encoder compress the data (from the initial space to the encoded space, also called latent space) whereas the decoder decompress them. Of course, depending on the initial data distribution, the latent space dimension and the encoder definition, this compression/representation can be lossy, meaning that a part of the information is lost during the encoding process and cannot be recovered when decoding.</p>
<p>See also <a href="./">I</a>, <a href="../e/#encoder">Encoder</a>, <a href="./#image-decoder">Image Decoder</a></p>
<h2 id="image-enhancement">Image Enhancement<a class="headerlink" href="#image-enhancement" title="Permanent link">#</a></h2>
<p>A type of <a href="./#image-processing">image processing</a> used to improve the appearance of an image by adjusting brightness, contrast, or reducing noise.</p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="image-filter">Image Filter<a class="headerlink" href="#image-filter" title="Permanent link">#</a></h2>
<p>~ in a <a href="../c/#convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</a>, this is a <a href="../f/#feature">feature</a> detector!</p>
<details class="question">
<summary>Is an image filter related to a <a href="../g/#graphical-processing-unit-gpu-kernel">GPU kernel</a> ?</summary>
<div class="language-text highlight"><pre><span></span><code>Yes!
</code></pre></div>
</details>
<p>A small matrix which you can use to multiply to the pixel area of the same size. The filter is applied to or convoluted with the same image for every area possible. As represented below, we can use a 2x2x1 filter, but we recommend a 3x3x1 filter.</p>
<p><img alt="" src="../img/i/image_filter.gif" width="100%" /></p>
<p>Where is the pattern of each filter coming from? Just like weights in a neural network, it comes from Backpropagation !</p>
<p>/// details | What are low, medium, and high frequency patterns?
    type:question</p>
<div class="language-text highlight"><pre><span></span><code>That seems to be referring to &#39;contrast&#39; or difference in values of the pixel in the kernel
</code></pre></div>
<p>///</p>
<p>Questions</p>
<ul>
<li>number of filters?<ul>
<li>that's a hyperparameter!</li>
<li>the output of a convolution of one filter with the input is called a <a href="../f/#feature-map">feature map</a></li>
<li>applying an <a href="../a/#activation-function">activation function</a> on the <a href="../f/#feature-map">feature maps</a> result in an <a href="../a/#activation-map">activation map</a></li>
</ul>
</li>
<li>filter size?<ul>
<li>3x3 as chained 3x3 (2 Conv2D) gives you the same 5x5 receptive field (or patch) as a single 5x5 convolution! --&gt; important because fewer weights/parameters = less/faster computation! ( 2 3x3 conv uses 72% of the param and computation of a 5x5 conv, 3 3x3 conv uses 55% of the params of a 7x7)</li>
<li>very first input conv can be &lt;&gt; because input only has 3 channels ==&gt; 5x5x3 or 7x7x3 on first layer</li>
<li>1x1 because it is the most easy way to change the number of features in the <a href="../f/#feature-map">feature map</a> (!?!?)</li>
<li>filters have a height, width, and a number of channels. The number of channels must match the number of channels of the input to be processed. That is why the number of channels of the filter is never specified as an input!!!</li>
</ul>
</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">keras</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Conv2D</span>     <span class="c1"># &lt;== 2D is how the filter moves!</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>                                    <span class="c1"># &lt;!&gt; 1D is used for time-series</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>                                    <span class="c1"># &lt;!&gt; 3D is used for videos or stacked images seen in medical imaging</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span> <span class="c1"># &lt;== filter is really 3x3x3!</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</span></code></pre></div>
<iframe src="https://www.youtube.com/embed/V9ZYDCnItr0" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://setosa.io/ev/image-kernels/">https://setosa.io/ev/image-kernels/</a></li>
<li><a href="https://medium.com/codex/kernels-filters-in-convolutional-neural-network-cnn-lets-talk-about-them-ee4e94f3319">https://medium.com/codex/kernels-filters-in-convolutional-neural-network-cnn-lets-talk-about-them-ee4e94f3319</a></li>
</ul>
<p>See also <a href="./">I</a>, <a href="../c/#convolutional-layer">Convolutional Layer</a></p>
<h2 id="image-generator">Image Generator<a class="headerlink" href="#image-generator" title="Permanent link">#</a></h2>
<p>AI image generators leverage advanced machine learning algorithms to transform text descriptions into images. These tools are trained on extensive datasets, allowing them to interpret prompts and create anything from simple illustrations to highly detailed, photorealistic scenes. The technology powering these generators often includes neural networks like <a href="../g/#generative-adversarial-network-gan">Generative Adversarial Networks (GANs)</a> or <a href="../d/#diffusion-model-dm">Diffusion Models</a>.</p>
<p>More at:</p>
<ul>
<li>DALL-E - <a href="https://openai.com/index/dall-e-3/">https://openai.com/index/dall-e-3/</a> </li>
<li>Flux - <a href="https://aitubo.ai/flux-image-generator/">https://aitubo.ai/flux-image-generator/</a> </li>
<li>Ideogram - <a href="https://ideogram.ai/">https://ideogram.ai/</a></li>
<li>midjourney - <a href="https://www.midjourney.com/home">https://www.midjourney.com/home</a></li>
<li>openArt - <a href="https://openart.ai/home">https://openart.ai/home</a></li>
<li>unstability.ai - <a href="https://www.unstability.ai/history">https://www.unstability.ai/history</a></li>
<li>Stable diffusion - <a href="https://www.diffus.me/">https://www.diffus.me/</a></li>
<li>articles<ul>
<li><a href="https://anakin.ai/blog/flux-midjourney-dalle-stable-diffusion-comparison/">https://anakin.ai/blog/flux-midjourney-dalle-stable-diffusion-comparison/</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="image-joint-embedding-predictive-architecture-i-jepa">Image Joint-Embedding Predictive Architecture (I-JEPA)<a class="headerlink" href="#image-joint-embedding-predictive-architecture-i-jepa" title="Permanent link">#</a></h2>
<p>A method for <a href="../j/#joint-embedding-predictive-architecture-jepa">Joint-Embedding Predictive Architecture (JEPA)</a> based on image</p>
<object data="https://arxiv.org/pdf/2301.08243" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2301.08243">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li><a href="https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/">https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/</a></li>
<li>paper - <a href="https://arxiv.org/abs/2301.08243">https://arxiv.org/abs/2301.08243</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="image-inpainting">Image Inpainting<a class="headerlink" href="#image-inpainting" title="Permanent link">#</a></h2>
<p>Masking of an area of an image and having it reconstructed by going through an autoencoder.</p>
<p>See also <a href="./">I</a>, [Masked Language Learning Model]</p>
<h2 id="image-processing">Image Processing<a class="headerlink" href="#image-processing" title="Permanent link">#</a></h2>
<p>Image processing involves various techniques to manipulate and analyze images to enhance their quality, extract useful information, or prepare them for further analysis or computer vision tasks. This field is widely used in applications like medical imaging, facial recognition, computer graphics, and digital photography.</p>
<p>The primary goals in image processing include:</p>
<ol>
<li><a href="./#image-enhancement">Image Enhancement</a> - Improving the appearance of an image by adjusting brightness, contrast, or reducing noise.</li>
<li><a href="./#image-restoration">Image Restoration</a> - Removing distortions or artifacts (like blurring) that may have occurred during the image capture process.</li>
<li><a href="./#image-segmentation">Image Segmentation</a> - Dividing an image into distinct regions or objects (at the pixel level) to make it easier to analyze specific areas.</li>
<li><a href="../o/#object-detection">Object Detection</a> and <a href="../o/#object-recognition">Object Recognition</a> - Identifying specific objects, patterns, or features within an image.</li>
<li><a href="./#image-compression">Image Compression</a> - Reducing the file size for storage and transmission without compromising image quality significantly.</li>
</ol>
<p>Techniques in image processing range from basic filters and transformations to complex algorithms involving machine learning and deep learning, especially for tasks like recognition and classification.</p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="image-reconstruction">Image Reconstruction<a class="headerlink" href="#image-reconstruction" title="Permanent link">#</a></h2>
<p><img alt="" src="../img/i/image_reconstruction.png" width="100%" /></p>
<p>Above is a pipeline for image reconstruction. The input image is fed to Flamingo/BLIP to generate a caption, which is fed to DALL-E/SD to reconstruct an image. The generated image is compared with the input image using the CLIP image encoder in the embedding space. Each input image has human-annotated captions which can be used to evaluate the generated caption.</p>
<p>See also <a href="./">I</a>, <a href="../b/#blip-model">BLIP Model</a>, <a href="../c/#clip-image-encoder">CLIP Image Encoder</a>, <a href="../t/#text-reconstruction">Text Reconstruction</a></p>
<h2 id="image-restoration">Image Restoration<a class="headerlink" href="#image-restoration" title="Permanent link">#</a></h2>
<p>A type of <a href="./#image-processing">image processing</a> used to remove distortions or artifacts (like blurring) that may have occurred during the image capture process.</p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="image-segmentation">Image Segmentation<a class="headerlink" href="#image-segmentation" title="Permanent link">#</a></h2>
<p>~ Dividing an image into distinct regions or objects (at the pixel level) to make it easier to analyze specific areas.</p>
<p>Image segmentation is a process of dividing an image into multiple segments or regions, each of which corresponds to a different object or part of the image. The goal of image segmentation is to simplify or change the representation of an image into something that is more meaningful and easier to analyze. It is a fundamental task in computer vision and is used in various applications, such as <a href="../o/#object-recognition">object recognition</a>, <a href="../o/#object-tracking">object tracking</a>, and image editing.</p>
<p>Image segmentation can be performed using a variety of methods, including <a href="../t/#thresholding">thresholding</a>, clustering, <a href="../e/#edge-detection">edge detection</a>, and machine learning algorithms. These methods typically involve grouping pixels or regions of pixels in an image based on similarities in color, texture, or other visual features.</p>
<p>The output of image segmentation is a set of labeled regions that can be used for further processing or analysis. For example, in object recognition, the segmented regions can be used to identify and classify objects in an image. In medical imaging, image segmentation can be used to identify and isolate specific structures or organs in a patient's body.</p>
<p>Overall, image segmentation is an essential step in many computer vision applications and is an active area of research in the field.</p>
<p>Algorithms:</p>
<ul>
<li><a href="../a/#alexnet-model">AlexNet</a></li>
<li>Faster <a href="../r/#region-based-cnn-r-cnn">R-CNN</a></li>
<li>Mask <a href="../r/#region-based-cnn-r-cnn">R-CNN</a></li>
<li>Panoptic FPN</li>
</ul>
<p>Models:</p>
<ul>
<li>[Segment Anything Model]</li>
</ul>
<p><img alt="" src="../img/i/image_segmentation_timeline.png" width="100%" /></p>
<p>More at:</p>
<ul>
<li>colab - <a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb">https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb</a></li>
</ul>
<p>See also <a href="./">I</a>, <a href="../o/#object-detection">Object Detection</a></p>
<h2 id="image-aware-decoder-enhanced-a-la-flamingo-with-interleaved-cross-attentions-idefics-model">Image-aware Decoder Enhanced a la Flamingo with Interleaved Cross-attentionS (IDEFICS) Model<a class="headerlink" href="#image-aware-decoder-enhanced-a-la-flamingo-with-interleaved-cross-attentions-idefics-model" title="Permanent link">#</a></h2>
<p>IDEFICS is based on <a href="../f/#flamingo-model">Flamingo</a>, a state-of-the-art visual language model initially developed by <a href="../d/#deepmind-company">DeepMind</a>, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs. IDEFICS is built solely on publicly available data and models (<a href="../l/#large-language-model-meta-ai-llama-model-family">LLaMA</a> v1 and OpenCLIP) and comes in two variants—the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes.</p>
<p>More at:
  * site - <a href="https://huggingface.co/HuggingFaceM4/idefics-80b-instruct">https://huggingface.co/HuggingFaceM4/idefics-80b-instruct</a>
  * articles
    * <a href="https://huggingface.co/blog/idefics">https://huggingface.co/blog/idefics</a></p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="imagen-model-family">Imagen Model Family<a class="headerlink" href="#imagen-model-family" title="Permanent link">#</a></h2>
<p>Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model.</p>
<iframe src="https://www.youtube.com/embed/qhtYPhPWCsI" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<object data="https://arxiv.org/pdf/2205.11487" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2205.11487">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>site - <a href="https://imagen.research.google/">https://imagen.research.google/</a></li>
<li>paper - <a href="https://arxiv.org/abs/2205.11487">https://arxiv.org/abs/2205.11487</a></li>
<li>articles<ul>
<li><a href="https://www.louisbouchard.ai/google-brain-imagen/">https://www.louisbouchard.ai/google-brain-imagen/</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">I</a>, [Latent Diffusion Model]</p>
<h2 id="imagen-video-model">Imagen Video Model<a class="headerlink" href="#imagen-video-model" title="Permanent link">#</a></h2>
<p>Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. </p>
<object data="https://arxiv.org/pdf/2210.02303" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2210.02303">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>site - <a href="https://imagen.research.google/video/">https://imagen.research.google/video/</a></li>
<li>paper - <a href="https://arxiv.org/abs/2210.02303">https://arxiv.org/abs/2210.02303</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="imagenet-dataset">ImageNet Dataset<a class="headerlink" href="#imagenet-dataset" title="Permanent link">#</a></h2>
<p>~ 1.2 million images with 1000 label each (= used for supervised learning, or not!) ImageNet is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a "synonym set" or "synset". There are more than 100,000 synsets in WordNet; the majority of them are nouns (80,000+). In ImageNet, we aim to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, we hope ImageNet will offer tens of millions of cleanly labeled and sorted images for most of the concepts in the WordNet hierarchy. The ImageNet project was inspired by two important needs in computer vision research. The first was the need to establish a clear North Star problem in computer vision. While the field enjoyed an abundance of important tasks to work on, from stereo vision to image retrieval, from 3D reconstruction to image segmentation, object categorization was recognized to be one of the most fundamental capabilities of both human and machine vision. Hence there was a growing demand for a high quality object categorization benchmark with clearly established evaluation metrics. Second, there was a critical need for more data to enable more generalizable machine learning methods. Ever since the birth of the digital era and the availability of web-scale data exchanges, researchers in these fields have been working hard to design more and more sophisticated algorithms to index, retrieve, organize and annotate multimedia data. But good research requires good resources. To tackle this problem at scale (think of your growing personal collection of digital images, or videos, or a commercial web search engine’s database), it was critical to provide researchers with a large-scale image database for both training and testing. The convergence of these two intellectual reasons motivated us to build ImageNet.</p>
<iframe src="https://www.youtube.com/embed/c_u4AHNjOpk" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>wikiepedia - <a href="https://en.wikipedia.org/wiki/ImageNet">https://en.wikipedia.org/wiki/ImageNet</a></li>
<li><a href="https://image-net.org/challenges/LSVRC/index.php">https://image-net.org/challenges/LSVRC/index.php</a></li>
<li>know-your-data - <a href="https://knowyourdata-tfds.withgoogle.com/#tab=STATS&amp;dataset=imagenet2012">https://knowyourdata-tfds.withgoogle.com/#tab=STATS&amp;dataset=imagenet2012</a></li>
</ul>
<p>See also <a href="./">I</a>, <a href="../a/#alexnet-model">AlexNet Model</a>, <a href="../f/#fei-fei-li-person">Fei-Fei Li Person</a>, <a href="../s/#supervised-learning">Supervised Learning</a>, <a href="../t/#transfer-learning">Transfer Learning</a>, <a href="../w/#wordnet-dataset">WordNet Dataset</a></p>
<h2 id="imagenet-large-scale-visual-recognition-ilsvrc-challenge">ImageNet Large Scale Visual Recognition (ILSVRC) Challenge<a class="headerlink" href="#imagenet-large-scale-visual-recognition-ilsvrc-challenge" title="Permanent link">#</a></h2>
<p>The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.</p>
<object data="https://arxiv.org/pdf/1409.0575" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/1409.0575">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>paper - <a href="https://arxiv.org/abs/1409.0575">https://arxiv.org/abs/1409.0575</a></li>
</ul>
<p>See also <a href="./">I</a>, <a href="../a/#alexnet-model">AlexNet Model</a> </p>
<h2 id="imitation-learning-il">Imitation Learning (IL)<a class="headerlink" href="#imitation-learning-il" title="Permanent link">#</a></h2>
<p>In imitation learning (IL), an agent is given access to samples of expert behavior (e.g. videos of humans playing online games or cars driving on the road) and it tries to learn a policy that mimics this behavior. This objective is in contrast to reinforcement learning (RL), where the goal is to learn a policy that maximizes a specified reward function. A major advantage of imitation learning is that it does not require careful hand-design of a reward function because it relies solely on expert behavior data, making it easier to scale to real-world tasks where one is able to gather expert behavior (like video games or driving). This approach of enabling the development of AI systems by data-driven learning, rather than specification through code or heuristic rewards, is consistent with the key principles behind Software 2.0.</p>
<iframe src="https://www.youtube.com/embed/gzpe60OtC_E" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://ai.stanford.edu/blog/learning-to-imitate/">https://ai.stanford.edu/blog/learning-to-imitate/</a></li>
<li><a href="https://www.technologyreview.com/2022/11/25/1063707/ai-minecraft-video-unlock-next-big-thing-openai-imitation-learning/">https://www.technologyreview.com/2022/11/25/1063707/ai-minecraft-video-unlock-next-big-thing-openai-imitation-learning/</a> (blocked?)</li>
</ul>
<p>See also <a href="./">I</a>, <a href="../a/#adversarial-imitation-learning">Adversarial Imitation Learning</a>, [Behavioral Cloning], <a href="./#iq-learn-model">IQ-Learn Model</a>, <a href="../l/#learning-method">Learning Method</a>, <a href="../r/#reinforcement-learning-rl">Reinforcement Learning</a>, <a href="../s/#software-20">Software 2.0</a></p>
<h2 id="imbalanced-dataset">Imbalanced Dataset<a class="headerlink" href="#imbalanced-dataset" title="Permanent link">#</a></h2>
<p>Imbalanced data refers to a situation in which the distribution of classes in a dataset is not equal. In a binary classification problem, where there are two classes (positive and negative), imbalanced data occurs when one class significantly outnumbers the other. This imbalance can lead to challenges when training machine learning models, as the model may become biased towards the majority class and perform poorly on the minority class.</p>
<p>For example, consider a medical diagnosis scenario where you are trying to predict whether a patient has a rare disease. If only a small percentage of the population has the disease, the dataset may be imbalanced, with the majority of examples belonging to the class of "non-disease" cases. In such cases, a model might achieve high <a href="../a/#accuracy">accuracy</a> by simply predicting the majority class for every instance, but it would fail to identify the minority class effectively.</p>
<p>Addressing imbalanced data is important because it can affect the performance of [machine learning] models. Various techniques can be employed to handle imbalanced datasets, including:</p>
<ul>
<li>Resampling: This involves either oversampling the minority class, undersampling the majority class, or a combination of both to create a more balanced dataset.</li>
<li>Synthetic Data Generation: Techniques such as SMOTE (Synthetic Minority Over-sampling Technique) involve generating synthetic examples of the minority class to balance the dataset.</li>
<li>Cost-sensitive learning: Assigning different misclassification costs to different classes to make the model more sensitive to errors on the minority class.</li>
<li><a href="../e/#ensemble-method">Ensemble Methods</a>: Using ensemble methods like Random Forests or boosting algorithms, which can be more robust to imbalanced data.</li>
<li>Different Evaluation Metrics: Instead of relying solely on <a href="../a/#accuracy">accuracy</a>, using metrics such as <a href="../p/#precision">precision</a>, <a href="../r/#recall">recall</a>, <a href="../f/#f1-score">F1 score</a>, or <a href="../a/#area-under-the-receiver-operating-characteristic-auroc-curve">Area Under the ROC (AUROC) curve</a> can provide a more comprehensive understanding of model performance on imbalanced datasets.</li>
</ul>
<p>It's crucial to carefully choose and implement these techniques based on the specific characteristics of the dataset and the goals of the machine learning task.</p>
<iframe src="https://www.youtube.com/embed/JnlM4yLFNuo" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">I</a>, ...</p>
<h2 id="impact-factor">Impact Factor<a class="headerlink" href="#impact-factor" title="Permanent link">#</a></h2>
<ul>
<li>of an <a href="../a/#ai-conference">AI conference</a></li>
<li>of an [AI publication]</li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="imputation">Imputation<a class="headerlink" href="#imputation" title="Permanent link">#</a></h2>
<p>A way to deal with missing/incomplete data. Instead of eliminating the data point, insert the average or another value to use the other attributes of the samples.
 &lt;!&gt; beware if you draw conclusion on imputed data!!</p>
<p>Imputed data increases uncertainty in the result. The question is how much? 5%, 10%, 50% ?</p>
<p>See also <a href="./">I</a>, <a href="../d/#data-point">Data Point</a></p>
<h2 id="incentive">Incentive<a class="headerlink" href="#incentive" title="Permanent link">#</a></h2>
<p>What is the incentive to reach the goal with shortest route? <a href="../r/#reward-shaping">Reward Shaping</a></p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="indexgpt-model">IndexGPT Model<a class="headerlink" href="#indexgpt-model" title="Permanent link">#</a></h2>
<p>More at:</p>
<ul>
<li>trademark - <a href="https://tsdr.uspto.gov/documentviewer?caseId=sn9793153">https://tsdr.uspto.gov/documentviewer?caseId=sn9793153</a></li>
<li>articles<ul>
<li><a href="https://www.cnbc.com/2023/05/25/jpmorgan-develops-ai-investment-advisor.html">https://www.cnbc.com/2023/05/25/jpmorgan-develops-ai-investment-advisor.html</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="inductive-bias">Inductive Bias<a class="headerlink" href="#inductive-bias" title="Permanent link">#</a></h2>
<p>~ bias coming from the assumptions made during <a href="./#inductive-reasoning">inductive reasoning</a></p>
<p>Inductive bias is fundamentally related to the [bias-variance tradeoff] because it influences the learning algorithm's tendency to [underfit] or [overfit] the data. A well-chosen inductive bias will help the model generalize well from the training data to unseen data by finding a good balance between <a href="../b/#bias">bias</a> and <a href="../v/#variance">variance</a>.</p>
<p>Examples:</p>
<ul>
<li><a href="../l/#linear-regression">Linear regression</a> - The inductive bias is that the target variable can be expressed as a linear combination of the input features. This is a strong assumption about the nature of the relationship between inputs and outputs.</li>
<li><a href="../d/#decision-tree">Decision tree</a> - The inductive bias is that the data can be segmented into smaller and smaller subsets based on feature values, often assuming that the data has a hierarchical structure.</li>
<li><a href="../a/#artificial-neural-network-ann">Artificial neural networks</a> - These have a more complex inductive bias, often assuming that real-world phenomena can be captured through layers of abstraction and representation.</li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="inductive-reasoning">Inductive Reasoning<a class="headerlink" href="#inductive-reasoning" title="Permanent link">#</a></h2>
<p>~ figuring out patterns</p>
<p>Coming up with <code>rules to explain the current observation</code>. Sometimes the truth can be learned ;-)</p>
<p>Type of inductive reasoning:</p>
<ul>
<li>Generalized induction: From observation, you infer a general rule.</li>
<li>Statistical induction: Based on the frequency of an occurrence in sampled instances, you infer the probability of that occurrence in general. For example, if 9 out of 10 sampled apples are red, you might infer a high probability that the next apple you see will be red.</li>
<li>Causal inference: Observing a consistent association between two events and inferring a causal relationship. For example, noticing that the ground is wet every time it rains and inferring that rain causes the ground to become wet.</li>
<li>Predictive induction: Observing a pattern or trend and predicting that it will continue. For example, observing that a company's stock has risen in the past few hours and predicting that it will keep rising in the next hour.</li>
</ul>
<p>See also <a href="./">I</a>, <a href="../a/#abductive-reasoning">Abductive Reasoning</a>, <a href="../d/#deductive-reasoning">Deductive Reasoning</a>, <a href="../t/#truth">Truth</a></p>
<h2 id="industrial-robot">Industrial Robot<a class="headerlink" href="#industrial-robot" title="Permanent link">#</a></h2>
<p>A kind of <a href="../r/#robot">robots</a> that ...</p>
<p>See also <a href="../r/">R</a>, ...</p>
<h2 id="inertial-measurement-unit-imu">Inertial Measurement Unit (IMU)<a class="headerlink" href="#inertial-measurement-unit-imu" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, <a href="../s/#simultaneous-localization-and-mapping-slam-algorithm">SLAM Algorithm</a></p>
<h2 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">#</a></h2>
<p>An inference means running your machine learning model on new data). A prediction/action/complex plan that is devised/based on acquired knowledge. That is based on deductive reasoning (sherlock holmes!).</p>
<p>See also <a href="./">I</a>, <a href="./#inference-point">Inference Point</a></p>
<h2 id="inference-configuration-parameter">Inference Configuration Parameter<a class="headerlink" href="#inference-configuration-parameter" title="Permanent link">#</a></h2>
<ul>
<li>Max New Token</li>
<li><a href="../t/#top-k-random-sampling">Top-K Random Sampling</a></li>
<li><a href="../t/#top-p-random-sampling">Top-P Random Sampling</a></li>
<li><a href="../t/#temperature">Temperature</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="inference-point">Inference Point<a class="headerlink" href="#inference-point" title="Permanent link">#</a></h2>
<p>An endpoint to connect to behind which your model is running.</p>
<p>See also <a href="./">I</a>, <a href="../m/#model">Model</a></p>
<h2 id="inflection-ai-company">Inflection AI Company<a class="headerlink" href="#inflection-ai-company" title="Permanent link">#</a></h2>
<p>A start-up <a href="../c/#company">company</a> that is launching Pi, a chatbot as a personal assistant. The Pi name comes from "Personal Intelligence".</p>
<p>More at:</p>
<ul>
<li>home - <a href="https://inflection.ai/">https://inflection.ai/</a></li>
<li>hey pi - <a href="https://heypi.com/talk">https://heypi.com/talk</a></li>
<li>articles<ul>
<li>v1 - <a href="https://www.forbes.com/sites/alexkonrad/2023/05/02/inflection-ai-ex-deepmind-launches-pi-chatbot/?sh=a14f4343d6dd">https://www.forbes.com/sites/alexkonrad/2023/05/02/inflection-ai-ex-deepmind-launches-pi-chatbot/?sh=a14f4343d6dd</a></li>
<li>v2.5 - <a href="https://venturebeat.com/ai/inflection-ai-launches-new-model-for-pi-chatbot-nearly-matches-gpt-4/">https://venturebeat.com/ai/inflection-ai-launches-new-model-for-pi-chatbot-nearly-matches-gpt-4/</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="informal-reasoning">Informal Reasoning<a class="headerlink" href="#informal-reasoning" title="Permanent link">#</a></h2>
<p>Informal Reasoning is a less structured approach to <a href="../r/#reasoning">reasoning</a> that relies on intuition, experience, and common sense. It is used in everyday life situations where strict formal rules may not apply. Informal reasoning allows for more flexibility and open-ended thinking. It often involves making decisions or drawing conclusions based on personal experiences, heuristics, and contextual factors. Informal reasoning is more adaptable but may also be less reliable compared to formal reasoning.</p>
<p>More at:</p>
<ul>
<li>LLM reasoning ability - <a href="https://www.kaggle.com/code/flaussy/large-language-models-reasoning-ability">https://www.kaggle.com/code/flaussy/large-language-models-reasoning-ability</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="information">Information<a class="headerlink" href="#information" title="Permanent link">#</a></h2>
<div class="language-text highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>Data &lt; Information &lt; Knowledge &lt; Use knowledge &lt; Mastery
</span></code></pre></div>
<p>See also <a href="./">I</a>, <a href="../d/#data">Data</a>, <a href="../k/#knowledge">Knowledge</a></p>
<h2 id="information-retrieval-ir">Information Retrieval (IR)<a class="headerlink" href="#information-retrieval-ir" title="Permanent link">#</a></h2>
<p>Information Retrieval (IR) is the process of obtaining relevant information or documents from a large collection of data in response to a user's query or information need. The goal of information retrieval is to effectively locate and present information that is most relevant to the user's request, enabling users to find the information they are looking for efficiently and accurately.</p>
<p>The main components of an information retrieval system include:</p>
<ul>
<li>User Query: The user provides a query, which is a set of keywords, phrases, or questions that describe their information need. This query is used to search for relevant documents.</li>
<li>Document Collection: This is the set of documents or data that the information retrieval system searches through. Documents can be text, images, audio, video, or any other type of data.</li>
<li>Indexing: To speed up the retrieval process, an index is created from the document collection. The index contains information about the terms (words or phrases) present in the documents and their locations.</li>
<li>Ranking: When a user submits a query, the information retrieval system retrieves documents that are relevant to the query. These documents are ranked based on their relevance to the query. The ranking is typically done using various algorithms that consider factors like term frequency, document frequency, and other relevance metrics.</li>
<li>Retrieval: The system retrieves a set of documents that are considered relevant to the user's query, based on the ranking process.</li>
<li>Presentation: The retrieved documents are presented to the user in a way that makes it easy for them to review and select the information they are interested in. This can involve displaying snippets of text, document titles, and other relevant information.</li>
</ul>
<p>Information retrieval systems are used in various applications, including:
  * Search Engines: Web search engines like Google, Bing, and Yahoo use information retrieval techniques to provide users with relevant search results from the vast amount of content available on the internet.
  * Document Management Systems: Organizations use information retrieval systems to manage and retrieve documents from their internal databases.
  * Digital Libraries: Libraries and archives use IR systems to help users find books, articles, and other resources.
  * Recommendation Systems: E-commerce platforms and streaming services use IR to recommend products, movies, music, and other content to users based on their preferences and behavior.
  * Question Answering Systems: IR is used to find relevant answers to user questions, either by searching for relevant documents or by generating answers directly.</p>
<p>Information retrieval is an essential component of modern technology, enabling users to access and make sense of the vast amount of information available in digital form.</p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="initialization">Initialization<a class="headerlink" href="#initialization" title="Permanent link">#</a></h2>
<p>Initialization (of clustering algorithm)</p>
<p>See also <a href="./">I</a>, <a href="../h/#hyperparameter">Hyperparameter</a></p>
<h2 id="inlier">Inlier<a class="headerlink" href="#inlier" title="Permanent link">#</a></h2>
<p>The opposite of <a href="../o/#outlier">Outlier</a>. Check the <a href="../r/#random-sample-consensus-ransac-algorithm">RANSAC Algorithm</a> for separation!</p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="input-layer">Input Layer<a class="headerlink" href="#input-layer" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, <a href="../a/#artificial-neural-network-ann">Artificial Neural Network</a>, <a href="../d/#dropout-layer">Dropout Layer</a>, <a href="../f/#feature">Feature</a></p>
<h2 id="input-space">Input Space<a class="headerlink" href="#input-space" title="Permanent link">#</a></h2>
<p>Ex: raw pixel values. After training, the last layer of the model has captured the important patterns of the input that are needed for the image classification task. In the latent space, images that depict the same object have very close representations. Generally, the distance of the vectors in the latent space corresponds to the semantic similarity of the raw images. Below, we can see how the latent space of an animal classification model may seem. The green points correspond to the latent vector of each image extracted from the last layer of the model. We observe that vectors of the same animals are closer to the latent space. Therefore, it is easier for the model to classify the input images using these feature vectors instead of the raw pixel values:</p>
<p><img alt="" src="../img/i/input_space.png" width="100%" /></p>
<p>See also <a href="./">I</a>, <a href="../e/#encoder">Encoder</a>, <a href="../l/#latent-space">Latent Space</a>, <a href="../l/#latent-vector">Latent Vector</a>, [Word Embedded Space]</p>
<h2 id="input-weight">Input Weight<a class="headerlink" href="#input-weight" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, <a href="../a/#artificial-neuron">Artificial Neuron</a>, <a href="../b/#backpropagation">Backpropagation</a></p>
<h2 id="instance-segmentation">Instance Segmentation<a class="headerlink" href="#instance-segmentation" title="Permanent link">#</a></h2>
<p>Along with pixel level classification, we expect the computer to classify each instance of class separately. It is called instance segmentation.That is different instances of the same class are segmented individually in instance segmentation. Once an instance is given a name, it becomes an entity!</p>
<p><img alt="" src="../img/i/instance_and_semantic_segmentation.png" width="100%" /></p>
<p>More at:</p>
<ul>
<li><a href="https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b">https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b</a></li>
</ul>
<p>See also <a href="./">I</a>, [Convoluted Neural Network], <a href="../e/#entity-extraction">Entity Extraction</a>, <a href="../s/#semantic-segmentation">Semantic Segmentation</a>, <a href="../u/#u-net-architecture">U-Net Architecture</a></p>
<h2 id="instructgpt-model">InstructGPT Model<a class="headerlink" href="#instructgpt-model" title="Permanent link">#</a></h2>
<p>A model that is a pre-trained GPT model and is fine tuned using reinforcement learning based on human feedback. A precursor of the <a href="../c/#chatgpt-model">ChatGPT model</a>. <a href="../l/#large-language-model-llm">Large language models</a> like GPT-3 are often used to follow instructions to execute user’s tasks. However, quite often, these models generate toxic or untruthful outputs that are not related to the input instructions. This is mostly due to the fact that models like GPT-3 are trained to predict the next word in a sentence rather than to execute a specific task. This is precisesly the problem OpenAI tried to address with InstructGPT, a language model that builds upon GPT-3 language capabilities but improves it its ability to follow instructions.</p>
<object data="https://arxiv.org/pdf/2203.02155" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2203.02155">Download PDF</a>.
    </p>
</object>

<p>The InstructGPT is build in three steps.</p>
<ol>
<li>The first step fine-tunes pretrained GPT-3 using 13k dataset. This dataset is from two sources:<ol>
<li>The team hired human labelers, who were asked to write and answer prompts — think NLP tasks. For example the human labeler was tasked to create an instruction and then multiple query &amp; response pairs for it.</li>
<li>The prompts by the end users in the Open.ai API, Playground. These prompts included various NLP tasks — text generation, Q&amp;A, summarization etc.
   Supervised learning is used for the fine-tuning of the pretrained GPT-3. The dataset includes both inputs, but as well corresponding human labeled output.</li>
</ol>
</li>
<li>The second step and third step rely on reinforcement learning. Let’s first review the second step — the reward model.<ul>
<li>The reward model is trained with 50k additional prompts. Prompt and multiple model outputs are generated. Model outputs are ranked by human from best to worse. The reward model is then trained to predict the human preferred output.</li>
<li>The third step is to optimize the policy using the reward model with 31k dataset. The data is purely from the Playground tool without any labeler written prompts. Therefore it differs from the first two steps.</li>
</ul>
</li>
</ol>
<p>A prompt is generated. An output is generated by the policy. Reward is given for the output based on the reward model. The achieved reward is then used to optimize the policy using <a href="../p/#proximal-policy-optimization-ppo-algorithm">PPO algorithm</a>.</p>
<p><img alt="" src="../img/i/instructgpt_model.png" width="100%" /></p>
<p>There is a difference between the way the GPT-3 and the InstructGPT generate outputs. GPT-3 was designed to predict next token. This is important to keep in mind. Despite GPT-3 is able to predict the next word — the output could be unhelpful. Think for example toxic speech in end-user application. The misalignment refers in NLP — to the issue of outputs not matching user’s intent. <code>The InstructGPT is fine-tuned to human preference using reinforcement learning</code>. This means, that rather than just predicting next token, it tries instead to respond with an output — preferred by human labeler. The InstructGPT model is optimized differently from the GPT-3. It rewards human preference. Therefore it is better able to solve user tasks.</p>
<p>More at:</p>
<ul>
<li>paper - <a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></li>
<li>announcement - <a href="https://openai.com/research/instruction-following">https://openai.com/research/instruction-following</a></li>
<li>model card - <a href="https://github.com/openai/following-instructions-human-feedback">https://github.com/openai/following-instructions-human-feedback</a></li>
<li>articles<ul>
<li>blog post - <a href="https://tmmtt.medium.com/the-instructgpt-e25797d8f4df">https://tmmtt.medium.com/the-instructgpt-e25797d8f4df</a></li>
<li>gpt vs chatgpt vs instructgpt - <a href="https://medium.com/@colin.fraser/chatgpt-automatic-expensive-bs-at-scale-a113692b13d5">https://medium.com/@colin.fraser/chatgpt-automatic-expensive-bs-at-scale-a113692b13d5</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">I</a>, <a href="../c/#chatgpt-model">ChatGPT Model</a>, <a href="../d/#digital-watermark">Digital Watermark</a>, <a href="../g/#generative-pre-trained-transformer-gpt-model-family">GPT Model</a>, <a href="../r/#reinforcement-learning-rl">Reinforcement Learning</a>, [Reinforcement Learning Human Feedback], <a href="../r/#reward-model">Reward Model</a></p>
<h2 id="instruction-tuned-it-model">Instruction Tuned (IT) Model<a class="headerlink" href="#instruction-tuned-it-model" title="Permanent link">#</a></h2>
<p>These versions of the model are trained with human language interactions and can respond to conversational input, similar to a chat bot.</p>
<ul>
<li>Flan-T5, a <a href="../t/#text-to-text-transfer-transformer-t5-model-family">T5 model</a></li>
<li>InstructGPT, a <a href="../g/#generative-pre-trained-transformer-gpt-model-family">GPT model</a></li>
</ul>
<p>See also <a href="./">I</a>, <a href="../p/#pretrained-model">Pretrained Model</a></p>
<h2 id="insufficient-data-algorithm">Insufficient Data Algorithm<a class="headerlink" href="#insufficient-data-algorithm" title="Permanent link">#</a></h2>
<p>Algorithms to deal with small datasets.</p>
<p>See also <a href="./">I</a>, <a href="../d/#data-augmentation">Data Augmentation</a>, [Graph Neural Network], [Meta Learning], [Multi-Task Leaning], <a href="../o/#one-shot-learning">One-Shot Learning</a>, <a href="../t/#transfer-learning">Transfer Learning</a>, <a href="../z/#zero-shot-learning">Zero-Shot Learning</a></p>
<h2 id="intel-company">Intel Company<a class="headerlink" href="#intel-company" title="Permanent link">#</a></h2>
<p>A <a href="../c/#company">company</a> that builds CPU and is about to integrate GPU/tensor computing in it.</p>
<p>{% youtube "<a href="https://youtu.be/PKfJmi83VjA?si=L23msCC8FiMuMC0P">https://youtu.be/PKfJmi83VjA?si=L23msCC8FiMuMC0P</a>" %}</p>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="intelligence">Intelligence<a class="headerlink" href="#intelligence" title="Permanent link">#</a></h2>
<p>(Prompt) What is intelligence?</p>
<p>[ChatGPT Answer] :</p>
<ul>
<li>Intelligence refers to the ability to learn, understand, and make judgments or have opinions that are reasonable.</li>
<li>It can also refer to the ability to acquire and apply knowledge and skills.</li>
<li>It is a complex trait that is influenced by both genetic and environmental factors.</li>
</ul>
<p><img alt="" src="../img/i/intelligence.png" width="100%" /></p>
<p>See also <a href="./">I</a>, <a href="../a/#artificial-intelligence-ai">Artificial Intelligence</a></p>
<h2 id="intelligence-augmentation">Intelligence Augmentation<a class="headerlink" href="#intelligence-augmentation" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="intelligence-explosion">Intelligence Explosion<a class="headerlink" href="#intelligence-explosion" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="intelligent-agent">Intelligent Agent<a class="headerlink" href="#intelligent-agent" title="Permanent link">#</a></h2>
<p>Intelligent agents are a specialized subset of autonomous agents. They differentiate themselves by incorporating learning and adaptation into their decision-making processes, enabling them to improve performance over time. Intelligent agents use data to refine their actions, allowing them to solve novel or complex problems that require more than rigid, rule-based approaches.</p>
<p>While all intelligent agents are autonomous, not all autonomous agents are intelligent. Some operate based on pre-defined, rigid rules without learning or adapting. Similarly, not all intelligent agents are rational – an agent may learn and adapt but still not make the most optimal decisions due to imperfect information or computational constraints. Rational agents strive to make the best decisions within the limits of their knowledge and capabilities.</p>
<p>More at:</p>
<ul>
<li><a href="https://www.turingpost.com/p/agentsvocabulary">https://www.turingpost.com/p/agentsvocabulary</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="intelligent-digital-assistant-ida">Intelligent Digital Assistant (IDA)<a class="headerlink" href="#intelligent-digital-assistant-ida" title="Permanent link">#</a></h2>
<object data="https://5691986.fs1.hubspotusercontent-na1.net/hubfs/5691986/Cornerstone%20Report/CORNERSTONE%20-%20KASISTO%20Chatbot%20Journey.pdf" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://5691986.fs1.hubspotusercontent-na1.net/hubfs/5691986/Cornerstone%20Report/CORNERSTONE%20-%20KASISTO%20Chatbot%20Journey.pdf">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>source - <a href="https://pages.kasisto.com/cornerstone-report?submissionGuid=8401e085-4f2d-46ed-8bf4-cb9c5afe4046">https://pages.kasisto.com/cornerstone-report?submissionGuid=8401e085-4f2d-46ed-8bf4-cb9c5afe4046</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="intent-analysis">Intent Analysis<a class="headerlink" href="#intent-analysis" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, <a href="../a/#aws-lex-service">AWS Lex</a></p>
<h2 id="interactive-planning">Interactive Planning<a class="headerlink" href="#interactive-planning" title="Permanent link">#</a></h2>
<p>Used for the completion of an agent's task in an environment , such as <a href="../o/#open-world-environment">Open-World</a></p>
<p>Decision</p>
<ul>
<li><a href="../d/#describe-explain-plan-select-deps-prompting">DEPS</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="international-conference-on-learning-representations-iclr-conference">International Conference on Learning Representations (ICLR) Conference<a class="headerlink" href="#international-conference-on-learning-representations-iclr-conference" title="Permanent link">#</a></h2>
<p>The International Conference on Learning Representations (ICLR) is a machine learning conference typically held in late April or early May each year. The conference includes invited talks as well as oral and poster presentations of refereed papers. Since its inception in 2013, ICLR has employed an open peer review process to referee paper submissions (based on models proposed by <a href="../y/#yann-lecun-person">Yann LeCun</a>). In 2019, there were 1591 paper submissions, of which 500 accepted with poster presentations (31%) and 24 with oral presentations (1.5%). In 2021, there were 2997 paper submissions, of which 860 were accepted (29%).</p>
<p>More at:</p>
<ul>
<li><a href="https://iclr.cc/">https://iclr.cc/</a></li>
<li><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations">https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="international-conference-on-machine-learning-icml-conference">International Conference on Machine Learning (ICML) Conference<a class="headerlink" href="#international-conference-on-machine-learning-icml-conference" title="Permanent link">#</a></h2>
<p>The International Conference on Machine Learning (ICML) is the leading international academic <a href="../a/#ai-conference">conference</a> in [machine learning]. Along with <a href="../n/#neural-information-processing-systems-neurips-conference">NeurIPS</a> and <a href="./#international-conference-on-learning-representations-iclr-conference">ICLR</a>, it is one of the three primary conferences of high impact in machine learning and artificial intelligence research.</p>
<p>More at:</p>
<ul>
<li>Home - <a href="https://icml.cc/">https://icml.cc/</a></li>
<li><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning">https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="interpretml">InterpretML<a class="headerlink" href="#interpretml" title="Permanent link">#</a></h2>
<p>Developed by <a href="../m/#microsoft-company">Microsoft</a> as an open source project, InterpretML is “a toolkit to help understand models and enable responsible machine learning”. </p>
<p>More at:</p>
<ul>
<li><a href="https://towardsdatascience.com/9-awesome-python-packages-for-machine-learning-that-should-deserve-more-credit-dbad17263145">https://towardsdatascience.com/9-awesome-python-packages-for-machine-learning-that-should-deserve-more-credit-dbad17263145</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="inverse-document-frequency-idf">Inverse Document Frequency (IDF)<a class="headerlink" href="#inverse-document-frequency-idf" title="Permanent link">#</a></h2>
<p>IDF measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>IDF(t) = log_e(Total number of documents / Number of documents with term t in it).
</span></code></pre></div>
<p>See also <a href="./">I</a>, <a href="../t/#term-frequency-inverse-document-frequency-tf-idf-retrieval-model">TF-IDF</a></p>
<h2 id="inverse-dynamics-model-idm">Inverse Dynamics Model (IDM)<a class="headerlink" href="#inverse-dynamics-model-idm" title="Permanent link">#</a></h2>
<p>OpenAI gathered 2,000 hours of video labeled with mouse and keyboard actions and trained an inverse dynamics model (IDM) to predict actions given past and future frames – this is the PreTraining part.</p>
<p>See also <a href="./">I</a>, [Video Pre-Trained Model]</p>
<h2 id="inverse-q-learning">Inverse Q-Learning<a class="headerlink" href="#inverse-q-learning" title="Permanent link">#</a></h2>
<p>See also <a href="./">I</a>, [Imitation Learning], <a href="./#iq-learn-model">IQ-Learn Model</a></p>
<h2 id="inverse-rl-irl">Inverse RL (IRL)<a class="headerlink" href="#inverse-rl-irl" title="Permanent link">#</a></h2>
<p>Learn reward function from expert demonstrations. Allows mimicking behavior without rewards.</p>
<p>See also <a href="./">I</a>, <a href="../b/#behavioural-cloning">Behavioural Cloning</a>, [Imitation Learning], <a href="./#iq-learn-model">IQ-Learn Model</a>, <a href="../r/#reinforcement-learning-rl">Reinforcement Learning</a>, <a href="../r/#reward-function">Reward Function</a></p>
<h2 id="inverted-file-index-ivd">Inverted File Index (IVD)<a class="headerlink" href="#inverted-file-index-ivd" title="Permanent link">#</a></h2>
<p>~ Used in <a href="../s/#similarity-search">similarity search</a></p>
<p>More at;</p>
<ul>
<li><a href="https://medium.com/towards-data-science/similarity-search-knn-inverted-file-index-7cab80cc0e79">https://medium.com/towards-data-science/similarity-search-knn-inverted-file-index-7cab80cc0e79</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="iq-learn-model">IQ-Learn Model<a class="headerlink" href="#iq-learn-model" title="Permanent link">#</a></h2>
<object data="https://arxiv.org/pdf/2106.12142" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2106.12142">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>site - <a href="https://div99.github.io/IQ-Learn/">https://div99.github.io/IQ-Learn/</a></li>
<li>paper - <a href="https://arxiv.org/abs/2106.12142">https://arxiv.org/abs/2106.12142</a></li>
<li>code - <a href="https://github.com/Div99/IQ-Learn">https://github.com/Div99/IQ-Learn</a></li>
<li>blog - <a href="https://ai.stanford.edu/blog/learning-to-imitate/">https://ai.stanford.edu/blog/learning-to-imitate/</a></li>
</ul>
<p>See also <a href="./">I</a>, [Imitation Learning], <a href="./#inverse-q-learning">Inverse Q-Learning</a></p>
<h2 id="isaac-gym-environment">Isaac Gym Environment<a class="headerlink" href="#isaac-gym-environment" title="Permanent link">#</a></h2>
<p>In <a href="../r/#reinforcement-learning-rl">reinforcement learning</a>, a physics-based <a href="../e/#environment">environment</a> built by <a href="../n/#nvidia-company">Nvidia</a></p>
<p>More at :</p>
<ul>
<li><a href="https://developer.nvidia.com/isaac-gym">https://developer.nvidia.com/isaac-gym</a></li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="iso-42001-standard">ISO 42001 Standard<a class="headerlink" href="#iso-42001-standard" title="Permanent link">#</a></h2>
<p>~ Manage risk and use AI responsibly while balancing innovation, governance, and ethics.</p>
<ul>
<li>ethical practice</li>
<li>treat individuals fairly</li>
<li>make decision based on accurate information</li>
</ul>
<p>It prepares companies for additional regulations that will be introduced in the next years, including the <a href="../e/#european-union-eu-ai-act">EU AI Act</a> published in 2024.</p>
<p>More at:</p>
<ul>
<li><a href="https://kpmg.com/ch/en/insights/artificial-intelligence/iso-iec-42001.html">https://kpmg.com/ch/en/insights/artificial-intelligence/iso-iec-42001.html</a></li>
</ul>
<p>See also <a href="./">I</a>, <a href="../m/#model-governance">Model Governance</a></p>
<h2 id="isolation-forest-if">Isolation Forest (IF)<a class="headerlink" href="#isolation-forest-if" title="Permanent link">#</a></h2>
<p>The Isolation Forest works a bit differently than a <a href="../r/#random-forest">Random Forest</a>. It also creates a bunch of decision trees, but then it calculates the path length necessary to isolate an observation in the tree. The idea being that isolated observations, or anomalies, are easier to isolate because there are fewer conditions necessary to distinguish them from the normal cases. Thus, the anomalies will have shorter paths than normal observations and reside closer to the root of the tree.</p>
<p>See also <a href="./">I</a>, <a href="../e/#ensemble-method">Ensemble Method</a>, <a href="../l/#local-outlier-factor-lof">Local Outlier Factor</a></p>
<h2 id="isomorphic-labs-company">Isomorphic Labs Company<a class="headerlink" href="#isomorphic-labs-company" title="Permanent link">#</a></h2>
<p>Spin off from <a href="../d/#deepmind-company">Deepmind</a> also founded by <a href="../d/#demis-hassabis-person">Demis Hassabis</a> to continue working on <a href="../a/#alphafold-model">AlphaFold</a> extensions</p>
<p>More at:</p>
<ul>
<li><a href="https://www.isomorphiclabs.com/">https://www.isomorphiclabs.com/</a></li>
<li>articles<ul>
<li><a href="https://endpts.com/isomorphic-labs-ceo-demis-hassabis-bets-on-biotechs-ai-future/">https://endpts.com/isomorphic-labs-ceo-demis-hassabis-bets-on-biotechs-ai-future/</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">I</a>, ...</p>
<h2 id="istio">Istio<a class="headerlink" href="#istio" title="Permanent link">#</a></h2>
<iframe src="https://www.youtube.com/embed/ZF9rPkm20NY" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">I</a>, <a href="../m/#mlops">MLOps</a></p>
<h2 id="iteration">Iteration<a class="headerlink" href="#iteration" title="Permanent link">#</a></h2>
<p>Each time a batch is processed is called an iteration. Note that the processing of the entire dataset, called an epoch, may require several iterations. This is particularly the case in the case of a large / very-large dataset.</p>
<p>In DeepRacer, an iteration refers to one full pass through the training data to update the <a href="../r/#reinforcement-learning-rl">reinforcement learning</a> (policy) model. Each iteration consists of multiple <a href="../e/#episode">episodes</a> (1 episode = car crash or complite an entire track) :</p>
<ul>
<li>One iteration involves running a specified number of <a href="../e/#episode">episodes</a> on the track.</li>
<li>For example, 10 episodes per iteration.</li>
<li>During each episode, the agent races around the track, gathering experience about taking actions in different states.</li>
<li>After completing the specified number of episodes, the reinforcement learning (policy) model is updated once based on the experience gathered.</li>
<li>This completes one full iteration. The episodes are reset and the process repeats for the next iteration.</li>
<li>Multiple iterations are run consecutively to train the (policy) model over time with more and more experience from the track.</li>
<li>The number of episodes per iteration and number of total iterations are key hyperparameters to configure the training.</li>
<li>More episodes per iteration gather more diverse experience for each update but reduce update frequency.</li>
<li>Running many iterations is needed for the agent to converge to a good policy. Hundreds of iterations are common.</li>
</ul>
<p>So in summary, one iteration involves multiple episodes followed by one model update. Multiple iterations drive the learning process over time to optimize the policy.</p>
<p>The concepts of iteration and epoch are sometimes used interchangeably, but they have some subtle differences in the context of reinforcement learning:</p>
<p>Iteration:</p>
<ul>
<li>In reinforcement learning, one iteration typically refers to running through a batch of experience data and updating the model once.</li>
<li>For example, Running 10 episodes to generate new experience data, then using that to improve the policy once.</li>
</ul>
<p>Epoch:</p>
<ul>
<li>Epoch usually refers to the number of complete passes through the full dataset to train the model.</li>
<li>For example, setting epoch=5 would mean passing through ALL available experience data 5 times, updating the model each time.</li>
</ul>
<p>So the key differences are:</p>
<ul>
<li>Iteration - Single update based on a batch of new experience (episodes).</li>
<li>Epoch - Full pass through all past experience with multiple updates.</li>
<li>Iterations happen sequentially, gathering new data over time.</li>
<li>Epochs reuse the same dataset multiple times.</li>
<li>In DeepRacer, iterations happen continuously as the car gathers more experience. Epochs are less common.</li>
</ul>
<p>So in reinforcement learning, iterations drive learning over time from new experience, while epochs reuse experience for regularization. But the terms are sometimes conflated.</p>
<p>Here is a concrete example to illustrate the differences between epoch and iteration:</p>
<p>Let's say we are training a DeepRacer model. We configure the following:</p>
<ul>
<li>Episodes per iteration: 10</li>
<li>Iterations: 100</li>
<li>Epochs: 5</li>
</ul>
<p>This means:</p>
<ul>
<li>During each iteration, the agent will run 10 episode races to generate experience data.</li>
<li>There will be 100 iterations, so 100 batches of 10 episodes.</li>
<li>1000 total episodes (10 * 100).</li>
<li>The experience from each iteration's episodes will be used to update the model weights once.</li>
<li>After the 100 iterations complete, representing gathering new experience over time, we will then run 5 epochs.</li>
<li>For each epoch, the agent will replay through ALL past 1000 episodes to further train the model.</li>
</ul>
<p>So:</p>
<ul>
<li>Iterations = New data gathered over time, single update per batch.</li>
<li>Epochs = Multiple passes over full past data for further training.</li>
</ul>
<p>This example highlights how iterations drive sequential learning, while epochs refine training on existing experience. The terms have distinct meanings in reinforcement learning.</p>
<p>There are a few key reasons why having both the notion of iterations and epochs can be useful in reinforcement learning:</p>
<ul>
<li>Iterations allow for sequential learning - New data is gathered over time through agent-environment interaction, and the policy is updated incrementally. This is crucial for online RL.</li>
<li>Epochs complement this by allowing offline refinement on past experience. The policy can be smoothed and regularized by replaying old episodes.</li>
<li>In the early stages of training, iterations quickly evolve the policy using fresh data. Epochs are less critical.</li>
<li>But epochs become more useful later. As sample efficiency increases, replay and reuse of past experience is helpful.</li>
<li>Epochs also help deal with correlated sequential experience. Random reshuffling during epoch replays helps de-correlate the data.</li>
<li>Multiple epochs can expose the model to a wider variety of transitions rather than just recent frequent ones.</li>
<li>Too many epochs can however lead to overfitting. The balance with iterations should be tuned.</li>
<li>For continuous training, iterations naturally align with expanding the dataset over time.</li>
</ul>
<p>So in essence, iterations drive continuous learning while epochs refine and generalize the behavior through experience replay. Their complementary strengths improve overall learning.</p>
<p>See also <a href="./">I</a>, ...</p>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">February 7, 2025</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">February 7, 2025</span>
  </span>

    
    
    
      
  <span class="md-source-file__fact">
    
      
  <span class="md-icon" title="Contributors">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34-.46-1.16-1.11-1.47-1.11-1.47-.91-.62.07-.6.07-.6 1 .07 1.53 1.03 1.53 1.03.87 1.52 2.34 1.07 2.91.83.09-.65.35-1.09.63-1.34-2.22-.25-4.55-1.11-4.55-4.92 0-1.11.38-2 1.03-2.71-.1-.25-.45-1.29.1-2.64 0 0 .84-.27 2.75 1.02.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02.55 1.35.2 2.39.1 2.64.65.71 1.03 1.6 1.03 2.71 0 3.82-2.34 4.66-4.57 4.91.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"/></svg>
  </span>
  <span>GitHub</span>

    
    <nav>
      
        <a href="https://github.com/emayssat" class="md-author" title="@emayssat">
          
          <img src="https://avatars.githubusercontent.com/u/1972699?v=4&size=72" alt="emayssat">
        </a>
      
      
      
    </nav>
  </span>

    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../h/" class="md-footer__link md-footer__link--prev" aria-label="Previous: H">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                H
              </div>
            </div>
          </a>
        
        
          
          <a href="../j/" class="md-footer__link md-footer__link--next" aria-label="Next: J">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                J
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 - 2025 <a href="https://www.midtown.ai/" rel="noopener" target="_blank">Midtown AI, Inc.</a>
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://x.com/midtown_ai" target="_blank" rel="noopener" title="Follow us on X" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:ai4all@midtown.ai" target="_blank" rel="noopener" title="Send us an email" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.footer", "navigation.indexes", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../../javascript/mathjax.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../../javascript/tablesort.js"></script>
      
    
  </body>
</html>