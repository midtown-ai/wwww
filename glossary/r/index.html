
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Let's explore this transforming technology. Let's shape the future of AI together.">
      
      
        <meta name="author" content="info@midtown.ai (Emmanuel M.)">
      
      
        <link rel="canonical" href="https://midtown-ai.github.io/wwww/glossary/r/">
      
      
        <link rel="prev" href="../q/">
      
      
        <link rel="next" href="../s/">
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.3">
    
    
      
        <title>R - Midtown AI</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.d7758b05.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M96 0C43 0 0 43 0 96v320c0 53 43 96 96 96h320c17.7 0 32-14.3 32-32s-14.3-32-32-32v-64c17.7 0 32-14.3 32-32V32c0-17.7-14.3-32-32-32H96m0 384h256v64H96c-17.7 0-32-14.3-32-32s14.3-32 32-32m32-240c0-8.8 7.2-16 16-16h192c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16m16 48h192c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16s7.2-16 16-16"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M320 0c17.7 0 32 14.3 32 32v64h120c39.8 0 72 32.2 72 72v272c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72h120V32c0-17.7 14.3-32 32-32M208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zM264 256a40 40 0 1 0-80 0 40 40 0 1 0 80 0m152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80M48 224h16v192H48c-26.5 0-48-21.5-48-48v-96c0-26.5 21.5-48 48-48m544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48h-16V224z"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M288 0H128c-17.7 0-32 14.3-32 32s14.3 32 32 32v132.8c0 11.8-3.3 23.5-9.5 33.5L10.3 406.2C3.6 417.2 0 429.7 0 442.6 0 480.9 31.1 512 69.4 512h309.2c38.3 0 69.4-31.1 69.4-69.4 0-12.8-3.6-25.4-10.3-36.4L329.5 230.4c-6.2-10.1-9.5-21.7-9.5-33.5V64c17.7 0 32-14.3 32-32S337.7 0 320 0zm-96 196.8V64h64v132.8c0 23.7 6.6 46.9 19 67.1l34.5 56.1h-171l34.5-56.1c12.4-20.2 19-43.4 19-67.1"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.1 52.4 442.6 6.5c-1.9-3.9-6.1-6.5-10.5-6.5s-8.5 2.6-10.4 6.5l-16.5 45.9-46 16.8c-4.3 1.6-7.3 5.9-7.2 10.4 0 4.5 3 8.7 7.2 10.2l45.7 16.8 16.8 45.8c1.5 4.4 5.8 7.5 10.4 7.5s8.9-3.1 10.4-7.5l16.5-45.8 45.7-16.8c4.2-1.5 7.2-5.7 7.2-10.2 0-4.6-3-8.9-7.2-10.4zm-132.4 53c-12.5-12.5-32.8-12.5-45.3 0l-2.9 2.9c-22-8-45.8-12.3-70.5-12.3C93.1 96 0 189.1 0 304s93.1 208 208 208 208-93.1 208-208c0-24.7-4.3-48.5-12.2-70.5l2.9-2.9c12.5-12.5 12.5-32.8 0-45.3l-80-80zM200 192c-57.4 0-104 46.6-104 104v8c0 8.8-7.2 16-16 16s-16-7.2-16-16v-8c0-75.1 60.9-136 136-136h8c8.8 0 16 7.2 16 16s-7.2 16-16 16z"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-40-176h24v-64h-24c-13.3 0-24-10.7-24-24s10.7-24 24-24h48c13.3 0 24 10.7 24 24v88h8c13.3 0 24 10.7 24 24s-10.7 24-24 24h-80c-13.3 0-24-10.7-24-24s10.7-24 24-24m40-208a32 32 0 1 1 0 64 32 32 0 1 1 0-64"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 216C0 149.7 53.7 96 120 96h8c17.7 0 32 14.3 32 32s-14.3 32-32 32h-8c-30.9 0-56 25.1-56 56v8h64c35.3 0 64 28.7 64 64v64c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64V216m256 0c0-66.3 53.7-120 120-120h8c17.7 0 32 14.3 32 32s-14.3 32-32 32h-8c-30.9 0-56 25.1-56 56v8h64c35.3 0 64 28.7 64 64v64c0 35.3-28.7 64-64 64h-64c-35.3 0-64-28.7-64-64V216"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M438.6 105.4c12.5 12.5 12.5 32.8 0 45.3l-256 256c-12.5 12.5-32.8 12.5-45.3 0l-128-128c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L160 338.7l233.4-233.3c12.5-12.5 32.8-12.5 45.3 0z"/></svg>');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/custom_admonitions.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_effects.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_tables.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_text.css">
    
      <link rel="stylesheet" href="../../stylesheets/custom_theme.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="R - Midtown AI" >
      
        <meta  property="og:description"  content="Let's explore this transforming technology. Let's shape the future of AI together." >
      
        <meta  property="og:image"  content="https://midtown-ai.github.io/wwww/assets/images/social/glossary/r.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://midtown-ai.github.io/wwww/glossary/r/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="R - Midtown AI" >
      
        <meta  name="twitter:description"  content="Let's explore this transforming technology. Let's shape the future of AI together." >
      
        <meta  name="twitter:image"  content="https://midtown-ai.github.io/wwww/assets/images/social/glossary/r.png" >
      
    
    
  <link rel="stylesheet" href="../../stylesheets/custom.7c86dd97.min.css">

  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#r" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Midtown AI" class="md-header__button md-logo" aria-label="Midtown AI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M22.7 33.4c13.5-4.1 28.1 1.1 35.9 12.9l165.4 248 165.4-248c7.8-11.7 22.4-17 35.9-12.9S448 49.9 448 64v384c0 17.7-14.3 32-32 32s-32-14.3-32-32V169.7L250.6 369.8c-5.9 8.9-15.9 14.2-26.6 14.2s-20.7-5.3-26.6-14.2L64 169.7V448c0 17.7-14.3 32-32 32S0 465.7 0 448V64c0-14.1 9.2-26.5 22.7-30.6"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Midtown AI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              R
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../blog/" class="md-tabs__link">
          
  
    
  
  Blog

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
    
  
  Glossary

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../about/" class="md-tabs__link">
        
  
    
  
  About

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Midtown AI" class="md-nav__button md-logo" aria-label="Midtown AI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M22.7 33.4c13.5-4.1 28.1 1.1 35.9 12.9l165.4 248 165.4-248c7.8-11.7 22.4-17 35.9-12.9S448 49.9 448 64v384c0 17.7-14.3 32-32 32s-32-14.3-32-32V169.7L250.6 369.8c-5.9 8.9-15.9 14.2-26.6 14.2s-20.7-5.3-26.6-14.2L64 169.7V448c0 17.7-14.3 32-32 32S0 465.7 0 448V64c0-14.1 9.2-26.5 22.7-30.6"/></svg>

    </a>
    Midtown AI
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../blog/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Blog
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Blog
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Archive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/archive/2025/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2025
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Categories
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Categories
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/category/entertainment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Entertainment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../blog/category/no-code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    No Code
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Glossary
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Glossary
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../0-9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    0-9
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../a/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../b/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../c/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    C
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    D
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../e/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../f/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    F
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../g/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    G
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../h/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    H
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../i/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    I
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../j/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    J
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../k/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    K
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../l/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    L
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../m/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    M
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../n/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    N
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../o/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    O
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../p/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    P
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../q/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Q
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    R
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    R
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#r-square" class="md-nav__link">
    <span class="md-ellipsis">
      R-Square
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rabbit-company" class="md-nav__link">
    <span class="md-ellipsis">
      Rabbit Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#radial-basis-function-rbf" class="md-nav__link">
    <span class="md-ellipsis">
      Radial Basis Function (RBF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#radiogpt" class="md-nav__link">
    <span class="md-ellipsis">
      RadioGPT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-cut-forest-rcf" class="md-nav__link">
    <span class="md-ellipsis">
      Random Cut Forest (RCF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Random Forest
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-sample-consensus-ransac-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Random Sample Consensus (RANSAC) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-sampling" class="md-nav__link">
    <span class="md-ellipsis">
      Random Sampling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-search" class="md-nav__link">
    <span class="md-ellipsis">
      Random Search
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ranking" class="md-nav__link">
    <span class="md-ellipsis">
      Ranking
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#raspberry-pi-computer" class="md-nav__link">
    <span class="md-ellipsis">
      Raspberry Pi Computer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rational-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Rational Agent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ray-framework" class="md-nav__link">
    <span class="md-ellipsis">
      Ray Framework
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ray-python-module" class="md-nav__link">
    <span class="md-ellipsis">
      Ray Python Module
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reactive-ai" class="md-nav__link">
    <span class="md-ellipsis">
      Reactive AI
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reason-act-react-prompting" class="md-nav__link">
    <span class="md-ellipsis">
      Reason-Act (ReAct) Prompting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Reasoning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recall" class="md-nav__link">
    <span class="md-ellipsis">
      Recall
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recall-oriented-understudy-for-gisting-evaluation-rouge-score" class="md-nav__link">
    <span class="md-ellipsis">
      Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#receiver-operating-characteristic-roc-curve" class="md-nav__link">
    <span class="md-ellipsis">
      Receiver Operating Characteristic (ROC) Curve
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#receptance-weighted-key-value-rwkv-model" class="md-nav__link">
    <span class="md-ellipsis">
      Receptance Weighted Key Value (RWKV) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#receptance-weighted-key-value-rwkv-world-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      Receptance Weighted Key Value (RWKV) World Tokenizer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recommendation-engine" class="md-nav__link">
    <span class="md-ellipsis">
      Recommendation Engine
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rectified-linear-unit-relu-activation-function" class="md-nav__link">
    <span class="md-ellipsis">
      Rectified Linear Unit (ReLU) Activation Function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rectified-linear-unit-relu-activation-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Rectified Linear Unit (ReLU) Activation Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recurrent-neural-network-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      Recurrent Neural Network (RNN)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#red-teaming" class="md-nav__link">
    <span class="md-ellipsis">
      Red Teaming
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reducible-error" class="md-nav__link">
    <span class="md-ellipsis">
      Reducible Error
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reducible-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Reducible Loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reflex-model" class="md-nav__link">
    <span class="md-ellipsis">
      Reflex Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#region-based-cnn-r-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      Region-Based CNN (R-CNN)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#region-of-interest-roi-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      Region-Of-Interest (ROI) Pooling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression-task" class="md-nav__link">
    <span class="md-ellipsis">
      Regression Task
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression-tree" class="md-nav__link">
    <span class="md-ellipsis">
      Regression Tree
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization-parameter" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization Parameter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regulatory-landscape" class="md-nav__link">
    <span class="md-ellipsis">
      Regulatory Landscape
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforce-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      REINFORCE Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforce-leave-one-out-rloo-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      REINFORCE Leave one Out (RLOO) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-fine-tuning-rft" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Fine-Tuning (RFT)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning (RL)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning (RL) Agent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning (RL) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-coordinated-feedback-rlcf" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning Coordinated Feedback (RLCF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-from-ai-feedback-rlaif" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning from AI Feedback (RLAIF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-from-human-feedback-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning from Human Feedback (RLHF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-with-executive-feedback-rlef" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning with Executive Feedback (RLEF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relation" class="md-nav__link">
    <span class="md-ellipsis">
      Relation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relation-extraction" class="md-nav__link">
    <span class="md-ellipsis">
      Relation Extraction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relational-deep-learning-rdl" class="md-nav__link">
    <span class="md-ellipsis">
      Relational Deep Learning (RDL)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relational-deep-learning-benchmark-relbench" class="md-nav__link">
    <span class="md-ellipsis">
      Relational Deep Learning Benchmark (RelBench)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relative-approximation-error-rae" class="md-nav__link">
    <span class="md-ellipsis">
      Relative Approximation Error (RAE)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relative-entropy" class="md-nav__link">
    <span class="md-ellipsis">
      Relative Entropy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relevancy" class="md-nav__link">
    <span class="md-ellipsis">
      Relevancy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replaced-word-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Replaced Word Prediction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replay-buffer" class="md-nav__link">
    <span class="md-ellipsis">
      Replay Buffer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replay-memory" class="md-nav__link">
    <span class="md-ellipsis">
      Replay Memory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replicate-company" class="md-nav__link">
    <span class="md-ellipsis">
      Replicate Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replit-company" class="md-nav__link">
    <span class="md-ellipsis">
      Replit Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#repls" class="md-nav__link">
    <span class="md-ellipsis">
      Repls
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#representation" class="md-nav__link">
    <span class="md-ellipsis">
      Representation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#representation-space" class="md-nav__link">
    <span class="md-ellipsis">
      Representation Space
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reproducibility" class="md-nav__link">
    <span class="md-ellipsis">
      Reproducibility
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reptile" class="md-nav__link">
    <span class="md-ellipsis">
      Reptile
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reranking" class="md-nav__link">
    <span class="md-ellipsis">
      Reranking
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resample" class="md-nav__link">
    <span class="md-ellipsis">
      Resample
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resampling-method" class="md-nav__link">
    <span class="md-ellipsis">
      Resampling Method
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reshaping" class="md-nav__link">
    <span class="md-ellipsis">
      Reshaping
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#residual" class="md-nav__link">
    <span class="md-ellipsis">
      Residual
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#residual-block" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Block
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#residual-network-resnet-model" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Network (ResNET) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resilient-backpropagation-rprop-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Resilient Backpropagation (Rprop) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#response-variable" class="md-nav__link">
    <span class="md-ellipsis">
      Response Variable
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#responsible-ai-rai" class="md-nav__link">
    <span class="md-ellipsis">
      Responsible AI (RAI)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#restricted-boltzmann-machine-rbm" class="md-nav__link">
    <span class="md-ellipsis">
      Restricted Boltzmann Machine (RBM)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rag-assessment-raga" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Assessment (RAGA)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rag-generator" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Generator
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rag-information-retriever-rag-ir" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Information Retriever (RAG-IR)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retrieval-augmented-generation-rag-system" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieval-Augmented Generation (RAG) System
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rag-triad-of-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Triad Of Metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retrieval-based-model" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieval-Based Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retrieval-interleaved-generation-rig-system" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieval-Interleaved Generation (RIG) System
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retriever" class="md-nav__link">
    <span class="md-ellipsis">
      Retriever
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward" class="md-nav__link">
    <span class="md-ellipsis">
      Reward
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-function" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-hacking" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Hacking
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-model" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-shaping" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Shaping
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-trap" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Trap
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ridge-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge Regression
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ridge-regression-penalty" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge Regression Penalty
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#riffusion-model" class="md-nav__link">
    <span class="md-ellipsis">
      Riffusion Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#riva-model" class="md-nav__link">
    <span class="md-ellipsis">
      Riva Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robot" class="md-nav__link">
    <span class="md-ellipsis">
      Robot
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robotics" class="md-nav__link">
    <span class="md-ellipsis">
      Robotics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robotic-foundation-model-rfm-family" class="md-nav__link">
    <span class="md-ellipsis">
      Robotic Foundation Model (RFM) Family
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robotschool" class="md-nav__link">
    <span class="md-ellipsis">
      RobotSchool
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robustly-optimized-bert-approach-roberta-model" class="md-nav__link">
    <span class="md-ellipsis">
      Robustly Optimized BERT Approach (RoBERTa) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robustness" class="md-nav__link">
    <span class="md-ellipsis">
      Robustness
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rocket-league-rl-gym" class="md-nav__link">
    <span class="md-ellipsis">
      Rocket League (RL) Gym
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#root-mean-square-error-rmse" class="md-nav__link">
    <span class="md-ellipsis">
      Root Mean Square Error (RMSE)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#root-mean-square-propagation-rmsprop-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Root Mean Square Propagation (RMSprop) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rosettafold-rf-diffusion-model" class="md-nav__link">
    <span class="md-ellipsis">
      RoseTTAFold (RF) Diffusion Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rule" class="md-nav__link">
    <span class="md-ellipsis">
      Rule
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rule-interaction" class="md-nav__link">
    <span class="md-ellipsis">
      Rule Interaction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#runway-company" class="md-nav__link">
    <span class="md-ellipsis">
      Runway Company
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../s/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    S
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../t/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    T
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../u/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    U
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../v/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../w/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    W
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../x/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    X
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../y/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Y
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../z/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Z
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#r-square" class="md-nav__link">
    <span class="md-ellipsis">
      R-Square
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rabbit-company" class="md-nav__link">
    <span class="md-ellipsis">
      Rabbit Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#radial-basis-function-rbf" class="md-nav__link">
    <span class="md-ellipsis">
      Radial Basis Function (RBF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#radiogpt" class="md-nav__link">
    <span class="md-ellipsis">
      RadioGPT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-cut-forest-rcf" class="md-nav__link">
    <span class="md-ellipsis">
      Random Cut Forest (RCF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    <span class="md-ellipsis">
      Random Forest
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-sample-consensus-ransac-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Random Sample Consensus (RANSAC) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-sampling" class="md-nav__link">
    <span class="md-ellipsis">
      Random Sampling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-search" class="md-nav__link">
    <span class="md-ellipsis">
      Random Search
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ranking" class="md-nav__link">
    <span class="md-ellipsis">
      Ranking
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#raspberry-pi-computer" class="md-nav__link">
    <span class="md-ellipsis">
      Raspberry Pi Computer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rational-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Rational Agent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ray-framework" class="md-nav__link">
    <span class="md-ellipsis">
      Ray Framework
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ray-python-module" class="md-nav__link">
    <span class="md-ellipsis">
      Ray Python Module
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reactive-ai" class="md-nav__link">
    <span class="md-ellipsis">
      Reactive AI
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reason-act-react-prompting" class="md-nav__link">
    <span class="md-ellipsis">
      Reason-Act (ReAct) Prompting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      Reasoning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recall" class="md-nav__link">
    <span class="md-ellipsis">
      Recall
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recall-oriented-understudy-for-gisting-evaluation-rouge-score" class="md-nav__link">
    <span class="md-ellipsis">
      Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#receiver-operating-characteristic-roc-curve" class="md-nav__link">
    <span class="md-ellipsis">
      Receiver Operating Characteristic (ROC) Curve
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#receptance-weighted-key-value-rwkv-model" class="md-nav__link">
    <span class="md-ellipsis">
      Receptance Weighted Key Value (RWKV) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#receptance-weighted-key-value-rwkv-world-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      Receptance Weighted Key Value (RWKV) World Tokenizer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recommendation-engine" class="md-nav__link">
    <span class="md-ellipsis">
      Recommendation Engine
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rectified-linear-unit-relu-activation-function" class="md-nav__link">
    <span class="md-ellipsis">
      Rectified Linear Unit (ReLU) Activation Function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rectified-linear-unit-relu-activation-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Rectified Linear Unit (ReLU) Activation Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recurrent-neural-network-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      Recurrent Neural Network (RNN)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#red-teaming" class="md-nav__link">
    <span class="md-ellipsis">
      Red Teaming
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reducible-error" class="md-nav__link">
    <span class="md-ellipsis">
      Reducible Error
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reducible-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Reducible Loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reflex-model" class="md-nav__link">
    <span class="md-ellipsis">
      Reflex Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#region-based-cnn-r-cnn" class="md-nav__link">
    <span class="md-ellipsis">
      Region-Based CNN (R-CNN)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#region-of-interest-roi-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      Region-Of-Interest (ROI) Pooling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression-task" class="md-nav__link">
    <span class="md-ellipsis">
      Regression Task
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression-tree" class="md-nav__link">
    <span class="md-ellipsis">
      Regression Tree
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization-parameter" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization Parameter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regulatory-landscape" class="md-nav__link">
    <span class="md-ellipsis">
      Regulatory Landscape
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforce-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      REINFORCE Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforce-leave-one-out-rloo-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      REINFORCE Leave one Out (RLOO) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-fine-tuning-rft" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Fine-Tuning (RFT)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning (RL)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning (RL) Agent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning (RL) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-coordinated-feedback-rlcf" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning Coordinated Feedback (RLCF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-from-ai-feedback-rlaif" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning from AI Feedback (RLAIF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-from-human-feedback-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning from Human Feedback (RLHF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-with-executive-feedback-rlef" class="md-nav__link">
    <span class="md-ellipsis">
      Reinforcement Learning with Executive Feedback (RLEF)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relation" class="md-nav__link">
    <span class="md-ellipsis">
      Relation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relation-extraction" class="md-nav__link">
    <span class="md-ellipsis">
      Relation Extraction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relational-deep-learning-rdl" class="md-nav__link">
    <span class="md-ellipsis">
      Relational Deep Learning (RDL)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relational-deep-learning-benchmark-relbench" class="md-nav__link">
    <span class="md-ellipsis">
      Relational Deep Learning Benchmark (RelBench)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relative-approximation-error-rae" class="md-nav__link">
    <span class="md-ellipsis">
      Relative Approximation Error (RAE)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relative-entropy" class="md-nav__link">
    <span class="md-ellipsis">
      Relative Entropy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relevancy" class="md-nav__link">
    <span class="md-ellipsis">
      Relevancy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replaced-word-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Replaced Word Prediction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replay-buffer" class="md-nav__link">
    <span class="md-ellipsis">
      Replay Buffer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replay-memory" class="md-nav__link">
    <span class="md-ellipsis">
      Replay Memory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replicate-company" class="md-nav__link">
    <span class="md-ellipsis">
      Replicate Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#replit-company" class="md-nav__link">
    <span class="md-ellipsis">
      Replit Company
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#repls" class="md-nav__link">
    <span class="md-ellipsis">
      Repls
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#representation" class="md-nav__link">
    <span class="md-ellipsis">
      Representation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#representation-space" class="md-nav__link">
    <span class="md-ellipsis">
      Representation Space
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reproducibility" class="md-nav__link">
    <span class="md-ellipsis">
      Reproducibility
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reptile" class="md-nav__link">
    <span class="md-ellipsis">
      Reptile
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reranking" class="md-nav__link">
    <span class="md-ellipsis">
      Reranking
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resample" class="md-nav__link">
    <span class="md-ellipsis">
      Resample
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resampling-method" class="md-nav__link">
    <span class="md-ellipsis">
      Resampling Method
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reshaping" class="md-nav__link">
    <span class="md-ellipsis">
      Reshaping
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#residual" class="md-nav__link">
    <span class="md-ellipsis">
      Residual
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#residual-block" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Block
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#residual-network-resnet-model" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Network (ResNET) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resilient-backpropagation-rprop-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Resilient Backpropagation (Rprop) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#response-variable" class="md-nav__link">
    <span class="md-ellipsis">
      Response Variable
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#responsible-ai-rai" class="md-nav__link">
    <span class="md-ellipsis">
      Responsible AI (RAI)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#restricted-boltzmann-machine-rbm" class="md-nav__link">
    <span class="md-ellipsis">
      Restricted Boltzmann Machine (RBM)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rag-assessment-raga" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Assessment (RAGA)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rag-generator" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Generator
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rag-information-retriever-rag-ir" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Information Retriever (RAG-IR)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retrieval-augmented-generation-rag-system" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieval-Augmented Generation (RAG) System
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rag-triad-of-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Triad Of Metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retrieval-based-model" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieval-Based Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retrieval-interleaved-generation-rig-system" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieval-Interleaved Generation (RIG) System
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retriever" class="md-nav__link">
    <span class="md-ellipsis">
      Retriever
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward" class="md-nav__link">
    <span class="md-ellipsis">
      Reward
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-function" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-hacking" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Hacking
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-model" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-shaping" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Shaping
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-trap" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Trap
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ridge-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge Regression
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ridge-regression-penalty" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge Regression Penalty
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#riffusion-model" class="md-nav__link">
    <span class="md-ellipsis">
      Riffusion Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#riva-model" class="md-nav__link">
    <span class="md-ellipsis">
      Riva Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robot" class="md-nav__link">
    <span class="md-ellipsis">
      Robot
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robotics" class="md-nav__link">
    <span class="md-ellipsis">
      Robotics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robotic-foundation-model-rfm-family" class="md-nav__link">
    <span class="md-ellipsis">
      Robotic Foundation Model (RFM) Family
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robotschool" class="md-nav__link">
    <span class="md-ellipsis">
      RobotSchool
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robustly-optimized-bert-approach-roberta-model" class="md-nav__link">
    <span class="md-ellipsis">
      Robustly Optimized BERT Approach (RoBERTa) Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robustness" class="md-nav__link">
    <span class="md-ellipsis">
      Robustness
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rocket-league-rl-gym" class="md-nav__link">
    <span class="md-ellipsis">
      Rocket League (RL) Gym
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#root-mean-square-error-rmse" class="md-nav__link">
    <span class="md-ellipsis">
      Root Mean Square Error (RMSE)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#root-mean-square-propagation-rmsprop-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Root Mean Square Propagation (RMSprop) Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rosettafold-rf-diffusion-model" class="md-nav__link">
    <span class="md-ellipsis">
      RoseTTAFold (RF) Diffusion Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rule" class="md-nav__link">
    <span class="md-ellipsis">
      Rule
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rule-interaction" class="md-nav__link">
    <span class="md-ellipsis">
      Rule Interaction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#runway-company" class="md-nav__link">
    <span class="md-ellipsis">
      Runway Company
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="r">R<a class="headerlink" href="#r" title="Permanent link">#</a></h1>
<h2 id="r-square">R-Square<a class="headerlink" href="#r-square" title="Permanent link">#</a></h2>
<p>Aka score. In regression, indicates that a large proportion of the variance in the test instances' prices is explained by the model. Test is a new dataset not used in the model.</p>
<p>Notes:</p>
<ul>
<li>It ranges from 0 to 1. An R^2 of 1 indicates that the regression line perfectly fits the data.</li>
<li>Higher values indicate that the model fits the data better. Values closer to 1 are preferred.</li>
<li>R^2 increases as you add more variables to the model, even if the additional variables are insignificant.</li>
<li>It does not indicate whether the coefficient estimates and predictions are biased or not.</li>
<li>It is a good measure of the predictive power of the whole model, not of individual variables.</li>
</ul>
<p>R-squared is widely used to assess the goodness of fit in linear regression models. However, it should be interpreted cautiously and in conjunction with other model evaluation metrics like residual plots, F-tests, etc. In some cases, an [adjusted R-squared] value is preferred over the regular R-squared.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>                     Var(mean) - Var(best-line-fit)              &lt;== always positive
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>R-square = R^2 = -------------------------------------
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>                           Var(mean)                             &lt;== make R^2 between and 1 (and a %)
</span></code></pre></div>
<p>Interpretations:</p>
<ul>
<li>There is R^2 % less variation around the best-fit-line than the mean</li>
<li>The weight of this input parameter (size/weight relationship) accounts for R^2 % of the variation</li>
</ul>
<iframe src="https://www.youtube.com/embed/bMccdk8EdGo" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">R</a>, <a href="./#regression-task">Regression</a></p>
<h2 id="rabbit-company">Rabbit Company<a class="headerlink" href="#rabbit-company" title="Permanent link">#</a></h2>
<p>Price: $200</p>
<iframe src="https://www.youtube.com/embed/22wlLy7hKP4" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>site - <a href="https://www.rabbit.tech/">https://www.rabbit.tech/</a></li>
<li>devices <ul>
<li>R1 - <a href="https://www.theverge.com/24138746/rabbit-r1-hands-on-ai-gadget-chatgpt">https://www.theverge.com/24138746/rabbit-r1-hands-on-ai-gadget-chatgpt</a></li>
</ul>
</li>
<li>alternative<ul>
<li><a href="../h/#humane-company">Humane</a> - with the AI pin</li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="radial-basis-function-rbf">Radial Basis Function (RBF)<a class="headerlink" href="#radial-basis-function-rbf" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, [Support Vector Classifier]</p>
<h2 id="radiogpt">RadioGPT<a class="headerlink" href="#radiogpt" title="Permanent link">#</a></h2>
<p>RadioGPT combines the power of GPT-4 technology with AI voice tech and <a href="../f/#futuri-media-company">Futuri Media</a>s AI-driven targeted story discovery and social content system, TopicPulse, to provide an unmatched localized radio experience for any market, any format.</p>
<p>{% include vimeoPlayer.html id=801620131 %}</p>
<p>More at:</p>
<ul>
<li><a href="https://futurimedia.com/radiogpt/">https://futurimedia.com/radiogpt/</a></li>
<li><a href="https://listen.streamon.fm/radiogpt">https://listen.streamon.fm/radiogpt</a></li>
<li><a href="https://www.techtimes.com/articles/288252/20230227/radiogpt-first-ai-radio.htm">https://www.techtimes.com/articles/288252/20230227/radiogpt-first-ai-radio.htm</a></li>
<li><a href="https://futurimedia.com/futuri-launches-radiogpt/">https://futurimedia.com/futuri-launches-radiogpt/</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../g/#generative-pre-trained-transformer-gpt-model-family">GPT Model</a></p>
<h2 id="random-cut-forest-rcf">Random Cut Forest (RCF)<a class="headerlink" href="#random-cut-forest-rcf" title="Permanent link">#</a></h2>
<p>Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a <a href="../d/#dataset">dataset</a>. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. They are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the "regular" data. Including these anomalies in a <a href="../d/#dataset">dataset</a> can drastically increase the complexity of a machine learning task since the "regular" data can often be described with a simple model. With each data point, RCF associates an anomaly score. Low score values indicate that the data point is considered "normal." High values indicate the presence of an anomaly in the data. The definitions of "low" and "high" depend on the application but common practice suggests that scores beyond three standard deviations from the mean score are considered anomalous. While there are many applications of anomaly detection algorithms to one-dimensional time series data such as traffic volume analysis or sound volume spike detection, RCF is designed to work with arbitrary-dimensional input. Amazon SageMaker RCF scales well with respect to number of features, dataset size, and number of instances.</p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="random-forest">Random Forest<a class="headerlink" href="#random-forest" title="Permanent link">#</a></h2>
<p>An ensemble method. Similar to XGBoost. Use prediction from several decision trees = supervised learning! Random Forest is a powerful and versatile supervised machine learning algorithm that grows and combines multiple decision trees to create a forest. It can be used for both classification and regression problems (if decision conflict --&gt; vote, regression --&gt; mean). Random Forest is a robust machine learning algorithm that can be used for a variety of tasks including regression and classification. It is an ensemble method, meaning that a random forest model is made up of a large number of small decision trees (weak learner), called estimators, which each produce their own predictions. The random forest model combines the predictions of the estimators to produce a more accurate prediction (strong learner!). Standard decision tree classifiers have the disadvantage that they are prone to overfitting to the training set. The random forest's ensemble design allows the random forest to compensate for this and generalize well to unseen data, including data with missing values. Random forests are also good at handling large datasets with high dimensionality and heterogeneous feature types (for example, if one column is categorical and another is numerical).</p>
<p><img alt="" src="../img/r/random_forest.png" width="100%" /></p>
<p>See also <a href="./">R</a>, <a href="../a/#attribute">Attribute</a>, <a href="../b/#bagging">Bagging</a>, <a href="../e/#ensemble-method">Ensemble Method</a>, <a href="../d/#decision-tree">Decision Tree</a>, <a href="../g/#gaussian-process">Gaussian Process</a>, <a href="../s/#supervised-learning">Supervised Learning</a>, [Tree Parzen Estimators], <a href="../e/#extreme-gradient-boosting-xgboost">XGBoost</a></p>
<h2 id="random-sample-consensus-ransac-algorithm">Random Sample Consensus (RANSAC) Algorithm<a class="headerlink" href="#random-sample-consensus-ransac-algorithm" title="Permanent link">#</a></h2>
<p>Developed in the early 1990s, in computer vision, but can be used in several fields.
 The algorithm removes <a href="../o/#outlier">outliers</a> and keep the <a href="../i/#inlier">inliers</a> from a sample set</p>
<p>Parameters:</p>
<ul>
<li>tolerance</li>
<li>number of iterations</li>
</ul>
<p>Assume the number of outliers is &lt; than the number of inliers.</p>
<p><img alt="" src="../img/r/random_sample_consensus_algorithm.png" width="100%" /></p>
<iframe src="https://www.youtube.com/embed/9D5rrtCC_E0" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/1YNjMxxXO-E" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Random_sample_consensus">https://en.wikipedia.org/wiki/Random_sample_consensus</a></li>
<li>code - <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html">https://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="random-sampling">Random Sampling<a class="headerlink" href="#random-sampling" title="Permanent link">#</a></h2>
<p>During model inference, the model produces a probability distribution across all tokens in the models known vocabulary. The model choosesor samplesa single token from this distribution as the next token to include in the response.</p>
<p>For each inference request, you can configure the model to choose the next token using either greedy or random sampling. For [greedy sampling], the token with the highest probability is selected. With random sampling, the model selects the next token using a random-weighted strategy across all predicted token probabilities. The different sampling methods are shown below for the phrase the student learns from the professor and her lectures.</p>
<p><img alt="" src="../img/r/random_sampling.png" width="100%" /></p>
<p>See also <a href="./">R</a>, <a href="../p/#passive-learning">Passive Learning</a></p>
<h2 id="random-search">Random Search<a class="headerlink" href="#random-search" title="Permanent link">#</a></h2>
<p>Sampling a parameter space - used with [hyperparameter optimization]</p>
<p>[Grid Searca] tries all combinations of hyperparameters hence increasing the time complexity of the computation and could result in an unfeasible computing cost. Providing a cheaper alternative, Random Search tests only as many tuples as you choose. The selection of the hyperparameter values is completely random.</p>
<p><img alt="" src="../img/r/random_search.webp" width="100%" /></p>
<p>More at:</p>
<ul>
<li><a href="https://towardsdatascience.com/a-practical-introduction-to-grid-search-random-search-and-bayes-search-d5580b1d941d">https://towardsdatascience.com/a-practical-introduction-to-grid-search-random-search-and-bayes-search-d5580b1d941d</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../s/#sobol-search">Sobol Search</a></p>
<h2 id="ranking">Ranking<a class="headerlink" href="#ranking" title="Permanent link">#</a></h2>
<p>Ranking. Suppose you are given a query and a set of documents. In ranking, the goal is to find the relative importance of the documents and order them based on relevance. An example use case of ranking is a product search for an ecommerce website. You could leverage data about search results, clicks, and successful purchases, and then apply XGBoost for training. This produces a model that gives relevance scores for the searched products.</p>
<iframe src="https://www.youtube.com/embed/YroewVVp7SM" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>ranking algorithms - <a href="https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a">https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a</a></li>
<li>overview - <a href="https://medium.com/airbnb-engineering/learning-to-rank-diversely-add6b1929621">https://medium.com/airbnb-engineering/learning-to-rank-diversely-add6b1929621</a></li>
<li>ranking evaluation metrics - <a href="https://towardsdatascience.com/comprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025">https://towardsdatascience.com/comprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025</a></li>
</ul>
<p>See also <a href="./">R</a>, [ML Algorithm], <a href="./#reranking">Reranking</a>, <a href="../e/#extreme-gradient-boosting-xgboost">XGBoost</a></p>
<h2 id="raspberry-pi-computer">Raspberry Pi Computer<a class="headerlink" href="#raspberry-pi-computer" title="Permanent link">#</a></h2>
<p>A &lt; $100 computer that is compatible with</p>
<ul>
<li><a href="../c/#coral-hardware">Coral Hardware</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="rational-agent">Rational Agent<a class="headerlink" href="#rational-agent" title="Permanent link">#</a></h2>
<p>Rational agents take decision-making a step further by aiming to maximize utility making decisions designed to achieve the best possible outcome based on the information available to them. These agents are not just autonomous or intelligent; they are focused on optimizing their decisions in a given environment, often under conditions of uncertainty. Rational agents are frequently used in simulations, economic models, or high-stakes scenarios where consistently optimal decision-making is critical.</p>
<p>BEWARE not all <a href="../i/#intelligent-agent">intelligent agents</a> are rational  an agent may learn and adapt but still not make the most optimal decisions due to imperfect information or computational constraints. Rational agents strive to make the best decisions within the limits of their knowledge and capabilities.</p>
<p>More at:</p>
<ul>
<li>articles<ul>
<li><a href="https://www.turingpost.com/p/agentsvocabulary">https://www.turingpost.com/p/agentsvocabulary</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="ray-framework">Ray Framework<a class="headerlink" href="#ray-framework" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="ray-python-module">Ray Python Module<a class="headerlink" href="#ray-python-module" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="reactive-ai">Reactive AI<a class="headerlink" href="#reactive-ai" title="Permanent link">#</a></h2>
<p>Tools that respond to specific inputs or situations without learning from past experiences (e.e. Alexa, Roomba, chess -playing computer)</p>
<p>See also <a href="./">R</a>, <a href="../a/#artificial-intelligence-ai">Artificial Intelligence</a></p>
<h2 id="reason-act-react-prompting">Reason-Act (ReAct) Prompting<a class="headerlink" href="#reason-act-react-prompting" title="Permanent link">#</a></h2>
<p>~ A <a href="../p/#prompt-engineering">prompt engineering</a> technique where <a href="../l/#large-language-model-llm">LLMs</a> are used to generate both reasoning traces and task-specific actions in an interleaved manner.</p>
<p>ReAct is a general paradigm that combines reasoning and acting with LLMs. ReAct prompts LLMs to generate verbal reasoning traces and actions for a task. This allows the system to perform dynamic reasoning to create, maintain, and adjust plans for acting while also enabling interaction to external environments (e.g., Wikipedia) to incorporate additional information into the reasoning. The figure below shows an example of ReAct and the different steps involved to perform question answering.</p>
<p><img alt="" src="../img/r/reason_act_prompting.webp" width="100%" /></p>
<object data="https://arxiv.org/pdf/2210.03629" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2210.03629">Download PDF</a>.
    </p>
</object>

<p>Alternative:</p>
<ul>
<li><a href="../s/#semantic-router">Semantic Router</a></li>
</ul>
<p>More at:</p>
<ul>
<li>site - <a href="https://react-lm.github.io/">https://react-lm.github.io/</a></li>
<li>paper - <a href="https://arxiv.org/abs/2210.03629">https://arxiv.org/abs/2210.03629</a></li>
<li>articles<ul>
<li><a href="https://www.promptingguide.ai/techniques/react">https://www.promptingguide.ai/techniques/react</a></li>
<li><a href="https://learnprompting.org/docs/advanced_applications/react">https://learnprompting.org/docs/advanced_applications/react</a></li>
<li><a href="https://blog.research.google/2022/11/react-synergizing-reasoning-and-acting.html">https://blog.research.google/2022/11/react-synergizing-reasoning-and-acting.html</a></li>
<li><a href="https://tsmatz.wordpress.com/2023/03/07/react-with-openai-gpt-and-langchain/">https://tsmatz.wordpress.com/2023/03/07/react-with-openai-gpt-and-langchain/</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, [Modular Reasoning Knowledge and Language]</p>
<h2 id="reasoning">Reasoning<a class="headerlink" href="#reasoning" title="Permanent link">#</a></h2>
<p>There are 5 types of reasoning:</p>
<ul>
<li><a href="../i/#inductive-reasoning">Inductive reasoning</a> - a conclusion is drawn based on observations or evidence. ( = figuring out patterns)</li>
<li><a href="../d/#deductive-reasoning">Deductive reasoning</a> - a conclusion is drawn based on the truth of the premises. ( = applying rules)</li>
<li><a href="../a/#abductive-reasoning">Abductive reasoning</a> - a conclusion is drawn based on the best explanation for a given set of observations.</li>
<li><a href="../f/#formal-reasoning">Formal reasoning</a> - a systematic and logical process that follows a set of rules and principles.</li>
<li><a href="../i/#informal-reasoning">Informal reasoning</a> - a less structured approach to reasoning that relies on intuition, experience, and common sense.</li>
</ul>
<p>Examples:</p>
<ul>
<li>[Case-Based Reasoning]</li>
<li><a href="../l/#logical-reasoning">Logical Reasoning</a> ???</li>
</ul>
<p>See <a href="./">R</a>, <a href="../m/#machine-reasoning">Machine Reasoning</a></p>
<h2 id="recall">Recall<a class="headerlink" href="#recall" title="Permanent link">#</a></h2>
<p>~ of all the actual positives, how many did we correctly identify?</p>
<p>~ same <a href="../t/#true-positive-rate-tpr">True Positive Rate (TPR)</a> or <a href="../s/#sensitivity">Sensitivity</a></p>
<p>~ a recall of 1 means that we correctly identified all the positive cases</p>
<p>~ a recall of 0 means we identified none of the positive cases</p>
<p>~ High recall = test is effective at detecting positive cases without missing many / describe the ability of a model to find all the relevant cases within a dataset</p>
<p>Metric used for <a href="../m/#model-evaluation">model evaluation</a> when the cost of [False Negatives (FN)] (missed positive) is high. For example, in disease prediction, it is critical not to miss any positive cases.</p>
<p>Recall is the fraction of malignant tumors (of one class) that the system identified (correctly, in the class). Recall measures the fraction of truly malignant tumors that were detected. Recall is important in medical cases where it doesnt matter whether we raise a false alarm but the actual positive cases should not go undetected!</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a># TP : The predicted value is positive and it is positive
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>       A cat is recognized as a cat
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a># FN : Type II eror : The predicted value is negative, but it is positive!
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>       A cat is recognized as a dog (not a cat!)
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a># TP + FN : Actual value is positive
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>       The cat is a cat!
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>           TP                     correctly identified        
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>Recall = -----------   =  ------------------------------------
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>           TP + FN             all identified in class       
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>Recall = % of positively identified
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>         % of cat identified as cat
</span></code></pre></div>
<p>Recall would be a better metric because we dont want to accidentally discharge an infected person and let them mix with the healthy population thereby spreading contagious virus. Now you can understand why accuracy is NOT always the best metric for a model.</p>
<p>More at:</p>
<ul>
<li><a href="https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5">https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../c/#confusion-matrix">Confusion Matrix</a></p>
<h2 id="recall-oriented-understudy-for-gisting-evaluation-rouge-score">Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score<a class="headerlink" href="#recall-oriented-understudy-for-gisting-evaluation-rouge-score" title="Permanent link">#</a></h2>
<p>ROUGE score is a set of metrics commonly used for <a href="../t/#text-summarization">text summarization</a> tasks, where the goal is to automatically generate a concise summary of a longer text. ROUGE was designed to evaluate the quality of machine-generated summaries by comparing them to reference summaries provided by humans.</p>
<p>ROUGE score measures the similarity between the machine-generated summary and the reference summaries using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summaries. The most common n-grams used are unigrams, bigrams, and trigrams. ROUGE score calculates the recall of n-grams in the machine-generated summary by comparing them to the reference summaries.</p>
<p>ROUGE =  (Recall of n-grams)</p>
<p>Where:</p>
<ul>
<li>Recall of n-grams is the number of n-grams that appear in both the machine-generated summary and the reference summaries divided by the total number of n-grams in the reference summaries.</li>
</ul>
<p>ROUGE score ranges from 0 to 1, with higher values indicating better summary quality. Like BLEU score, a perfect summary would have a ROUGE score of 1, while a completely incorrect summary would have a ROUGE score of 0.</p>
<p>ROUGE scores are branched into ROUGE-N,ROUGE-L, and ROUGE-S.</p>
<p>In general:</p>
<ul>
<li><a href="../b/#bilingual-evaluation-understudy-bleu-score">BLEU</a> focuses on precision: how much the words (and/or n-grams) in the candidate model outputs appear in the human reference.</li>
<li>ROUGE focuses on recall: how much the words (and/or n-grams) in the human references appear in the candidate model outputs.</li>
</ul>
<p>These results are complementing, as is often the case in the precision-recall tradeoff.</p>
<iframe src="https://www.youtube.com/embed/TMshhnrEXlg" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<object data="https://aclanthology.org/W04-1013.pdf" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://aclanthology.org/W04-1013.pdf">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>paper - <a href="https://aclanthology.org/W04-1013/">https://aclanthology.org/W04-1013/</a></li>
<li><a href="https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb">https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb</a></li>
<li><a href="https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/">https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/</a></li>
</ul>
<p>See also <a href="./">R</a>, [MS COCO Caption Dataset], [NLP Metrics]</p>
<h2 id="receiver-operating-characteristic-roc-curve">Receiver Operating Characteristic (ROC) Curve<a class="headerlink" href="#receiver-operating-characteristic-roc-curve" title="Permanent link">#</a></h2>
<p>~ provides a more holistic view of the validator's performance across different threshold settings. It's particularly useful when you need to balance <a href="../s/#sensitivity">sensitivity</a> and <a href="../s/#specificity">specificity</a> in your evaluations.</p>
<p>~ summarize all the <a href="../c/#confusion-matrix">confusion matrices</a> of a logistic model, if the classification 'Probability-threshold' is changed. (<a href="../t/#thresholding">thresholding</a>)</p>
<p>~ A <a href="../c/#confusion-matrix">confusion matrix</a> is 1 point on the ROC curve!</p>
<p>~ The diagonal is where FPR = TPR or random guess model (?). The top-right point is when the threshold is the lowest (all samples are predicted in positive class). The bottom-left point is when the threshold is the highest (all samples are predicted in negative class).</p>
<p>~ The best threshold is for a desired TPR to get the lowest FPR.</p>
<p>Receiver Operating Characteristic (ROC) is a graphical representation of the performance of a binary classifier system as the discrimination threshold is varied. It plots the <a href="../t/#true-positive-rate-tpr">true positive rate (TPR)</a> on the y-axis and the <a href="../f/#false-positive-rate-fpr">false positive rate (FPR)</a> on the x-axis. The [true positive rate] (also known as <a href="../s/#sensitivity">sensitivity</a> or <a href="./#recall">recall</a> ) is the proportion of positive cases that are correctly identified by the classifier. The false positive rate (also known as the fall-out) is the proportion of negative cases that are incorrectly identified as positive. An ROC curve plots the <a href="../t/#true-positive-rate-tpr">TPR</a> against the <a href="../f/#false-positive-rate-fpr">FPR</a> at different threshold settings. A perfect classifier will have a <a href="../t/#true-positive-rate-tpr">TPR</a> of 1 and a <a href="../f/#false-positive-rate-fpr">FPR</a> of 0, resulting in a point in the top left corner of the ROC space. A random classifier will have a <a href="../t/#true-positive-rate-tpr">TPR</a> and <a href="../f/#false-positive-rate-fpr">FPR</a> of 0.5, resulting in a point along a diagonal line from the bottom left to the top right corner of the ROC space. The <a href="../a/#area-under-the-receiver-operating-characteristic-auroc-curve">Area Under the ROC (AUROC) curve</a> is a measure of the classifier's overall performance, with a value of 1 indicating perfect performance and a value of 0.5 indicating a performance no better than random guessing.</p>
<p><img alt="" src="../img/r/receiver_operating_characteristic_curve_thresholding_table.png" width="100%" /></p>
<p><img alt="" src="../img/r/receiver_operating_characteristic_curve.png" width="100%" /></p>
<iframe src="https://www.youtube.com/embed/4jRBRDbJemM" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at :</p>
<ul>
<li>articles<ul>
<li><a href="https://blog.revolutionanalytics.com/2016/11/calculating-auc.html">https://blog.revolutionanalytics.com/2016/11/calculating-auc.html</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, [Area Under The Curve]</p>
<h2 id="receptance-weighted-key-value-rwkv-model">Receptance Weighted Key Value (RWKV) Model<a class="headerlink" href="#receptance-weighted-key-value-rwkv-model" title="Permanent link">#</a></h2>
<p>~ a <a href="./#recurrent-neural-network-rnn">RNN</a> with GPT-level LLM performance, and can also be directly trained like a GPT transformer (parallelizable). &lt;!&gt; Pronounced RwaKuv &lt;!&gt;</p>
<p>&lt;!&gt; Is attention all you need? &lt;!&gt; This model and the [mamba model] disagree! The RNN fight back!</p>
<p>[Transformers] have revolutionized almost all <a href="../n/#natural-language-processing-nlp">Natural Language Processing (NLP)</a> tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, <a href="./#recurrent-neural-network-rnn">recurrent neural networks (RNNs)</a> exhibit linear scaling in memory and computational requirements but struggle to match the same performance as [Transformers] due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs.</p>
<p>Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense <a href="./#recurrent-neural-network-rnn">RNN</a> ever trained, and find RWKV performs on par with similarly sized [Transformers], suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.</p>
<iframe src="https://www.youtube.com/embed/I-HMKky7Qsw" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<object data="https://arxiv.org/pdf/2305.13048" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2305.13048">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>site - <a href="https://www.rwkv.com/">https://www.rwkv.com/</a></li>
<li>wiki - <a href="https://wiki.rwkv.com/">https://wiki.rwkv.com/</a></li>
<li>paper - <a href="https://arxiv.org/abs/2305.13048">https://arxiv.org/abs/2305.13048</a></li>
<li>code - <a href="https://github.com/BlinkDL/RWKV-LM">https://github.com/BlinkDL/RWKV-LM</a></li>
<li>articles<ul>
<li><a href="https://johanwind.github.io/2023/03/23/rwkv_overview.html">https://johanwind.github.io/2023/03/23/rwkv_overview.html</a></li>
<li><a href="https://johanwind.github.io/2023/03/23/rwkv_details.html">https://johanwind.github.io/2023/03/23/rwkv_details.html</a></li>
<li><a href="https://ben.bolte.cc/rwkv-model">https://ben.bolte.cc/rwkv-model</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="receptance-weighted-key-value-rwkv-world-tokenizer">Receptance Weighted Key Value (RWKV) World Tokenizer<a class="headerlink" href="#receptance-weighted-key-value-rwkv-world-tokenizer" title="Permanent link">#</a></h2>
<p>A tokenizer used by open-source [RWKV Models] that is taking ALL spoken languages in consideration. It solves limitations of the [BPE Tokenizer] by removing bias against non-english and non-spaced languages.</p>
<iframe src="https://www.youtube.com/embed/I-HMKky7Qsw" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">R</a>, ...</p>
<h2 id="recommendation-engine">Recommendation Engine<a class="headerlink" href="#recommendation-engine" title="Permanent link">#</a></h2>
<ul>
<li><a href="../a/#apriori-algorithm">Apriori Algorithm</a></li>
<li><a href="../l/#link-prediction">Link Prediction</a> in a graph database</li>
<li><a href="../t/#two-tower-model">Two-Tower Model</a> uses cosine or euclidian similarity between <a href="../e/#embedding">embeddings</a>. Used in <a href="./#retrieval-augmented-generation-rag-system">RAG</a> !</li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="rectified-linear-unit-relu-activation-function">Rectified Linear Unit (ReLU) Activation Function<a class="headerlink" href="#rectified-linear-unit-relu-activation-function" title="Permanent link">#</a></h2>
<p><mark>Everything that has a negative value, change it to zero</mark></p>
<p>We can avoid this problem by using activation functions which don't have this property of 'squashing' the input space into a small region. A popular choice is Rectified Linear Unit which maps x to max(0,x). Benefits:</p>
<ul>
<li>easy to compute the derivative</li>
<li>helps with the vanishing gradient problem in backpropagation</li>
<li>derivative is always 0 if input signal is below the threshold --&gt; solution is LeakyRelu</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../a/#activation-function">Activation Function</a>, <a href="../e/#exploding-gradient-problem">Exploding Gradient Problem</a>, [LeakyReLU Activation Function], <a href="./#residual-network-resnet-model">ResNET Model</a>, <a href="../v/#vanishing-gradient-problem">Vanishing Gradient Problem</a></p>
<h2 id="rectified-linear-unit-relu-activation-layer">Rectified Linear Unit (ReLU) Activation Layer<a class="headerlink" href="#rectified-linear-unit-relu-activation-layer" title="Permanent link">#</a></h2>
<p>~ an <a href="../a/#activation-layer">activation layer</a> that uses the <a href="./#rectified-linear-unit-relu-activation-function">ReLU activation function</a></p>
<p>A stack of images (matrix of pixels) becomes a stack of images with no negative values.</p>
<p>Such layer is used in <a href="../c/#convolutional-neural-network-cnn">CNNs</a> after each <a href="../c/#convolutional-layer">convolutional layer</a> and before each <a href="../p/#pooling-layer">pooling layer</a></p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)<a class="headerlink" href="#recurrent-neural-network-rnn" title="Permanent link">#</a></h2>
<p><mark>When successive input have a relationship between each of them</mark></p>
<p>Ex characters in a word. Output of a layer can feed the input of self or an upstream layer. AFAIK the input is taken into consideration at the next round/processing. The opposite of a Feedforward Neural Network. Example: Prediction of the next letter/word given the previous letter/word (useful when there is correlation between the sequence of objects/classification). Also useful for timeseries data. Became widespread thanks to [Long Short Term Memory (LSTM) Network] a more multi-layer version of RNN.</p>
<p><img alt="" src="../img/r/recurrent_neural_network.png" width="100%" /></p>
<p>These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they arent all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:</p>
<p><img alt="" src="../img/r/recurrent_neural_network_unrolled.png" width="100%" /></p>
<p><img alt="" src="../img/r/recurrent_neural_network_repeating_module.png" width="100%" /></p>
<p><a href="../n/#neural-network">Neural networks</a> will "loops" that are optimized for speech recognition, language modeling, translation. Essential to these successes is the use of LSTMs, a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them.</p>
<p>/// details | Can or cannot use <a href="../b/#backpropagation">backpropagation</a>?
    type:question</p>
<div class="language-text highlight"><pre><span></span><code>Yes, can use [backpropagation through time] !
</code></pre></div>
<p>///</p>
<p><img alt="" src="../img/r/recurrent_neural_network_cells.png" width="100%" /></p>
<p>There are several types of RNNs, including:</p>
<ul>
<li>one-to-many</li>
<li>many-to-one</li>
<li>many-to-many</li>
</ul>
<p><img alt="" src="../img/r/recurrent_neural_network_types.png" width="100%" /></p>
<p>Beware:</p>
<ul>
<li>The most modern RNN uses <a href="../l/#long-short-term-memory-lstm-cell">Long-Short Term Memory (LSTM)</a> or <a href="../g/#gated-recurrent-unit-gru-cell">Gated Recurrent Unit (GRU)</a> cells</li>
<li>Memory = hidden state (output of previous stage) ?</li>
</ul>
<p>Beware:</p>
<ul>
<li>RNN are now deprecated by attention-based models such as those based on the <a href="../t/#transformer-architecture">transformer architecture</a></li>
<li>deprecated previous approach using [bag of words] and <a href="../w/#word2vec-model">word2vec</a></li>
<li>deprecated by <a href="../a/#attention-based-model">attention-based models</a></li>
<li>RNN use <a href="../b/#backpropagation-through-time">backpropagation through time</a> instead of 'normal' <a href="../b/#backpropagation">backpropagation</a></li>
</ul>
<iframe src="https://www.youtube.com/embed/DFZ1UA7-fxY" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/AsNTP8Kwu80" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>keras and RNN - <a href="https://medium.com/analytics-vidhya/music-generation-using-deep-learning-a2b2848ab177">https://medium.com/analytics-vidhya/music-generation-using-deep-learning-a2b2848ab177</a></li>
</ul>
<p>See also <a href="./">R</a>, [Bidirectional RNN], <a href="../f/#folded-rnn">Folded RNN</a>, <a href="../f/#feedforward-neural-network">Feedforward Neural Network</a>, <a href="../h/#hidden-state">Hidden State</a>, [Long Short-Term Memory Network], <a href="../p/#pixel-rnn">Pixel RNN</a>, <a href="../u/#unfolded-rnn">Unfolded RNN</a>, <a href="../v/#vanishing-gradient-problem">Vanishing Gradient Problem</a></p>
<h2 id="red-teaming">Red Teaming<a class="headerlink" href="#red-teaming" title="Permanent link">#</a></h2>
<p>Red-teaming is a form of evaluation that elicits model vulnerabilities that might lead to undesirable behaviors. Jailbreaking is another term for red-teaming wherein the LLM is manipulated to break away from its guardrails. Microsofts Chatbot Tay launched in 2016 and the more recent Bing's Chatbot Sydney are real-world examples of how disastrous the lack of thorough evaluation of the underlying ML model using red-teaming can be. The origins of the idea of a red-team traces back to adversary simulations and wargames performed by militaries.</p>
<p>The goal of red-teaming language models is to craft a prompt that would trigger the model to generate text that is likely to cause harm. Red-teaming shares some similarities and differences with the more well-known form of evaluation in ML called adversarial attacks. The similarity is that both red-teaming and adversarial attacks share the same goal of attacking or fooling the model to generate content that would be undesirable in a real-world use case. However, adversarial attacks can be unintelligible to humans, for example, by prefixing the string aaabbbcc to each prompt because it deteriorates model performance. Many examples of such attacks on various NLP classification and generation tasks is discussed in <a href="https://arxiv.org/abs/1908.07125">Wallace et al., 19</a>. Red-teaming prompts, on the other hand, look like regular, natural language prompts.</p>
<p>Red-teaming can reveal model limitations that can cause upsetting user experiences or enable harm by aiding violence or other unlawful activity for a user with malicious intentions. The outputs from red-teaming (just like adversarial attacks) are generally used to train the model to be less likely to cause harm or steer it away from undesirable outputs.</p>
<p>Since red-teaming requires creative thinking of possible model failures, it is a problem with a large search space making it resource intensive. A workaround would be to augment the LLM with a classifier trained to predict whether a given prompt contains topics or phrases that can possibly lead to offensive generations and if the classifier predicts the prompt would lead to a potentially offensive text, generate a canned response. Such a strategy would err on the side of caution. But that would be very restrictive and cause the model to be frequently evasive. So, there is tension between the model being helpful (by following instructions) and being harmless (or at least less likely to enable harm).</p>
<p>The red team can be a human-in-the-loop or an LM that is testing another LM for harmful outputs. Coming up with red-teaming prompts for models that are fine-tuned for safety and alignment (such as via <a href="./#reinforcement-learning-from-human-feedback-rlhf">RLHF</a> or <a href="../s/#supervised-fine-tuning-sft">SFT</a>) requires creative thinking in the form of roleplay attacks wherein the LLM is instructed to behave as a malicious character as in <a href="https://arxiv.org/abs/2209.07858">Ganguli et al., 22</a> Instructing the model to respond in code instead of natural language can also reveal the models learned biases such as examples below.</p>
<p><img alt="" src="../img/r/red_teaming_prompt.png" width="100%" /></p>
<object data="https://arxiv.org/pdf/2209.07858" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2209.07858">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li><a href="https://huggingface.co/blog/red-teaming">https://huggingface.co/blog/red-teaming</a></li>
<li>papers<ul>
<li><a href="https://arxiv.org/abs/1908.07125">https://arxiv.org/abs/1908.07125</a></li>
<li>Anthropic paper - Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned - <a href="https://arxiv.org/abs/2209.07858">https://arxiv.org/abs/2209.07858</a></li>
<li>Red Teaming Language Models with Language Models - <a href="https://arxiv.org/abs/2202.03286">https://arxiv.org/abs/2202.03286</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../s/#supervised-fine-tuning-sft">Supervised Fine-Tuning</a></p>
<h2 id="reducible-error">Reducible Error<a class="headerlink" href="#reducible-error" title="Permanent link">#</a></h2>
<p>Suppose that we want to predict a value Y based upon a set X = (X1, X2, , Xp) of variables. For the predictions to have any chance of being good predictions, X needs to contain the core set of variables that drive the behavior of Y. But there will almost always be lesser variables, not included in X, that nonetheless exert some minor influence on Y. We capture the situation as follows:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a> Y = f(X) +  
</span></code></pre></div>
<p>Here, f is the function describing the relationship between X and Y, and  is an error term that accounts for all the unmeasured influences on Y. We assume that  is independent of X and has mean 0.</p>
<p>Usually we dont know f exactly, so we use statistical methods (such as linear regression) to estimate f. We use f to denote this estimate. This allows us to predict Y from X using the following:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a> Y =  f(X) +  
</span></code></pre></div>
<p>Our predictions will generally be imperfect: there will be some nonzero difference between the predicted and true values. This difference is called prediction error. In general we cant see the true values directly, but we can see evidence of the gap by looking at the [Residuals], which are the difference between the observed and predicted values.</p>
<p>To minimize prediction error, we need to understand its source. Broadly speaking there are two: reducible error and irreducible error.</p>
<p>Reducible error is the error arising from the mismatch between f and f. f is the true relationship between X and Y, but we cant see f directly we can only estimate it. We can reduce the gap between our estimate and the true function by applying improved methods.</p>
<p>Irreducible error arises from the fact that X doesnt completely determine Y. That is, there are variables outside of X  and independent of X that still have some small effect on Y. The only way to improve prediction error related to irreducible error is to identify these outside influences and incorporate them as predictors.</p>
<p>More at:</p>
<ul>
<li><a href="https://medium.com/wwblog/reducible-vs-irreducible-error-e469036969fa">https://medium.com/wwblog/reducible-vs-irreducible-error-e469036969fa</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../l/#loss-function">Loss Function</a></p>
<h2 id="reducible-loss">Reducible Loss<a class="headerlink" href="#reducible-loss" title="Permanent link">#</a></h2>
<p>See <a href="./#reducible-error">Reducible Error</a></p>
<h2 id="reflex-model">Reflex Model<a class="headerlink" href="#reflex-model" title="Permanent link">#</a></h2>
<p>An inference you can make almost instantaneously. Ex: flash the image of a zebra in front of me and recognize it is a zebra.</p>
<p>See also <a href="./">R</a>, <a href="../m/#model-type">Model Type</a></p>
<h2 id="region-based-cnn-r-cnn">Region-Based CNN (R-CNN)<a class="headerlink" href="#region-based-cnn-r-cnn" title="Permanent link">#</a></h2>
<p>~ algorithm used for <a href="../i/#image-segmentation">image segmentation</a></p>
<ol>
<li><a href="../o/#object-detection">Object detection</a> --&gt; <a href="../b/#bounding-box">bounding boxes</a> for 2K proposed region-of-interest, aka ROI pooling</li>
<li>Warped image regions</li>
<li>pass image in <a href="../c/#convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</a></li>
</ol>
<iframe src="https://www.youtube.com/embed/PlXE1_FVtMQ" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b">https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="region-of-interest-roi-pooling">Region-Of-Interest (ROI) Pooling<a class="headerlink" href="#region-of-interest-roi-pooling" title="Permanent link">#</a></h2>
<p>The first step in a <a href="./#region-based-cnn-r-cnn">R-CNN</a></p>
<iframe src="https://www.youtube.com/embed/oj6Ub874gHI" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">R</a>, ...</p>
<h2 id="regression-task">Regression Task<a class="headerlink" href="#regression-task" title="Permanent link">#</a></h2>
<p>A type of <a href="../s/#supervised-learning">supervised learning</a> algorithm used for forecasting and prediction.</p>
<p>Regression is a statistical method used to analyze the relationship between a dependent variable (also known as the response or outcome variable) and one or more independent variables (also known as predictor or explanatory variables). The goal of regression is to find the line of best fit that describes the relationship between the variables, which can be used for prediction or understanding the relationship. There are many different types of regression, including linear, logistic, and polynomial regression.</p>
<p>In regression, instead of mapping inputs to a discrete number of classes like a classification, <code>the goal is to output a number</code> (ex stock price, temperature, probability, ...) . An example regression problem is predicting the price that a house will sell for. In this case, when XGBoost is given historical data about houses and selling prices, it can learn a function that predicts the selling price of a house given the corresponding metadata about the house. Another example: predictive maintenance, customer churn prediction. Practicality: outcome should be easy to measure, use historical observations. </p>
<p>The different algorithms are:</p>
<ul>
<li>Regression trees : Finite number of number output!</li>
<li><a href="../l/#linear-regression">Linear regression</a></li>
<li>[Logistic regression] : probability between 2 outcomes</li>
<li><a href="../n/#non-linear-regression">Non-Linear Regression</a><ul>
<li>Polynomial regression : dependent variable y is modelled as an nth degree polynomial in x.</li>
<li>Cubic and quadratic regression </li>
</ul>
</li>
<li>(occasionally) <a href="../s/#support-vector-machine-svm">Support vector machine</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../c/#classification-task">Classification</a>, <a href="../c/#custom-churn-prediction">Custom Churn Prediction</a>, <a href="../p/#predictive-maintenance">Predictive Maintenance</a>, <a href="./#regression-tree">Regression Tree</a>, <a href="../e/#extreme-gradient-boosting-xgboost">XGBoost</a></p>
<h2 id="regression-tree">Regression Tree<a class="headerlink" href="#regression-tree" title="Permanent link">#</a></h2>
<p>A decision tree using the MSE loss function and used for regression (predict a range, or specific "real" value). </p>
<p>More at:</p>
<ul>
<li><a href="https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047">https://medium.com/analytics-vidhya/regression-trees-decision-tree-for-regression-machine-learning-e4d7525d8047</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../d/#decision-tree">Decision Tree</a></p>
<h2 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">#</a></h2>
<p>Force a 'simpler model' to avoid memorizing training data, aka <a href="../o/#overfitting">overfitting</a> and encourage generalization!.</p>
<p>There is an approach that prefers some <a href="../b/#bias">bias</a> over high <a href="../v/#variance">variance</a>, this approach is called regularization. It works well for most of the <a href="../c/#classification-task">classification</a> / <a href="./#regression-task">regression</a> problems.</p>
<p>The main ideas is </p>
<ol>
<li>to constrain the model to simplify it (fewer degrees of freedom)</li>
<li>to add information, aka <a href="../d/#data-augmentation">data augmentation</a></li>
</ol>
<p>Methods:</p>
<ul>
<li><a href="../l/#lasso-regression">Lasso Regression</a>, aka <a href="../l/#l1-regularization">L1 regularization</a><ul>
<li>Adds the absolute values of the coefficients as a penalty term to the <a href="../l/#loss-function">loss function</a>.</li>
<li>Encourages <a href="../s/#sparsity">sparsity</a> in the model by driving some of the coefficients to exactly zero.</li>
<li>Useful for <a href="../f/#feature-selection">feature selection</a> as it tends to eliminate less important <a href="../f/#feature">features</a>.</li>
</ul>
</li>
<li><a href="./#ridge-regression">Ridge Regression</a>, aka <a href="../l/#l2-regularization">L2 regularization</a><ul>
<li>Adds the squared values of the coefficients as a penalty term to the <a href="../l/#loss-function">loss function</a>.</li>
<li>Encourages the model to have smaller weights overall.</li>
<li>Helps prevent multicollinearity (high correlation between features) and stabilizes the training process.</li>
<li>Compare to L1, this method exaggerates the impact of the higher value over smaller values</li>
</ul>
</li>
<li><a href="../e/#elastic-net-regression">Elastic Net Regression</a><ul>
<li>Combines both L1 and L2 regularization terms.</li>
<li>It has two <a href="../h/#hyperparameter">hyperparameters</a> (alpha and l1_ratio) that control the strength of L1 and L2 regularization.</li>
</ul>
</li>
<li><a href="../d/#dropout-regularization">Dropout regularization</a> <ul>
<li>Applied in neural networks, dropout involves randomly setting a fraction of input units to zero during each update of the model.</li>
<li>Helps prevent co-adaptation of units by providing a form of ensemble learning within a single model.</li>
</ul>
</li>
<li><a href="../e/#early-stopping">Early stopping</a><ul>
<li>Monitors the model's performance on a validation set during training and stops the training process when the performance starts to degrade.</li>
<li>Prevents the model from overfitting by terminating training before it becomes too specialized to the training data.</li>
<li>When validation loss increases --&gt; overfitting</li>
</ul>
</li>
<li><a href="../w/#weight-regularization">Weight regularization</a> or weight decay<ul>
<li>Adds a penalty term proportional to the sum of the squared weights to the <a href="../l/#loss-function">loss function</a>.</li>
<li>Similar to L2 regularization and helps control the magnitude of the weights.</li>
</ul>
</li>
<li><a href="../d/#data-augmentation">Data Augmentation</a><ul>
<li>Not always possible, but works well with images, etc.</li>
</ul>
</li>
</ul>
<p>These regularization techniques play a crucial role in preventing <a href="../o/#overfitting">overfitting</a>, improving model generalization, and creating models that perform well on unseen data. The choice of regularization method and <a href="../h/#hyperparameter">hyperparameter</a> values depends on the specific characteristics of the dataset and the [machine learning] model being used.</p>
<p><img alt="" src="../img/r/regularization_l1.png" width="100%" /></p>
<p><img alt="" src="../img/r/regularization_l2.png" width="100%" /></p>
<iframe src="https://www.youtube.com/embed/VqKq78PVO9g" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/EehRcPo1M-Q" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/">https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../b/#bias-variance-trade-off">Bias-Variance Trade-Off</a>, <a href="../b/#balanced-fitting">Balanced Fitting</a>, <a href="../o/#overfitting">Overfitting</a>, <a href="../u/#underfitting">Underfitting</a></p>
<h2 id="regularization-parameter">Regularization Parameter<a class="headerlink" href="#regularization-parameter" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, [Support Vector Classifier]</p>
<h2 id="regulatory-landscape">Regulatory Landscape<a class="headerlink" href="#regulatory-landscape" title="Permanent link">#</a></h2>
<p>Regulatory landscape refers to the complete framework of laws, rules, and regulations that govern an industry or business activity. This includes:</p>
<ul>
<li>All applicable laws and regulations</li>
<li>Government agencies and regulatory bodies</li>
<li>Current and upcoming regulatory requirements</li>
<li>Policy directions and regulatory trends</li>
</ul>
<p><img alt="" src="../img/r/regulatory_landscape.png" width="100%" /></p>
<p>/// details | How are regulation difference from AI principles?
    type:question
///</p>
<p>See also <a href="./">R</a>, <a href="../a/#ai-bill-of-rights">AI Bill Of Rights</a>, <a href="../a/#ai-principle">AI Principles</a>, [European Union AI Act]</p>
<h2 id="reinforce-algorithm">REINFORCE Algorithm<a class="headerlink" href="#reinforce-algorithm" title="Permanent link">#</a></h2>
<p>REINFORCE is a Monte-Carlo variant of policy gradients (Monte-Carlo: taking random samples). The agent collects a <a href="../t/#trajectory">trajectory</a>  of one <a href="../e/#episode">episode</a> using its current <a href="../p/#policy">policy</a>, and uses it to update the policy parameter. Since one full trajectory must be completed to construct a sample space, REINFORCE is updated in an [off-policy] way.</p>
<iframe src="https://www.youtube.com/embed/5eSh5F8gjWU" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>pytorch implementation - <a href="https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63">https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../p/#policy-gradient-algorithm">Policy Gradient Algorithm</a></p>
<h2 id="reinforce-leave-one-out-rloo-algorithm">REINFORCE Leave one Out (RLOO) Algorithm<a class="headerlink" href="#reinforce-leave-one-out-rloo-algorithm" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, <a href="../p/#policy-gradient-algorithm">Policy Gradient Algorithm</a></p>
<h2 id="reinforcement-fine-tuning-rft">Reinforcement Fine-Tuning (RFT)<a class="headerlink" href="#reinforcement-fine-tuning-rft" title="Permanent link">#</a></h2>
<p>~ allows users to create custom models using the same process <a href="../o/#openai-company">OpenAI</a> uses internally.</p>
<p>See also <a href="./">R</a>, <a href="../s/#supervised-fine-tuning-sft">Supervised Fine-Tuning</a></p>
<h2 id="reinforcement-learning-rl">Reinforcement Learning (RL)<a class="headerlink" href="#reinforcement-learning-rl" title="Permanent link">#</a></h2>
<p>~ feedback loop that comes back to adjust decision variables (action) in order to improve decision-making (policy model) over time.</p>
<p><code>Pavlov's dog experiment!</code> also <code>How we learn to bike!</code> Beware: <code>No training set is provided, training is coming from experience! = learn by try and error</code>. Continue doing the behavior that led you to the most <a href="./#reward">reward</a>. Imagine teaching a program to play chess. It level of playing is only as good as the training data provided. If it learns/analyses the games played by average players, the program will only be average. If it analyses the games of the best player in the work, it will be just as good as them, but not better. <code>Reinforcement learning is the way to make a computer be better  than human at chess or any other activity</code> using <a href="./#reward">rewards</a> and <a href="../p/#punishment">punishments</a>. <code>Learning through trials and errors</code> input/sensor --&gt; software agent --&gt; <a href="../a/#action">Action</a>, leads to [supervised feedback] in the form of a <a href="./#reward">reward</a>.</p>
<p>The <a href="./#reinforcement-learning-rl-agent">RL agent</a> continuously learns. There is no final state. Gives a <a href="./#reward">reward</a> for each move. Get's better based on past-experience.</p>
<p><code>Reinforcement learning is located near the supervised end of the spectrum</code>. Unlike <a href="../s/#supervised-learning">supervised learning</a>, reinforcement learning programs do not learn from labeled pairs of inputs and outputs. Instead, they receive feedback for their decisions, but errors are not explicitly corrected. For example, a reinforcement learning program that is learning to play a side-scrolling video game like Super Mario Bros may receive a <a href="./#reward">reward</a> when it completes a level or exceeds a certain score, and a <a href="../p/#punishment">punishment</a> when it loses a life. However, this [supervised feedback] is not associated with specific decisions to run, avoid Goombas, or pick up fire flowers.</p>
<p><img alt="" src="../img/r/reinforcement_learning.png" width="100%" /></p>
<p>Imagine a mouse in a maze trying to find hidden pieces of cheese. The more times we expose the mouse to the maze, the better it gets at finding the cheese. At first, the mouse might move randomly, but after some time, the mouses experience helps it realize which actions bring it closer to the cheese. The process for the mouse mirrors what we do with Reinforcement Learning (RL) to train a system or a game. Generally speaking, RL is a [machine learning] method that helps an <a href="./#reinforcement-learning-rl-agent">RL agent</a> learn from experience. By recording actions and using a trial-and-error approach in a set <a href="../e/#environment">environment</a>, RL can maximize a <a href="../c/#cumulative-reward">cumulative reward</a>. In our example, the mouse is the <a href="./#reinforcement-learning-rl-agent">RL agent</a> and the maze is the <a href="../e/#environment">environment</a>. The set of possible <a href="../a/#action">actions</a> for the mouse are: move front, back, left or right. The <a href="./#reward">reward</a> is the cheese.</p>
<p>You can use RL when you have little to no historical data about a problem, because it doesnt need information in advance (unlike traditional machine learning methods). In a RL framework, you learn from the data as you go. Not surprisingly, RL is especially successful with games, especially games of <a href="../p/#perfect-information">perfect information</a> like chess and Go. With games, feedback from the <a href="./#reinforcement-learning-rl-agent">RL agent</a> and the <a href="../e/#environment">environment</a> comes quickly, allowing the model to learn fast. The downside of RL is that it can take a very long time to train if the problem is complex. Just as IBMs <a href="../d/#deep-blue-challenge">Deep Blue</a> beat the best human chess player in 1997, <a href="../a/#alphago-model">AlphaGo</a>, a RL-based algorithm, beat the best Go player in 2016. The current pioneers of RL are the teams at <a href="../d/#deepmind-company">DeepMind</a> in the UK. </p>
<p>On April, 2019, the <a href="../o/#openai-five-model">OpenAI Five</a> team was the first AI to beat a world champion team of e-sport Dota 2, a very complex video game that the <a href="../o/#openai-five-model">OpenAI Five</a> team chose because there were no RL algorithms that were able to win it at the time. The same AI team that beat Dota 2s champion human team also developed a robotic hand that can reorient a block. </p>
<p>This category of AI algorithms involves a feedback loop that comes back to adjust decision variables in order to improve decision-making over time. One marketing application is establishing a feedback loop between marketing mix actions and KPIs in order to improve upon the marketing mix decisions over time.</p>
<p>One industry use case of this is Netflixs What to watch queue, which consistently responds to what viewers choose to watch by adjusting their suggestions of other media they might also enjoy.</p>
<p>Some possible uses for reinforcement learning:</p>
<ul>
<li>Showing different types of ads based on whether consumers click on them or not.</li>
<li>Continuously optimizing promotional offerings based on what consumers have purchased or not in the past.</li>
<li>Continuously optimizing A/B test experiments based on the output of previous experiments.</li>
<li>Providing guidance on what A/B tests to run.</li>
</ul>
<iframe src="https://www.youtube.com/embed/mqma6GpM7vM" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://neptune.ai/blog/category/reinforcement-learning">https://neptune.ai/blog/category/reinforcement-learning</a></li>
<li><a href="http://karpathy.github.io/2016/05/31/rl/">http://karpathy.github.io/2016/05/31/rl/</a></li>
<li>Tutorials<ul>
<li><a href="https://rl-lab.com/">https://rl-lab.com/</a></li>
<li><a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction">https://huggingface.co/learn/deep-rl-course/unit0/introduction</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../a/#action">Action</a>, <a href="../a/#action-space">Action Space</a>, <a href="../c/#continual-reinforcement-learning-crl">Continual Reinforcement Learning</a>, <a href="../d/#delayed-reward">Delayed Reward</a>, <a href="../e/#environment">Environment</a>, <a href="../e/#exploitation">Exploitation</a>, <a href="../e/#exploration">Exploration</a>, <a href="../l/#learning-method">Learning Method</a>, [Machine Learning], [Machine Learning Algorithm], <a href="../m/#markov-decision-process-mdp">Markov Decision Process</a>, <a href="../m/#meta-learning">Meta-Learning</a>, <a href="../o/#observation">Observation</a>, <a href="./#reward-shaping">Reward Shaping</a>, <a href="../s/#state">State</a></p>
<h2 id="reinforcement-learning-rl-agent">Reinforcement Learning (RL) Agent<a class="headerlink" href="#reinforcement-learning-rl-agent" title="Permanent link">#</a></h2>
<p>In <a href="./#reinforcement-learning-rl">reinforcement learning</a>, an <a href="../a/#agent">agent</a> whose goal is to maximize its <a href="../c/#cumulative-reward">cumulative reward</a>.
 To observe the right behavior, be sure to use appropriate <a href="./#reward">reward</a> and correct <a href="./#reward-shaping">reward shaping</a>.</p>
<p>Examples:</p>
<ul>
<li>In <a href="../a/#aws-deepracer-service">AWS DeepRacer</a>, the goal of the program running on the car is to go around the track as fast as possible without getting out of the track.</li>
</ul>
<p>The agent simulates the AWS DeepRacer vehicle in the simulation for training. More specifically, it embodies the neural network that controls the vehicle, taking inputs and deciding <a href="../a/#action">actions</a>. <mark>The agent embodies a neural network that represents a function to approximate the agent's policy.</mark></p>
<ul>
<li>The essence of Reinforced Learning is to enforce behavior based on the actions performed by the agent. The agent is rewarded if the action positively affects the overall goal.</li>
<li>The basic aim of reinforcement Learning is reward maximization. The agent is trained to take the best action to maximize the overall reward.</li>
<li>RL agents work by using the already known exploited information or exploring unknown information about an environment.</li>
<li>...</li>
</ul>
<p>Its also important to understand that the learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the <a href="../e/#environment">environment</a>.</p>
<p>During the training phase of an <a href="./#reinforcement-learning-rl-agent">RL agent</a>, its policy is updated after each <a href="../i/#iteration">iteration</a> based on the preset <a href="../l/#learning-rate">learning rate</a>, an <a href="../h/#hyperparameter">hyperparameter</a></p>
<p>See also <a href="./">R</a>, <a href="../a/#addiction">Addiction</a></p>
<h2 id="reinforcement-learning-rl-algorithm">Reinforcement Learning (RL) Algorithm<a class="headerlink" href="#reinforcement-learning-rl-algorithm" title="Permanent link">#</a></h2>
<p><a href="../p/#policy-gradient-algorithm">Policy Gradient Algorithms</a></p>
<ul>
<li>[Actor-Critic with Experience (ACER)]</li>
<li>[Advanced Actor-Critic (A2C)]</li>
<li>[Asynchronous Advanced Actor-Critic (A3C)]</li>
<li><a href="../d/#deep-deterministic-policy-gradient-ddpg-algorithm">Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="../p/#proximal-policy-optimization-ppo-algorithm">Proximal Policy Optimization (PPO)</a></li>
<li>[Soft Actor Critic (SAC)]</li>
<li>...</li>
</ul>
<p><a href="../v/#value-based-algorithm">Value-Based Algorithms</a></p>
<ul>
<li><a href="../q/#q-learning-algorithm">Q-Learning</a> </li>
<li><a href="../s/#state-action-reward-state-action-sarsa-algorithm">State-Action-Reward-State-Action (SARSA)</a></li>
</ul>
<p>RL with Targeted feedback</p>
<ul>
<li>[Reinforcement Learning From Human Feedback] - human, slow and expensive</li>
<li>[Reinforcement Learning From AI Feedback] - AI, fast and low cost, but feedback on final results</li>
<li>[Reinforcement Learning With Executive Feedback] - AI, fast, low cost, on intermediate results</li>
<li>[Reinforcement Learning Coordinated Feedback] - 2 &lt;&gt; teachers, on is LLM, the other is tool/validation based. Used with coding.</li>
</ul>
<p>Others:</p>
<ul>
<li>[Evolutionary Algorithms] = Generate policies through ab evolutionary process of selection, mutation, and fitness evaluation. Ex: <a href="../g/#genetic-programming">Genetic Programming</a>, <a href="../e/#evolution-strategy-es">Evolution Strategy</a></li>
<li><a href="../m/#model-based-reinforcement-learning">Model-Based RL</a> = Learn model of environment transistions and rewards, then optimizes policy through planning. Ex: <a href="../d/#dyna-model">Dyna Model</a>, <a href="../a/#alphago-model">AlphaGo</a></li>
<li>[Inverse RL] = learn reward function from expert demonstrations. Allows mimicking behavior without rewards.</li>
<li><a href="../h/#hierarchical-rl">Hierarchical RL</a> = Decomposes problem into hierarchy of sub-policies over different timescales</li>
<li><a href="../t/#transfer-learning">Transfer Learning</a> = Leverage knowledge fro previous tasks to accelerate learning on new tasks.</li>
</ul>
<p><img alt="" src="../img/r/reinforcement_learning_with_targeted_feedback.png" width="100%" /></p>
<p>More at:
  * articles
    * <a href="https://www.turingpost.com/p/rl-f">https://www.turingpost.com/p/rl-f</a></p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="reinforcement-learning-coordinated-feedback-rlcf">Reinforcement Learning Coordinated Feedback (RLCF)<a class="headerlink" href="#reinforcement-learning-coordinated-feedback-rlcf" title="Permanent link">#</a></h2>
<p>RLCF leverages compiler and LLM feedback to reinforce code quality, guiding models to generate syntactically correct, semantically sound code. It's ideal for tuning code models without human input. This training happens after traditional pre-training but before task-specific fine-tuning.</p>
<p><img alt="" src="../img/r/reinforcement_learning_coordinated_feedback.avif" width="100%" /></p>
<p>More at:</p>
<ul>
<li>articles<ul>
<li><a href="https://www.turingpost.com/p/rl-f">https://www.turingpost.com/p/rl-f</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="reinforcement-learning-from-ai-feedback-rlaif">Reinforcement Learning from AI Feedback (RLAIF)<a class="headerlink" href="#reinforcement-learning-from-ai-feedback-rlaif" title="Permanent link">#</a></h2>
<p>~ popularized by <a href="../a/#anthropic-company">Anthropic</a> as a play on word on <a href="./#reinforcement-learning-from-human-feedback-rlhf">RLHF</a>. Similar to <a href="../c/#constitutional-ai">constitutional AI</a></p>
<p>It is expensive to collect accurate labels to implement traditional <a href="./#reinforcement-learning-from-human-feedback-rlhf">RLHF</a> model, RLAIF uses another off-the-shelf model (AI) to evaluate the results of your primary model. Using AI for evaluation saves time, is more efficient, and gives comparable performance to <a href="./#reinforcement-learning-from-human-feedback-rlhf">RLHF</a> in many use cases.</p>
<object data="https://arxiv.org/pdf/2212.08073" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2212.08073">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>paper - <a href="https://arxiv.org/abs/2212.08073">https://arxiv.org/abs/2212.08073</a></li>
<li>constitutional AI - <a href="https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback">https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback</a></li>
<li>articles<ul>
<li><a href="https://www.turingpost.com/p/rl-f">https://www.turingpost.com/p/rl-f</a></li>
</ul>
</li>
</ul>
<p>See also <a href="../f/">F</a>, ...</p>
<h2 id="reinforcement-learning-from-human-feedback-rlhf">Reinforcement Learning from Human Feedback (RLHF)<a class="headerlink" href="#reinforcement-learning-from-human-feedback-rlhf" title="Permanent link">#</a></h2>
<p>Reinforcement learning process using human feedback as a reward model. RLHF is use in InstructGPT model, a precursor to ChatGPT model. A way to prevent or make <a href="./#red-teaming">Red Teaming</a> language models more difficult?</p>
<object data="https://arxiv.org/pdf/2203.02155" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2203.02155">Download PDF</a>.
    </p>
</object>

<object data="https://arxiv.org/pdf/1706.03741" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/1706.03741">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>paper <ul>
<li><a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></li>
<li><a href="https://arxiv.org/abs/1706.03741">https://arxiv.org/abs/1706.03741</a></li>
</ul>
</li>
<li><a href="https://huggingface.co/blog/rlhf">https://huggingface.co/blog/rlhf</a></li>
<li>articles<ul>
<li>RLHF vs - <a href="https://www.turingpost.com/p/rl-f">https://www.turingpost.com/p/rl-f</a></li>
<li>RLHF is flawed? - <a href="https://astralcodexten.substack.com/p/perhaps-it-is-a-bad-thing-that-the">https://astralcodexten.substack.com/p/perhaps-it-is-a-bad-thing-that-the</a></li>
<li>challenges - <a href="https://bdtechtalks.com/2023/09/04/rlhf-limitations/">https://bdtechtalks.com/2023/09/04/rlhf-limitations/</a></li>
<li>instructGPT - <a href="https://tmmtt.medium.com/the-instructgpt-e25797d8f4df">https://tmmtt.medium.com/the-instructgpt-e25797d8f4df</a></li>
<li>what is RLHF - <a href="https://bdtechtalks.com/2023/01/16/what-is-rlhf/">https://bdtechtalks.com/2023/01/16/what-is-rlhf/</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../c/#chatgpt-model">ChatGPT Model</a>, <a href="../f/#feedback-based-learning">Feedback-Based Learning</a>, <a href="../i/#instructgpt-model">InstructGPT Model</a>, <a href="./#reinforcement-learning-rl">Reinforcement Learning</a>, </p>
<h2 id="reinforcement-learning-with-executive-feedback-rlef">Reinforcement Learning with Executive Feedback (RLEF)<a class="headerlink" href="#reinforcement-learning-with-executive-feedback-rlef" title="Permanent link">#</a></h2>
<p>RLEF provides feedback to the model while it generates intermediate results. Instead of relying solely on external evaluations (like rewards or penalties based on results), RLEF provides feedback throughout the steps of an action sequence, allowing the model to adjust in real-time.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>Imagine you are builiding a tower with blocks.
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>As you place each block, a teacher says if you are doing it right or wrong right away, not just at the end.
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>So you can fix mistakes right away, making sure the tower stands strong!
</span></code></pre></div>
<p><img alt="" src="../img/r/reinforcement_learning_with_executive_feedback.avif" width="100%" /></p>
<p>More at:</p>
<ul>
<li>articles<ul>
<li>RLEF vs - <a href="https://www.turingpost.com/p/rl-f">https://www.turingpost.com/p/rl-f</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="relation">Relation<a class="headerlink" href="#relation" title="Permanent link">#</a></h2>
<p>A triples (X, r, Y)</p>
<p>See also <a href="./">R</a>, <a href="./#relation-extraction">Relation Extraction</a></p>
<h2 id="relation-extraction">Relation Extraction<a class="headerlink" href="#relation-extraction" title="Permanent link">#</a></h2>
<p>Extract relations between entities in a text or image to build a scene graph.  Possible methods:</p>
<ul>
<li>text<ul>
<li>rule-based technique: 'such as', 'including' , ...</li>
<li>supervised technique: stack binary classifier to determine if there is a specific relation between 2 entities <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/26a0.svg" title=":warning:" /> Very expensive to label </li>
<li>distant supervision: If 2 entities belongs to a certain relation, any sentence containing those 2 entities is likely to express a relation then 2 entities  </li>
</ul>
</li>
<li>video</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../e/#entity-extraction">Entity Extraction</a>, <a href="./#relation">Relation</a>, <a href="../s/#scene-graph">Scene Graph</a></p>
<h2 id="relational-deep-learning-rdl">Relational Deep Learning (RDL)<a class="headerlink" href="#relational-deep-learning-rdl" title="Permanent link">#</a></h2>
<p>Data mining using <a href="../g/#graph-neural-network-gnn">GNN</a> to learn embedding without <a href="../f/#feature-engineering">feature engineering</a></p>
<object data="https://arxiv.org/pdf/2312.04615" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2312.04615">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>paper - <a href="https://arxiv.org/abs/2312.04615">https://arxiv.org/abs/2312.04615</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="./#relational-deep-learning-benchmark-relbench">RelBench</a></p>
<h2 id="relational-deep-learning-benchmark-relbench">Relational Deep Learning Benchmark (RelBench)<a class="headerlink" href="#relational-deep-learning-benchmark-relbench" title="Permanent link">#</a></h2>
<p>The Relational Deep Learning Benchmark (RelBench) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on relational databases. RelBench datasets are automatically downloaded, processed, and split using the Data Loader. The model performance can be evaluated using the Evaluator in a unified manner. RelBench is a community-driven initiative in active development. We expect the benchmark datasets to evolve.</p>
<object data="https://relbench.stanford.edu/paper.pdf" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://relbench.stanford.edu/paper.pdf">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>Site - <a href="https://relbench.stanford.edu/">https://relbench.stanford.edu/</a></li>
<li>paper - </li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="relative-approximation-error-rae">Relative Approximation Error (RAE)<a class="headerlink" href="#relative-approximation-error-rae" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, <a href="../p/#prediction-error">Prediction Error</a></p>
<h2 id="relative-entropy">Relative Entropy<a class="headerlink" href="#relative-entropy" title="Permanent link">#</a></h2>
<p>See [Kullback-Leibler Divergence]</p>
<h2 id="relevancy">Relevancy<a class="headerlink" href="#relevancy" title="Permanent link">#</a></h2>
<p>Relevancy --&gt; approximate of neightbor bias used in <a href="../s/#similarity-metric">similarity metrics</a></p>
<p>Low relevancy = this hot dog looks like this ice cream.</p>
<p>High relevancy = this hot dog looks like this other hot dog.</p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="replaced-word-prediction">Replaced Word Prediction<a class="headerlink" href="#replaced-word-prediction" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, <a href="../s/#self-supervised-learning-ssl">Self-Supervised Learning</a></p>
<h2 id="replay-buffer">Replay Buffer<a class="headerlink" href="#replay-buffer" title="Permanent link">#</a></h2>
<p>See <a href="./#replay-memory">Replay Memory</a></p>
<h2 id="replay-memory">Replay Memory<a class="headerlink" href="#replay-memory" title="Permanent link">#</a></h2>
<p>In DQN-like algorithms, the memory used by the <a href="./#reinforcement-learning-rl-agent">RL agent</a> to store state transitions for use in <a href="../e/#experience-replay">experience replay</a>.</p>
<p>Used for <a href="../e/#experience-replay">experience replay</a></p>
<p>Circular buffer of fixed size that stores the last trailing state transitions, aka <a href="../e/#experience">experience</a>. To train the <a href="../d/#deep-q-network-dqn">Deep Q-Network (DQN)</a>, the training algorithm sample from the <a href="../e/#experience">experiences</a> from that memory!  </p>
<iframe src="https://www.youtube.com/embed/Bcuj2fTH4_4" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">R</a>, ...</p>
<h2 id="replicate-company">Replicate Company<a class="headerlink" href="#replicate-company" title="Permanent link">#</a></h2>
<p>More at:</p>
<ul>
<li>site - <a href="https://replicate.com/">https://replicate.com/</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="replit-company">Replit Company<a class="headerlink" href="#replit-company" title="Permanent link">#</a></h2>
<iframe src="https://www.youtube.com/embed/D4f7_lPwXtE" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/JI2rmCII4fg" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>site - <a href="https://replit.com/">https://replit.com/</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../c/#custom-gpt">Custom GPT</a></p>
<h2 id="repls">Repls<a class="headerlink" href="#repls" title="Permanent link">#</a></h2>
<p>Repl with</p>
<ul>
<li>Tutorial + video</li>
<li>GIT integration</li>
</ul>
<iframe src="https://www.youtube.com/embed/l650luzyQGs" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/3uiOuIZlx_U" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">R</a>, ...</p>
<h2 id="representation">Representation<a class="headerlink" href="#representation" title="Permanent link">#</a></h2>
<p>The process of mapping data to useful features.</p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="representation-space">Representation Space<a class="headerlink" href="#representation-space" title="Permanent link">#</a></h2>
<p>The meaning of the dimensions in the representation space is set by the label/ground truth + backpropagation from the loss/cost function (?)</p>
<p>See also <a href="./">R</a>, <a href="../b/#backpropagation">Backpropagation</a>, <a href="../d/#decoder-representation-space">Decoder Representation Space</a>, <a href="../e/#encoder-representation-space">Encoder Representation Space</a>, <a href="../g/#ground-truth">Ground Truth</a>, <a href="../l/#label">Label</a>, <a href="../l/#loss-function">Loss Function</a></p>
<h2 id="reproducibility">Reproducibility<a class="headerlink" href="#reproducibility" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, <a href="../m/#model-governance">Model Governance</a></p>
<h2 id="reptile">Reptile<a class="headerlink" href="#reptile" title="Permanent link">#</a></h2>
<p>Use the direction of theta (parameter?) to change phy (hyper parameter).</p>
<p>See also <a href="./">R</a>, <a href="../m/#model-agnostic-meta-learning-maml">MAML</a>, <a href="../m/#meta-learning">Meta-Learning</a></p>
<h2 id="reranking">Reranking<a class="headerlink" href="#reranking" title="Permanent link">#</a></h2>
<p>~ reordering results returned by a <a href="./#ranking">ranking</a> engine that you do not control or do not want to change/touch using another engine that use different ranking dimensions to up-rank or down-rank previously returned results.</p>
<p>Used in <a href="../s/#semantic-search">semantic search</a> to find the best answer to a question!</p>
<p>Reranking refers to the process of reordering or reorganizing a list of items based on certain criteria to improve the quality of the <a href="./#ranking">ranking</a>. This concept is often used in [information retrieval], search engines, recommendation systems, and other applications where a list of items needs to be presented in a specific order to provide better user experiences or more relevant results.</p>
<p>In various scenarios, the initial <a href="./#ranking">ranking</a> of items might not be perfect or optimized for the user's preferences, needs, or relevance. Reranking aims to address this by adjusting the order of items in the list to better match the user's intent or to improve the quality of the presented information. The reranking process can be guided by different factors, such as user feedback, relevance scores, contextual information, or machine learning models.</p>
<p>Example of applications:</p>
<ul>
<li>Implementing societal objective function in ranking of social posts</li>
<li><a href="./#retrieval-augmented-generation-rag-system">RAG</a> implementing document semantic search with reranking</li>
</ul>
<p><img alt="" src="../img/r/reranking_social_objective_functions.png" width="100%" /></p>
<p><img alt="" src="../img/r/reranking_with_semantic_search.png" width="100%" /></p>
<iframe src="https://www.youtube.com/embed/GSixIsI1eZE" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>applications<ul>
<li>social objective functions - <a href="https://youtu.be/IzK55L26FgA?t=8579">https://youtu.be/IzK55L26FgA?t=8579</a></li>
</ul>
</li>
<li>notebooks<ul>
<li><a href="https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb">https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="resample">Resample<a class="headerlink" href="#resample" title="Permanent link">#</a></h2>
<p>a new sample of data that is created by selecting observations from an existing dataset.</p>
<p>See also <a href="./">R</a>, <a href="./#resampling-method">Resampling Method</a></p>
<h2 id="resampling-method">Resampling Method<a class="headerlink" href="#resampling-method" title="Permanent link">#</a></h2>
<p>are techniques used to estimate the performance of a model or algorithm on unseen data by using the existing dataset. The most common resampling methods are:</p>
<ul>
<li>bootstrap sampling</li>
<li>jackknife sampling</li>
<li>cross-validation sampling</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../b/#bootstrap-sampling-method">Bootstrap Sampling Method</a>, <a href="../c/#cross-validation-sampling-method">Cross-Validation Sampling Method</a>, <a href="../j/#jackknife-sampling-method">Jackknife Sampling Method</a>, <a href="./#resample">Resample</a></p>
<h2 id="reshaping">Reshaping<a class="headerlink" href="#reshaping" title="Permanent link">#</a></h2>
<p>Before multiplying a <a href="../v/#vector">vector</a> by a <a href="../m/#matrix">matrix</a>, we need to reshape the <a href="../v/#vector">vector</a> to have 2 dimensions! Likewise if you want to multiply a <a href="../m/#matrix">matrix</a> and a <a href="../t/#tensor">tensor</a>.</p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="residual">Residual<a class="headerlink" href="#residual" title="Permanent link">#</a></h2>
<p>Y - estimateY for a given X. Use the residual in the loss function. <img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/26a0.svg" title=":warning:" /> How do you use the residuals in the loss function? absolute values? not easy to work with. Squares? Yes.</p>
<p>See also <a href="./">R</a>, <a href="../l/#linear-regression">Linear Regression</a>, <a href="../l/#loss-function">Loss Function</a></p>
<h2 id="residual-block">Residual Block<a class="headerlink" href="#residual-block" title="Permanent link">#</a></h2>
<p>In Residual Networks, to solve the problem of the vanishing/exploding gradient, this architecture introduced the concept called Residual Blocks. In this network, we use a technique called skip connections. The skip connection connects activations of a  layer to further layers by skipping some layers in between. This forms a residual block. Resnets are made by stacking these residual blocks together.  The approach behind this network is instead of layers learning the underlying mapping, we allow the network to fit the residual mapping. So, instead of say H(x), initial mapping, let the network fit, </p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>F(x) := H(x) - x which gives H(x) := F(x) + x. 
</span></code></pre></div>
<p><img alt="" src="../img/r/residual_block.png" width="100%" /></p>
<p>See also <a href="./">R</a>, [Residual Network Model], <a href="../s/#skip-connection">Skip Connection</a></p>
<h2 id="residual-network-resnet-model">Residual Network (ResNET) Model<a class="headerlink" href="#residual-network-resnet-model" title="Permanent link">#</a></h2>
<p>ResNET, short for Residual Networks is a classic neural network used as a backbone for many computer vision tasks = <code>a CNN image model</code> This model was the winner of ImageNET challenge in 2015. The fundamental breakthrough with ResNET was it allowed us to train extremely deep neural networks with 150+layers successfully. Prior to ResNET training very deep neural networks was difficult due to the <a href="../v/#vanishing-gradient-problem">problem of vanishing gradients</a>.</p>
<object data="https://arxiv.org/pdf/1512.03385" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/1512.03385">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>paper - <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></li>
<li><a href="https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/">https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/</a></li>
</ul>
<p>See also <a href="./">R</a>, [Convoluted Neural Network], <a href="../c/#computer-vision-cv">Computer Vision</a>, [Rectified Linear Unit Activation Function], <a href="./#residual-block">Residual Block</a>, <a href="../v/#vanishing-gradient-problem">Vanishing Gradient Problem</a></p>
<h2 id="resilient-backpropagation-rprop-algorithm">Resilient Backpropagation (Rprop) Algorithm<a class="headerlink" href="#resilient-backpropagation-rprop-algorithm" title="Permanent link">#</a></h2>
<p>Rprop, short for resilient <a href="../b/#backpropagation">backpropagation</a>, is a type of [optimization algorithm] commonly used in [machine learning] and <a href="../a/#artificial-neural-network-ann">neural networks</a> to minimize the <a href="../c/#cost-function">cost function</a> or error function.</p>
<p>Unlike other optimization algorithms that use a fixed <a href="../l/#learning-rate">learning rate</a>, Rprop adapts the step size for each parameter based on the sign of the gradient. This allows the algorithm to take larger steps in flat regions and smaller steps in steep regions of the cost function, thus improving convergence speed.</p>
<p>The Rprop algorithm maintains a separate update value for each parameter and adjusts the update value based on the sign of the gradient at each iteration. If the gradient changes sign, the update value is reset to its initial value, otherwise, it is increased or decreased by a fixed factor. The step size for each parameter is then calculated based on the current update value and the sign of the gradient.</p>
<p>Rprop is particularly effective when dealing with high-dimensional optimization problems, noisy gradients, or sparse data. It is also computationally efficient and does not require the calculation of second-order derivatives.</p>
<p>One drawback of Rprop is that it may get stuck in local minima or plateaus, and it may not perform well in non-convex optimization problems. To address this issue, hybrid variants of Rprop have been proposed that combine it with other optimization algorithms such as Adam or momentum to improve its robustness and generalization capabilities.</p>
<p>More at:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Rprop">https://en.wikipedia.org/wiki/Rprop</a></li>
<li><a href="https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a">https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a</a></li>
</ul>
<p>See also <a href="./">R</a>, [Root Mean Square Propagation Algorithm]</p>
<h2 id="response-variable">Response Variable<a class="headerlink" href="#response-variable" title="Permanent link">#</a></h2>
<p>Y or prediction Y_hat There are many names for the output of a machine learning program. Several disciplines converge in machine learning, and many of those disciplines use their own terminology. We will refer to the output as the response variable. Other names for response variables include "dependent variables", "regressands", "criterion variables", "measured variables", "responding variables", "explained variables", "outcome variables", "experimental variables", "labels", and "output variables".</p>
<p>See also <a href="./">R</a>, <a href="../e/#explanatory-variable">Explanatory Variable</a>, <a href="../p/#predictor-variable">Predictor Variable</a>, <a href="./#regression-task">Regression</a></p>
<h2 id="responsible-ai-rai">Responsible AI (RAI)<a class="headerlink" href="#responsible-ai-rai" title="Permanent link">#</a></h2>
<p>Responsible AI (RAI) refers to the individual and collective effort to promote beneficial users of AI and safeguard stakeholders -- clients, employees, members of the public, and beyond -- from harms or risks associated with AI, while acting ethically as institutions and individuals</p>
<ul>
<li>At its core, RAI is about protecting and benefiting people and society</li>
<li>Using AI responsibly  is more than compliance, risk management, efficiency, or personal accountability</li>
<li>RAI encompasses the full spectrum of social, technical, business, and governance practices involved in advancing AI's use in society</li>
</ul>
<p>Related terms are <a href="../e/#ethical-ai">Ethical AI</a>, <a href="../t/#trustworthy-ai">Trustworthy AI</a></p>
<p>To whom are we responsible?</p>
<ul>
<li>Employees and shareholders</li>
<li>Clients and customers</li>
<li>Regulators and policymakers</li>
<li>People and planet</li>
</ul>
<p>What are we responsible for?</p>
<ul>
<li>Promoting ethical development, implementation, and monitoring throughout the AI life cycle</li>
<li>Centering well-being, human and civil rights, professional ethics, and people in our technology.</li>
</ul>
<p>What processes, tools, and norms can achieve these goals?</p>
<ul>
<li>Development and implementation of transparent and safe AI systems</li>
<li>Software toolkits, data science and data platform, controls and governance that enable RAI</li>
<li>Culture and education around responsible practices</li>
<li>Multi-objective and social good use cases</li>
</ul>
<object data="https://assets.ctfassets.net/3nanhbfkr0pc/acoo1Fj5uungnGNPJ3QWy/3a1dafd64f22efcf8f27380aafae9789/2021_RAI_Report-v3.pdf" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://assets.ctfassets.net/3nanhbfkr0pc/acoo1Fj5uungnGNPJ3QWy/3a1dafd64f22efcf8f27380aafae9789/2021_RAI_Report-v3.pdf">Download PDF</a>.
    </p>
</object>

<p>{% pdf "../pdf/r/responsible_ai_by_us_dod.pdf" %}</p>
<p>More at:</p>
<ul>
<li><a href="https://venturebeat.com/security/pwc-highlights-11-chatgpt-and-generative-ai-security-trends-to-watch-in-2023/">https://venturebeat.com/security/pwc-highlights-11-chatgpt-and-generative-ai-security-trends-to-watch-in-2023/</a></li>
<li>US DoD<ul>
<li><a href="https://www.cnn.com/videos/business/2023/05/11/nightcap-ai-drones-clip-orig-jc.cnn">https://www.cnn.com/videos/business/2023/05/11/nightcap-ai-drones-clip-orig-jc.cnn</a></li>
<li><a href="https://www.defense.gov/Spotlights/Artificial-Intelligence/">https://www.defense.gov/Spotlights/Artificial-Intelligence/</a></li>
<li><a href="https://www.diu.mil/responsible-ai-guidelines">https://www.diu.mil/responsible-ai-guidelines</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../a/#ai-alignment">AI Alignment</a>, <a href="../a/#ai-bias">AI Bias</a>, <a href="../a/#ai-ethics">AI Ethics</a>, [AI Fairness]</p>
<h2 id="restricted-boltzmann-machine-rbm">Restricted Boltzmann Machine (RBM)<a class="headerlink" href="#restricted-boltzmann-machine-rbm" title="Permanent link">#</a></h2>
<ul>
<li>Visible layer = what we observe</li>
<li>hidden layer = what we cannot see</li>
</ul>
<p>In a full Boltzmann machine, each node is connected to every other node and hence the connections grow exponentially. This is the reason we use RBMs. The restrictions in the node connections in RBMs are as follows:
   * Hidden nodes cannot be connected to one another.
   * Visible nodes connected to one another.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>Consider  Mary watches four movies out of the six available movies and rates four of them. Say, she watched m1, m3, m4 and m5 and likes m3, m5 (rated 1) and dislikes the other two, that is m1, m4 (rated 0) whereas the other two movies  m2, m6 are unrated. Now, using our RBM, we will recommend one of these movies for her to watch next. Say  
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>m3, m5 are of Drama genre.
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>m1, m4 are of Action genre.
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>Dicaprio played a role in m5.
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>m3, m5 have won Oscar.
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>Tarantino directed m4.
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>m2 is of the Action genre.
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>m6 is of both the genres Action and Drama, Dicaprio acted in it and it has won an Oscar.
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a>We have the following observations 
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>
</span><span id="__span-6-12"><a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a>Mary likes m3, m5 and they are of genre Drama, she probably likes Drama movies.
</span><span id="__span-6-13"><a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a>Mary dislikes m1, m4 and they are of action genre, she probably dislikes Action movies.
</span><span id="__span-6-14"><a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a>Mary likes m3, m5 and they have won an Oscar, she probably likes an Oscar movie.
</span><span id="__span-6-15"><a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a>Since Dicaprio acted in m5 and Mary likes it, she will probably like a movie in which Dicaprio acted.
</span><span id="__span-6-16"><a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a>Mary does not like m4 which is directed by Tarantino, she probably dislikes any movie directed by Tarantino.
</span><span id="__span-6-17"><a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a>Therefore, based on the observations and the details of m2, m6; our RBM recommends m6 to Mary (Drama, Dicaprio and Oscar matches both Marys interests and m6). This is how an RBM works and hence is used in recommender systems.
</span></code></pre></div>
<p><img alt="" src="../img/r/restricted_boltzmann_machine.jpeg" width="100%" /></p>
<iframe src="https://www.youtube.com/embed/Fkw0_aAtwIw" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">R</a>, <a href="../b/#boltzmann-machine">Boltzmann Machine</a></p>
<h2 id="rag-assessment-raga">RAG Assessment (RAGA)<a class="headerlink" href="#rag-assessment-raga" title="Permanent link">#</a></h2>
<p>~ A framework with metrics and LLM-generated data to evaluate the performance of your Retrieval-Augmented Generation pipeline</p>
<p>By now, we know that building a proof of concept for a Retrieval-Augmented Generation (RAG) application is easy, but making it production-ready is very difficult. Getting the RAG pipeline's performance to a satisfying state is especially difficult because of the different components in a RAG pipeline:</p>
<ul>
<li>Retriever component: retrieves additional context from an external database for the LLM to answer the query.</li>
<li>Generator component: generates an answer based on a prompt augmented with the retrieved information.</li>
</ul>
<p>When evaluating a RAG pipeline, you must evaluate both components separately and together to understand if and where the RAG pipeline still needs improvement. Additionally, to understand whether your RAG applications performance is improving, you must evaluate it quantitatively. For this, you will need two ingredients: An evaluation metric and an evaluation dataset.</p>
<p>Test for:</p>
<ul>
<li>Retrieval Quality</li>
<li>Relevance</li>
<li>Diversity</li>
<li>Test hallucinations</li>
<li>noise robustness - which useful info to extract from documents to provide useful response</li>
<li>Negative rejection - when the LLM/RAG does not know the answer</li>
<li>Information integration - the answer is in multiple documents</li>
<li>Counterfactual robustness - when documents contain errors</li>
<li>Unclear queries - query is sequence of words that does not make any sense!</li>
<li>Privacy breaches</li>
<li>Malicious use</li>
<li>Illegal activities</li>
<li>create harmful content</li>
<li>Inquiry about harmful activities</li>
<li>Security breaches</li>
<li>Emotional manipulation</li>
<li>Prefix injection</li>
<li>Refusal suppression</li>
<li>Mismatched generation</li>
<li>Out-of-domain questions</li>
<li>Test for completeness</li>
<li>Test for brand damage</li>
</ul>
<p>More at:</p>
<ul>
<li>articles<ul>
<li><a href="https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a">https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a</a></li>
<li><a href="https://www.rungalileo.io/blog/mastering-rag-8-scenarios-to-test-before-going-to-production">https://www.rungalileo.io/blog/mastering-rag-8-scenarios-to-test-before-going-to-production</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../l/#langsmith-python-module">LangSmith</a></p>
<h2 id="rag-generator">RAG Generator<a class="headerlink" href="#rag-generator" title="Permanent link">#</a></h2>
<p>A component in a <a href="./#retrieval-augmented-generation-rag-system">RAG</a> that uses its augmented context for additional information, but will also generate new info. Beware of hallucinations.</p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="rag-information-retriever-rag-ir">RAG Information Retriever (RAG-IR)<a class="headerlink" href="#rag-information-retriever-rag-ir" title="Permanent link">#</a></h2>
<p>A component of a <a href="./#retrieval-augmented-generation-rag-system">RAG</a> that uses its augmented context to give an answer. Using it results in limited hallucinations.</p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="retrieval-augmented-generation-rag-system">Retrieval-Augmented Generation (RAG) System<a class="headerlink" href="#retrieval-augmented-generation-rag-system" title="Permanent link">#</a></h2>
<p>~ A <a href="../c/#compound-ai-system">compound AI system</a> that consists of an LLM that works with a <a href="./#retriever">retriever</a></p>
<p>Fields:</p>
<ul>
<li>Document ID (DOI)   &lt;== used for deletion, not for query</li>
<li>Chunk ID (chunk id)</li>
<li>metadata &lt;== delete on metadata not available in pinecone serverless</li>
<li>vector ID (ID)<ul>
<li>prefix = field constructed for filtering (a bit like metadata) ex: <DOI>#<chunk-ID># ...</li>
</ul>
</li>
</ul>
<p>Retrieval-augmented generation is a technique used in <a href="../n/#natural-language-processing-nlp">natural language processing</a> that combines the power of both <a href="./#retrieval-based-model">retrieval models</a> and <a href="../g/#generative-model">generative models</a> to enhance the quality and relevance of generated text.</p>
<p>Now, retrieval-augmented generation combines these two approaches to overcome their individual limitations. In this framework, a retrieval model is used to retrieve relevant information from a knowledge base or a set of documents based on a given query or context. The retrieved information is then used as input or additional context for the generative model.</p>
<p>There are 2 components in RAGs:</p>
<ul>
<li><a href="./#rag-information-retriever-rag-ir">RAG Information Retriever (RAG-IR)</a> or reader componentbased on <a href="../v/#vector-retrieval">vector retrieval</a></li>
<li><a href="./#rag-generator">RAG Generator</a> or writer component</li>
</ul>
<p>RAG paradigm</p>
<ul>
<li>Naive RAG</li>
<li>Advanced RAG<ul>
<li><a href="../c/#corrective-retrieval-augmented-generation-crag-system">Corrective RAG</a> - advanced retriever</li>
<li><a href="../s/#self-reflective-retrieval-augmented-generation-sr-rag-system">Self-Reflective RAG</a> - RAG as a state machine</li>
<li>[Multimodal RAG] - Extract information from slide deck and other images</li>
</ul>
</li>
<li>[Modular RAG]</li>
</ul>
<p>Current evaluation frameworks:
  * <a href="./#rag-triad-of-metrics">RAG Triad Of Metrics</a>
  * <a href="./#recall-oriented-understudy-for-gisting-evaluation-rouge-score">ROUGE</a>
  * <a href="../b/#bilingual-evaluation-understudy-bleu-score">BLEU</a>
  * <a href="./#rag-assessment-raga">RAG Assessment (RAGA)</a>
  * [Automated RAG Evaluation System (ARES)]</p>
<p><a href="../l/#large-language-model-llm">Large Language Models (LLMs)</a> demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework.</p>
<p><img alt="" src="../img/r/retrieval_augmented_generation_emergence.png" width="100%" /></p>
<p><img alt="" src="../img/r/retrieval_augmented_generation.webp" width="100%" /></p>
<p><img alt="" src="../img/r/retrieval_augmented_generation_troubleshooting.png" width="100%" /></p>
<iframe src="https://www.youtube.com/embed/T-D1OfcDW1M" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/ypzmPwLH_Q4" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<object data="https://arxiv.org/pdf/2312.10997" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/2312.10997">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>paper - <a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a></li>
<li>code - <a href="https://github.com/Tongji-KGLLM/RAG-Survey">https://github.com/Tongji-KGLLM/RAG-Survey</a></li>
<li>articles<ul>
<li><a href="https://www.datacamp.com/tutorial/corrective-rag-crag">https://www.datacamp.com/tutorial/corrective-rag-crag</a></li>
<li><a href="https://community.fullstackretrieval.com/">https://community.fullstackretrieval.com/</a></li>
<li><a href="https://colabdoge.medium.com/what-is-rag-retrieval-augmented-generation-b0afc5dd5e79">https://colabdoge.medium.com/what-is-rag-retrieval-augmented-generation-b0afc5dd5e79</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, [Modular Reasoning Knowledge and Language System], <a href="../v/#vector-retrieval">Vector Retrieval</a></p>
<h2 id="rag-triad-of-metrics">RAG Triad Of Metrics<a class="headerlink" href="#rag-triad-of-metrics" title="Permanent link">#</a></h2>
<p>More at:</p>
<ul>
<li><a href="https://learn.deeplearning.ai/building-evaluating-advanced-rag/lesson/3/rag-triad-of-metrics">https://learn.deeplearning.ai/building-evaluating-advanced-rag/lesson/3/rag-triad-of-metrics</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="retrieval-based-model">Retrieval-Based Model<a class="headerlink" href="#retrieval-based-model" title="Permanent link">#</a></h2>
<p>These models are designed to retrieve relevant information from a given set of documents or a knowledge base. They typically use techniques like [information retrieval] or <a href="../s/#semantic-search">semantic search</a> techniques to identify the most relevant pieces of information based on a given query. Retrieval-based models excel at finding accurate and specific information but lack the ability to generate creative or novel content.</p>
<p>Retrieval models:</p>
<ul>
<li>Neural Network <a href="../e/#embedding">Embeddings</a></li>
<li><a href="../b/#best-match-25-bm25-retrieval-model">Best Match 25</a></li>
<li><a href="../t/#term-frequency-inverse-document-frequency-tf-idf-retrieval-model">Term Frequency-Inverse Document Frequency (TF-IDF)</a></li>
<li><a href="../l/#linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)</a></li>
<li>Hybrid search = a combination of the above methodologies with different weightings.</li>
</ul>
<p>More at:</p>
<ul>
<li><a href="https://colabdoge.medium.com/what-is-rag-retrieval-augmented-generation-b0afc5dd5e79">https://colabdoge.medium.com/what-is-rag-retrieval-augmented-generation-b0afc5dd5e79</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="retrieval-interleaved-generation-rig-system">Retrieval-Interleaved Generation (RIG) System<a class="headerlink" href="#retrieval-interleaved-generation-rig-system" title="Permanent link">#</a></h2>
<p>Retrieval-Interleaved Generation (RIG) is a technique that combines retrieval and generation to enhance the quality and factuality of AI model outputs. Here's how it works:</p>
<ul>
<li>During generation, the model periodically pauses to retrieve relevant information from a knowledge base</li>
<li>The retrieved information is then integrated into the ongoing generation process, helping guide and inform the output</li>
</ul>
<p>The key benefits are:</p>
<ul>
<li>Improved factual accuracy since the model can reference external knowledge</li>
<li>Better grounding of responses in verified information</li>
<li>Reduced hallucination as the model isn't purely relying on its learned parameters</li>
</ul>
<p>A simple example:</p>
<p>If asked about Barack Obama's presidency, a RIG system might:</p>
<ul>
<li>Start generating about Obama</li>
<li>Pause to retrieve specific dates and events from a knowledge base</li>
<li>Continue generating while incorporating the retrieved facts</li>
<li>Repeat this process throughout the response</li>
</ul>
<p>This is different from pure retrieval (which just looks up answers) or pure generation (which creates text from learned parameters). RIG tries to get the best of both approaches.</p>
<p><img alt="" src="../img/r/retrieval_interleaved_generation.webp" width="100%" /></p>
<iframe src="https://www.youtube.com/embed/XclqphX9VAM" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>articles<ul>
<li><a href="https://medium.com/@sahin.samia/retrieval-interleaved-generation-rig-using-llm-what-is-it-and-how-it-works-aa8be0e27bbc">https://medium.com/@sahin.samia/retrieval-interleaved-generation-rig-using-llm-what-is-it-and-how-it-works-aa8be0e27bbc</a></li>
<li>RIG vs RAG - <a href="https://research.google/blog/grounding-ai-in-reality-with-a-little-help-from-data-commons/">https://research.google/blog/grounding-ai-in-reality-with-a-little-help-from-data-commons/</a></li>
</ul>
</li>
</ul>
<p>See also <a href="./">R</a>, <a href="../d/#data-commons-dataset">Data Commons Dataset</a></p>
<h2 id="retriever">Retriever<a class="headerlink" href="#retriever" title="Permanent link">#</a></h2>
<p>See [Information Retriever]</p>
<h2 id="reward">Reward<a class="headerlink" href="#reward" title="Permanent link">#</a></h2>
<p><img alt="" class="twemoji" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/26a0.svg" title=":warning:" /> The delta of the reward can reward negative behavior!</p>
<p>In [Reinforcement Learning (RL)], a reward is a form of feedback from a real or simulated <a href="../e/#environment">environment</a>, a program, or a human.</p>
<p>The reward is the score given as feedback to the agent when it takes an action in a given state. A reward can be positive or negative. In training the AWS DeepRacer model, the reward is returned by a <a href="./#reward-function">reward function</a>. In general, you define or supply a reward function to specify what is desirable or undesirable action for the <a href="./#reinforcement-learning-rl-agent">RL agent</a> to take in a given <a href="../s/#state">state</a>. There is an immediate reward associated with any <a href="../a/#action">action</a>. In Contrast to Reward, which implies a short-term gain, <a href="../q/#q-value">Q-Value</a> refers to the long-term return with discount.</p>
<p>Rewards must consider the following:
  * Positive vs negative reward (or Cost)
  * Immediate vs delayed vs cumulative
  * Immediate vs long-term reward, aka Q-value.
  * Long-term reward = Good in the long term
    * Cumulative reward = Good in the long run? 
    * Return / Value = total reward we are expecting to get
      * Aim for high value
      * value function = expected sum of discounted reward from a given state for all action or particular action
  * Intrinsic vs Extrinsic rewards
    * Extrinsic
      * Examples: Capture, achieve, collect, ...
      * Specific to the environment
        * "Getting rich"
        * "Control of resources"
        * "Power"
    * Intrinsic
      * Examples: Curiosity, (im)patience, happiness,love, empathy, ...
  * Deterministic vs stochastic
    * Deterministic = always the one expected
    * Stochastic = change all the time, but can be defined with probabilities</p>
<p>How you write your reward function matters! This is called <a href="./#reward-shaping">reward shaping</a>. More important than the immediate reward is the <a href="../c/#cumulative-reward">cumulative reward</a> which the <a href="../a/#agent">agent</a> is optimizing on! </p>
<p>See also <a href="./">R</a>, <a href="../a/#addiction">Addiction</a>, <a href="../c/#cost">Cost</a></p>
<h2 id="reward-function">Reward Function<a class="headerlink" href="#reward-function" title="Permanent link">#</a></h2>
<p>How you write your reward function matters to obtain the expected behavior of the <a href="./#reinforcement-learning-rl-agent">RL agent</a>. This is called <a href="./#reward-shaping">reward shaping</a></p>
<p>An agent receives something from this in order to learn the appropriate actions to take. With all these parameters at your disposal, you can define a reward function to incentivize whatever driving behavior you like. Let's see a few examples of reward functions and how they use the parameters to determine a reward.</p>
<p>Example of reward function for a self-driving car in <a href="../a/#aws-deepracer-service">AWS DeepRacer</a>.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">reward_function</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="sd">    Example of rewarding the agent to follow center line</span>
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="sd">    &#39;&#39;&#39;</span>
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a>
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>    <span class="c1"># Read input parameters</span>
</span><span id="__span-7-7"><a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>    <span class="n">track_width</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;track_width&#39;</span><span class="p">]</span>
</span><span id="__span-7-8"><a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a>    <span class="n">distance_from_center</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;distance_from_center&#39;</span><span class="p">]</span>
</span><span id="__span-7-9"><a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a>
</span><span id="__span-7-10"><a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a>    <span class="c1"># Calculate 3 markers that are increasingly further away from the center line</span>
</span><span id="__span-7-11"><a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a>    <span class="n">marker_1</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">track_width</span>
</span><span id="__span-7-12"><a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a>    <span class="n">marker_2</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="o">*</span> <span class="n">track_width</span>
</span><span id="__span-7-13"><a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a>    <span class="n">marker_3</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">track_width</span>
</span><span id="__span-7-14"><a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a>
</span><span id="__span-7-15"><a id="__codelineno-7-15" name="__codelineno-7-15" href="#__codelineno-7-15"></a>    <span class="c1"># Give higher reward if the car is closer to center line and vice versa</span>
</span><span id="__span-7-16"><a id="__codelineno-7-16" name="__codelineno-7-16" href="#__codelineno-7-16"></a>    <span class="k">if</span> <span class="n">distance_from_center</span> <span class="o">&lt;=</span> <span class="n">marker_1</span><span class="p">:</span>
</span><span id="__span-7-17"><a id="__codelineno-7-17" name="__codelineno-7-17" href="#__codelineno-7-17"></a>        <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-7-18"><a id="__codelineno-7-18" name="__codelineno-7-18" href="#__codelineno-7-18"></a>    <span class="k">elif</span> <span class="n">distance_from_center</span> <span class="o">&lt;=</span> <span class="n">marker_2</span><span class="p">:</span>
</span><span id="__span-7-19"><a id="__codelineno-7-19" name="__codelineno-7-19" href="#__codelineno-7-19"></a>        <span class="n">reward</span> <span class="o">=</span> <span class="mf">0.5</span>
</span><span id="__span-7-20"><a id="__codelineno-7-20" name="__codelineno-7-20" href="#__codelineno-7-20"></a>    <span class="k">elif</span> <span class="n">distance_from_center</span> <span class="o">&lt;=</span> <span class="n">marker_3</span><span class="p">:</span>
</span><span id="__span-7-21"><a id="__codelineno-7-21" name="__codelineno-7-21" href="#__codelineno-7-21"></a>        <span class="n">reward</span> <span class="o">=</span> <span class="mf">0.1</span>
</span><span id="__span-7-22"><a id="__codelineno-7-22" name="__codelineno-7-22" href="#__codelineno-7-22"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-7-23"><a id="__codelineno-7-23" name="__codelineno-7-23" href="#__codelineno-7-23"></a>        <span class="n">reward</span> <span class="o">=</span> <span class="mf">1e-3</span>  <span class="c1"># likely crashed/ close to off track</span>
</span><span id="__span-7-24"><a id="__codelineno-7-24" name="__codelineno-7-24" href="#__codelineno-7-24"></a>
</span><span id="__span-7-25"><a id="__codelineno-7-25" name="__codelineno-7-25" href="#__codelineno-7-25"></a>    <span class="k">return</span> <span class="n">reward</span>
</span></code></pre></div>
<iframe src="https://www.youtube.com/embed/u647osOlrHQ" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>deepracer reward functions - <a href="https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-examples.html">https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-examples.html</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="reward-hacking">Reward Hacking<a class="headerlink" href="#reward-hacking" title="Permanent link">#</a></h2>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="reward-model">Reward Model<a class="headerlink" href="#reward-model" title="Permanent link">#</a></h2>
<p>A model that is built to simulate human evaluation method and give rewards. For example, a human can evaluate/rank multiple outputs from the same prompt and generated by a language model (as in InstructGPT/ChatGPT).</p>
<p>See also <a href="./">R</a>, <a href="../c/#chatgpt-model">ChatGPT Model</a>, <a href="../i/#instructgpt-model">InstructGPT Model</a>, <a href="./#reward">Reward</a>, <a href="./#reward-shaping">Reward Shaping</a>,  </p>
<h2 id="reward-shaping">Reward Shaping<a class="headerlink" href="#reward-shaping" title="Permanent link">#</a></h2>
<p>How the <a href="./#reward">reward</a> needs to be structure given the rule of the game (ex chess where delayed reward is given for winning the game).</p>
<p>What about <a href="../i/#incentive">Incentive</a>?</p>
<iframe src="https://www.youtube.com/embed/xManAGjbx2k" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>See also <a href="./">R</a>, <a href="../a/#addiction">Addiction</a>, <a href="../d/#delayed-reward">Delayed Reward</a>, <a href="./#reinforcement-learning-rl">Reinforcement Learning</a></p>
<h2 id="reward-trap">Reward Trap<a class="headerlink" href="#reward-trap" title="Permanent link">#</a></h2>
<p>Beware of incorrect <a href="./#reward-shaping">reward shaping</a> in digital evolution! The AI may find unexpected shortcuts or solutions!</p>
<iframe src="https://www.youtube.com/embed/GdTBqBnqhaQ" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<object data="https://arxiv.org/pdf/1803.03453" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/1803.03453">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>paper - [<a href="https://arxiv.org/abs/1803.03453(https://arxiv.org/abs/1803.03453">https://arxiv.org/abs/1803.03453(https://arxiv.org/abs/1803.03453</a>)</li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="ridge-regression">Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permanent link">#</a></h2>
<p>~ aka L2 <a href="./#regularization">Regularization</a>, use this regression when the <a href="../l/#linear-regression">linear regression</a> is <a href="../o/#overfitting">overfitting</a> ? when you don't have enough training data? when number of training samples is smaller than the number of parameters for which we need to find a value.</p>
<p>~ When the sample sizes are relatively small, then Ridge Regression can improve predictions made from new data (i.e. reduce <a href="../v/#variance">variance</a> ) by making the predictions less sensitive to the training data.</p>
<p>==&gt; we introduce <a href="../b/#bias">bias</a> (a penalty through the loss function) to get a lower <a href="../v/#variance">variance</a> (better predictions)</p>
<ul>
<li>
<p>This is done by reducing the weights or the steepness of the slopes (aka sensitivity to input)</p>
</li>
<li>
<p>lambda = a positive number that define how big is the penalty</p>
</li>
<li>lambda &gt;= 0</li>
<li>the higher the lambda the smaller the slope</li>
<li>
<p>to find the correct lambda use <a href="../c/#cross-validation-sampling-method">cross-validation</a></p>
</li>
<li>
<p>Weights cannot exclude useless variables in a model with useless variables (see <a href="../l/#lasso-regression">lasso regression</a> for this!)</p>
</li>
<li>The ridge is better than the lasso regression in the case where all input variables are meaningful.</li>
</ul>
<iframe src="https://www.youtube.com/embed/Q81RR3yKn30" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/Xm2C_gTAl8c" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://www.geeksforgeeks.org/lasso-vs-ridge-vs-elastic-net-ml/">https://www.geeksforgeeks.org/lasso-vs-ridge-vs-elastic-net-ml/</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="ridge-regression-penalty">Ridge Regression Penalty<a class="headerlink" href="#ridge-regression-penalty" title="Permanent link">#</a></h2>
<p><img alt="" src="../img/r/ridge_regression_penalty.png" width="100%" /></p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="riffusion-model">Riffusion Model<a class="headerlink" href="#riffusion-model" title="Permanent link">#</a></h2>
<p>A stable diffusion model trained on [spectrograms] and which can therefore generate music.</p>
<p>More at:</p>
<ul>
<li><a href="https://www.riffusion.com/">https://www.riffusion.com/</a></li>
<li><a href="https://www.riffusion.com/about">https://www.riffusion.com/about</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="riva-model">Riva Model<a class="headerlink" href="#riva-model" title="Permanent link">#</a></h2>
<p>A <a href="../t/#text-to-speech-tts-model">text-to-speech model</a> developer by <a href="../n/#nvidia-company">Nvidia</a>.</p>
<p>More at:</p>
<ul>
<li>site - <a href="https://resources.nvidia.com/en-us-riva-tts-briefcase/speech-synthesis-documentation">https://resources.nvidia.com/en-us-riva-tts-briefcase/speech-synthesis-documentation</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="robot">Robot<a class="headerlink" href="#robot" title="Permanent link">#</a></h2>
<p>A robot is the physical product or device created through the field of <a href="./#robotics">robotics</a>. It is a programmable machine designed to perform specific tasks, often with a degree of autonomy.</p>
<p>It refers specifically to the hardware and software systems implemented to perform tasks.
 Typically it includes sensors, actuators, controllers, and a power supply.</p>
<p>Key Components of a robot includes:</p>
<ol>
<li>Mechanical Systems:<ul>
<li>Parts like arms, wheels, or joints for movement and interaction.</li>
<li>Structural frameworks and actuators (e.g., motors, hydraulics).</li>
</ul>
</li>
<li>Sensors:<ul>
<li>Detect physical stimuli (e.g., light, heat, pressure, or motion) to gather data about the environment.</li>
</ul>
</li>
<li>Control Systems:<ul>
<li>Process information and dictate how the robot responds, often involving algorithms or AI.</li>
</ul>
</li>
<li>Power Supply:<ul>
<li>Provides energy, often through batteries or electrical systems.</li>
</ul>
</li>
<li>Software:<ul>
<li>Programs and algorithms for controlling and coordinating robot actions.</li>
</ul>
</li>
</ol>
<p>Example of robots includes:</p>
<ul>
<li>[Social robots] like [Ameca], <a href="../s/#sophia-robot">Sophia</a>, [Pepper]</li>
<li>Other <a href="../h/#humanoid-robot">Humanoid robots</a> like [Atlas]</li>
<li><a href="../i/#industrial-robot">Industrial robots</a> assembling cars.</li>
<li><a href="../s/#service-robot">Service robots</a> like vacuum cleaners or delivery bots.</li>
</ul>
<iframe src="https://www.youtube.com/embed/Oqq5tgday_w" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<iframe src="https://www.youtube.com/embed/DeDNyIFZFr4" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li>stuntronic robot <a href="https://www.youtube.com/watch?v=oyXl3IhonRM">https://www.youtube.com/watch?v=oyXl3IhonRM</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="robotics">Robotics<a class="headerlink" href="#robotics" title="Permanent link">#</a></h2>
<p>Robotics is an interdisciplinary branch of science and engineering focused on the design, construction, operation, and use of <a href="./#robot">robots</a>. A robot is a machine capable of carrying out complex tasks automatically, often programmed to replicate or augment human actions. Robotics combines multiple fields, including:</p>
<ul>
<li>Mechanical Engineering: For the design and physical construction of robots.</li>
<li>Electrical/Electronics Engineering: For powering robots and enabling sensory and control systems.</li>
<li>Computer Science and AI: For programming, decision-making, and advanced functionality like perception, learning, and adaptability.</li>
</ul>
<h2 id="robotic-foundation-model-rfm-family">Robotic Foundation Model (RFM) Family<a class="headerlink" href="#robotic-foundation-model-rfm-family" title="Permanent link">#</a></h2>
<p>In 2024/03/11, <a href="../c/#covariant-ai-company">Covariant AI</a> launched RFM-1, a foundation model designed to bring AIs learning capabilities directly into the physical realm of robotics. This isn't just about programming a robot to do a job; it's about teaching a robot how to learn to do any job.</p>
<p>RFM is basically an LLM for robot language. Its trained on internet data as well as massive datasets of robot camera feeds, sensor data, and language.</p>
<ul>
<li>RFM-1s advanced AI algorithms enable robots to understand, interact with, and learn the physics of their environment by themselves. </li>
<li>For example, users can tell a robot running RFM-1 to pick up an apple. After identifying the apple by relying on learned characteristics (like shape and color), RFM-1 simulates the best action through video predictions based on its training. </li>
<li>This process is like the human method of planning actions mentally before executing them.</li>
</ul>
<p>RFM-1 is already being deployed in the logistics sector, where it's proving to be a game-changer in warehousing and order fulfillment processes. Covariant believes RFM-1 addresses the growing shortage of workers willing to perform highly repetitive and dangerous tasks (particularly at assembly lines). </p>
<p>RFM-1 hints at a future where machines can learn, adapt, and evolve without needing a programmer. This opens up a new world of possibilities: from manufacturing lines that adjust in real-time to customer demands, to service robots in healthcare that improve their assistance strategies as they interact with people.</p>
<p>More at:</p>
<ul>
<li>announcement - <a href="https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/">https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="robotschool">RobotSchool<a class="headerlink" href="#robotschool" title="Permanent link">#</a></h2>
<p>DEPRECATED by PyBullet</p>
<p>More at : </p>
<ul>
<li><a href="https://openai.com/blog/roboschool/">https://openai.com/blog/roboschool/</a></li>
<li>code - <a href="https://github.com/openai/roboschool/tree/master/agent_zoo">https://github.com/openai/roboschool/tree/master/agent_zoo</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../p/#pybullet-python-module">PyBullet</a>, <a href="../i/#isaac-gym-environment">Isaac Gym</a></p>
<h2 id="robustly-optimized-bert-approach-roberta-model">Robustly Optimized BERT Approach (RoBERTa) Model<a class="headerlink" href="#robustly-optimized-bert-approach-roberta-model" title="Permanent link">#</a></h2>
<p>An update on the <a href="../b/#bidirectional-encoder-representation-from-transformer-bert-model-family">BERT model</a> optimized by <a href="../m/#meta-company">Meta</a>. The RoBERTa model also uses the <a href="../t/#transformer-architecture">transformer architecture</a></p>
<p>A robustly optimized method for pretraining <a href="../n/#natural-language-processing-nlp">natural language processing (NLP)</a> systems that improves on [Bidirectional Encoder Representations from Transformers, or BERT], the self-supervised method released by <a href="../g/#google-company">Google</a> in 2018. BERT is a revolutionary technique that achieved state-of-the-art results on a range of NLP tasks while relying on unannotated text drawn from the web, as opposed to a language <a href="../c/#corpus">corpus</a> thats been labeled specifically for a given task. The technique has since become popular both as an NLP research baseline and as a final task architecture. BERT also highlights the collaborative nature of AI research  thanks to Googles open release, we were able to conduct a replication study of BERT, revealing opportunities to improve its performance. Our optimized method, RoBERTa, produces state-of-the-art results on the widely used NLP benchmark, [General Language Understanding Evaluation (GLUE)].</p>
<p>Pretraining objectives:</p>
<ul>
<li>[Masked Language Model (MLM)]</li>
<li>but not <a href="../n/#next-sentence-prediction-nsp">Next Sentence Prediction (NSP)</a> as the original <a href="../b/#bidirectional-encoder-representation-from-transformer-bert-model-family">BERT model</a> was</li>
</ul>
<iframe src="https://www.youtube.com/embed/-MCYbmU9kfg" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<object data="https://arxiv.org/pdf/1907.11692" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://arxiv.org/pdf/1907.11692">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li><a href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/">https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/</a></li>
<li>paper - <a href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a></li>
<li>RoBERTa with hugginface - <a href="https://anubhav20057.medium.com/step-by-step-guide-abstractive-text-summarization-using-roberta-e93978234a90">https://anubhav20057.medium.com/step-by-step-guide-abstractive-text-summarization-using-roberta-e93978234a90</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="robustness">Robustness<a class="headerlink" href="#robustness" title="Permanent link">#</a></h2>
<p>In <a href="../a/#ai-safety">AI Safety</a>, ...</p>
<ul>
<li>Black swan robustness</li>
<li>Adversarial robustness</li>
</ul>
<p><img alt="" src="../img/r/robustness_adversarial.png" width="100%" /></p>
<p>More at:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/AI_safety#Robustness">https://en.wikipedia.org/wiki/AI_safety#Robustness</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="rocket-league-rl-gym">Rocket League (RL) Gym<a class="headerlink" href="#rocket-league-rl-gym" title="Permanent link">#</a></h2>
<p>More at:
  * <a href="https://rlgym.org/">https://rlgym.org/</a></p>
<p>See also <a href="./">R</a>, <a href="../o/#openai-gym-environment">OpenAI Gym</a>, </p>
<h2 id="root-mean-square-error-rmse">Root Mean Square Error (RMSE)<a class="headerlink" href="#root-mean-square-error-rmse" title="Permanent link">#</a></h2>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">rmse</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">predictions</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</span></code></pre></div>
<p>More at:</p>
<ul>
<li>code - <a href="https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory">https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory</a></li>
</ul>
<p>See also <a href="./">R</a>, <a href="../p/#prediction-error">Prediction Error</a></p>
<h2 id="root-mean-square-propagation-rmsprop-algorithm">Root Mean Square Propagation (RMSprop) Algorithm<a class="headerlink" href="#root-mean-square-propagation-rmsprop-algorithm" title="Permanent link">#</a></h2>
<p>An [optimization algorithm] used by <a href="../o/#optimizer">optimizer</a> to compute parameters to minimize the loss function.</p>
<p>RMSprop (Root Mean Square Propagation) is an optimization algorithm used in machine learning to update the weights of a neural network during training. It is similar to the adaptive learning rate methods, such as <a href="../a/#adaptive-gradient-adagrad-algorithm">AdaGrad</a> and <a href="../a/#adaptive-moment-adam-estimation-algorithm">Adam</a>, in that it adjusts the learning rate for each weight based on the estimated variance of the gradients.</p>
<p>It is an unpublished algorithm first proposed in the Coursera course. "Neural Network for Machine Learning" lecture six by Geoff Hinton. RMSProp lies in the realm of adaptive learning rate methods, which have been growing in popularity in recent years because it is the extension of <a href="../s/#stochastic-gradient-descent-sgd-algorithm">Stochastic Gradient Descent (SGD)</a> algorithm, momentum method, and the foundation of <a href="../a/#adaptive-moment-adam-estimation-algorithm">Adam algorithm</a>. One of the applications of RMSProp is the stochastic technology for <a href="../m/#mini-batch-gradient-descent-algorithm">mini-batch gradient descent</a>.</p>
<iframe src="https://www.youtube.com/embed/_e-LFe_igno" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<p>More at:</p>
<ul>
<li><a href="https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a">https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a</a></li>
<li><a href="https://optimization.cbe.cornell.edu/index.php?title=RMSProp">https://optimization.cbe.cornell.edu/index.php?title=RMSProp</a></li>
<li>from scratch - <a href="https://machinelearningmastery.com/gradient-descent-with-rmsprop-from-scratch/">https://machinelearningmastery.com/gradient-descent-with-rmsprop-from-scratch/</a></li>
</ul>
<p>See also <a href="./">R</a>, [Resilient Backpropagation Algorithm]</p>
<h2 id="rosettafold-rf-diffusion-model">RoseTTAFold (RF) Diffusion Model<a class="headerlink" href="#rosettafold-rf-diffusion-model" title="Permanent link">#</a></h2>
<p>A diffusion model for protein design</p>
<p>A team led by Baker Lab scientists Joseph Watson, David Juergens, Nate Bennett, Brian Trippe, and Jason Yim has created a powerful new way to design proteins by combining structure prediction networks and generative diffusion models. The team demonstrated extremely high computational success and tested hundreds of A.I.-generated proteins in the lab, finding that many may be useful as medications, vaccines, or even new nanomaterials.</p>
<p>Similar to the <a href="../d/#dall-e-model-family">DALL-E model</a> and other [Denoising Diffusion Probabilistic Models (DDPM)] that are used to generate images, we have developed a guided diffusion model for generating new proteins. With prior design methods, tens of thousands of molecules may have to be tested before finding a single one that performs as intended. Using the new design method, dubbed RF Diffusion, the team had to test as little as one per design challenge.</p>
<p>RF Diffusion outperforms existing protein design methods across a broad range of problems, including topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. Highlights include a picomolar binder generated through pure computation and a series of novel symmetric assemblies experimentally confirmed by electron microscopy.</p>
<iframe src="https://www.youtube.com/embed/i8fGzddGbU8" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="allowfullscreen"></iframe>

<object data="https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1.full.pdf" type="application/pdf" width="100%" height="600px">
    <p>Your browser does not support PDFs. Please download the PDF: 
       <a href="https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1.full.pdf">Download PDF</a>.
    </p>
</object>

<p>More at:</p>
<ul>
<li>blog - <a href="https://www.bakerlab.org/2022/11/30/diffusion-model-for-protein-design/">https://www.bakerlab.org/2022/11/30/diffusion-model-for-protein-design/</a></li>
<li>paper - <a href="https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1">https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1</a> </li>
</ul>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="rule">Rule<a class="headerlink" href="#rule" title="Permanent link">#</a></h2>
<p>Rules are used in <a href="../e/#expert-system">expert systems</a></p>
<p>The increased number of rules leads to the <a href="../c/#complexity-ceiling">complexity ceiling</a> due in part to <a href="./#rule-interaction">rule interactions</a></p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>A typical MYCIN rule read:
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>IF 1) The infection requires therapy is meningitis, and
</span><span id="__span-9-4"><a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a>   2) THe type of infection is fungal and
</span><span id="__span-9-5"><a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>   3) Organisms were not seen on the stain of the culture, and
</span><span id="__span-9-6"><a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a>   4) The patient is not a compromised host, and
</span><span id="__span-9-7"><a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>   5) The patient has been to an area that is endemic for coccidiomycoses, and
</span><span id="__span-9-8"><a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>   6) The race of the patient is one of [B]lack [A]sian [I]ndian, and
</span><span id="__span-9-9"><a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a>   7) The cryptococcal antigen in the csf was positive
</span><span id="__span-9-10"><a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a>
</span><span id="__span-9-11"><a id="__codelineno-9-11" name="__codelineno-9-11" href="#__codelineno-9-11"></a>THEN
</span><span id="__span-9-12"><a id="__codelineno-9-12" name="__codelineno-9-12" href="#__codelineno-9-12"></a>   There is a suggestive evidence (.5)
</span><span id="__span-9-13"><a id="__codelineno-9-13" name="__codelineno-9-13" href="#__codelineno-9-13"></a>   that cryptococcus is not one of the organisms (other than those seen on cultures or smears)i
</span><span id="__span-9-14"><a id="__codelineno-9-14" name="__codelineno-9-14" href="#__codelineno-9-14"></a>   which might be causing the infection
</span></code></pre></div>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="rule-interaction">Rule Interaction<a class="headerlink" href="#rule-interaction" title="Permanent link">#</a></h2>
<p>Rule interaction in <a href="../e/#expert-system">expert systems</a> refers to how different rules in the knowledge base can affect each other, sometimes in unexpected ways. Here's an example to illustrate this concept:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>Let&#39;s consider a simple medical diagnosis expert system with the following rules:
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>IF patient has fever AND cough THEN consider flu
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>IF patient has fever AND sore throat THEN consider strep throat
</span><span id="__span-10-5"><a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>IF patient has flu THEN recommend rest and fluids
</span><span id="__span-10-6"><a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>IF patient has strep throat THEN recommend antibiotics
</span><span id="__span-10-7"><a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>IF patient is allergic to penicillin AND strep throat is suspected THEN use alternative antibiotic
</span></code></pre></div>
<p>Now, let's say we have a patient with the following symptoms:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>Fever
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>Cough
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>Sore throat
</span><span id="__span-11-4"><a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>Allergic to penicillin
</span></code></pre></div>
<p>Here's how rule interaction can create complexity:</p>
<ul>
<li>Rules 1 and 2 both fire because the patient has fever with both cough and sore throat. The system now has to decide between flu and strep throat, or consider both.</li>
<li>Depending on which diagnosis is prioritized, either rule 3 or 4 will fire, leading to different treatment recommendations.</li>
<li>If strep throat is considered, rule 5 also becomes relevant due to the penicillin allergy.</li>
</ul>
<p>This simple example demonstrates several types of rule interactions:</p>
<ul>
<li>Conflict: Rules 1 and 2 lead to different diagnoses based on overlapping symptoms.</li>
<li>Chaining: The outcome of rules 1 and 2 affects which of rules 3 and 4 will fire.</li>
<li>Interdependence: Rule 5 depends on both the outcome of rule 2 and additional patient information.</li>
</ul>
<p>In a real-world <a href="../e/#expert-system">expert system</a> with hundreds or thousands of rules, these interactions can become extremely complex. The system might need to employ sophisticated conflict resolution strategies, certainty factors, or other mechanisms to handle these interactions effectively. As the number of rules grows, ensuring that all possible interactions are accounted for and lead to correct outcomes becomes increasingly challenging, contributing to the <a href="../c/#complexity-ceiling">complexity ceiling</a> of <a href="../e/#expert-system">expert systems</a>.</p>
<p>See also <a href="./">R</a>, ...</p>
<h2 id="runway-company">Runway Company<a class="headerlink" href="#runway-company" title="Permanent link">#</a></h2>
<p>An AI company focusing on the generative AI for images and videos.</p>
<p>Models:</p>
<ul>
<li>2021 - [Latent Diffusion Model]</li>
<li>2022 - <a href="../s/#stable-diffusion-model">Stable Diffusion Model</a></li>
<li>2023 - <a href="../g/#gen-model">Gen Model</a></li>
</ul>
<p>More at:</p>
<ul>
<li><a href="https://runwayml.com/">https://runwayml.com/</a></li>
</ul>
<p>See also <a href="./">R</a>, ...</p>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">February 7, 2025</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">February 7, 2025</span>
  </span>

    
    
    
      
  <span class="md-source-file__fact">
    
      
  <span class="md-icon" title="Contributors">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34-.46-1.16-1.11-1.47-1.11-1.47-.91-.62.07-.6.07-.6 1 .07 1.53 1.03 1.53 1.03.87 1.52 2.34 1.07 2.91.83.09-.65.35-1.09.63-1.34-2.22-.25-4.55-1.11-4.55-4.92 0-1.11.38-2 1.03-2.71-.1-.25-.45-1.29.1-2.64 0 0 .84-.27 2.75 1.02.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02.55 1.35.2 2.39.1 2.64.65.71 1.03 1.6 1.03 2.71 0 3.82-2.34 4.66-4.57 4.91.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"/></svg>
  </span>
  <span>GitHub</span>

    
    <nav>
      
        <a href="https://github.com/emayssat" class="md-author" title="@emayssat">
          
          <img src="https://avatars.githubusercontent.com/u/1972699?v=4&size=72" alt="emayssat">
        </a>
      
      
      
    </nav>
  </span>

    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../q/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Q">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256l137.3-137.4c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Q
              </div>
            </div>
          </a>
        
        
          
          <a href="../s/" class="md-footer__link md-footer__link--next" aria-label="Next: S">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                S
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 - 2025 <a href="https://www.midtown.ai/" rel="noopener" target="_blank">Midtown AI, Inc.</a>
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://x.com/midtown_ai" target="_blank" rel="noopener" title="Follow us on X" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:ai4all@midtown.ai" target="_blank" rel="noopener" title="Send us an email" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.footer", "navigation.indexes", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../../javascript/mathjax.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../../javascript/tablesort.js"></script>
      
    
  </body>
</html>